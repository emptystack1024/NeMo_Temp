{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emptystack1024/NeMo_Temp/blob/main/tutorials/WFST_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Qq1Hz6CKWdwl",
        "outputId": "78f3aeb6-01c7-43d9-b1cc-6aafc3a3eca4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n您可以在本地或 Google Colab 上运行本笔记本。\\n\\n设置 Colab 的说明如下：\\n1. 打开一个新的 Python 3 笔记本。\\n2. 从 GitHub 导入此笔记本（文件 -> 上传笔记本 -> “GITHUB ”选项卡 -> 复制/粘贴 GitHub URL）\\n3. 可选： 重启运行时（运行时 -> 重启运行时），使升级的软件包生效\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "您可以在本地或 Google Colab 上运行本笔记本。\n",
        "\n",
        "设置 Colab 的说明如下：\n",
        "1. 打开一个新的 Python 3 笔记本。\n",
        "2. 从 GitHub 导入此笔记本（文件 -> 上传笔记本 -> “GITHUB ”选项卡 -> 复制/粘贴 GitHub URL）\n",
        "3. 可选： 重启运行时（运行时 -> 重启运行时），使升级的软件包生效\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFf9PVUkSRZ"
      },
      "source": [
        "先决条件：\n",
        "1. 在使用本笔记本之前，请务必阅读[文本处理文档](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/intro.html)和[文本规范化](https://colab.research.google.com/github/NVIDIA/NeMo-text-processing/blob/main/tutorials/Text_(Inverse)_Normalization.ipynb)入门教程。本手册是关于如何定制和开发自己的文本规范化或逆文本规范化语法的深入教程。\n",
        "2. 下载 NeMo 源代码。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1QVk1vU1kSRa",
        "outputId": "95cd1649-fd35-441f-8ba2-00012cf76a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nemo_text_processing\n",
            "  Cloning https://github.com/NVIDIA/NeMo-text-processing.git (to revision main) to /tmp/pip-install-m0wtd_3p/nemo-text-processing_34dba6f6755b42e7b429cc1ae04ea15f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo-text-processing.git /tmp/pip-install-m0wtd_3p/nemo-text-processing_34dba6f6755b42e7b429cc1ae04ea15f\n",
            "  Resolved https://github.com/NVIDIA/NeMo-text-processing.git to commit ac07488b145262a15fd5c2d7e204be53cb0b6037\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cdifflib (from nemo_text_processing)\n",
            "  Downloading cdifflib-1.2.9.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (0.8.1)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (7.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (1.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (2.2.2)\n",
            "Collecting pynini==2.1.6.post1 (from nemo_text_processing)\n",
            "  Downloading pynini-2.1.6.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (2024.11.6)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_text_processing)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (75.2.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (4.51.3)\n",
            "Collecting wget (from nemo_text_processing)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from nemo_text_processing) (1.17.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses>=0.0.43->nemo_text_processing) (8.2.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_text_processing) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_text_processing) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_text_processing) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_text_processing) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_text_processing) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_text_processing) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->nemo_text_processing) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->nemo_text_processing) (0.31.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->nemo_text_processing) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->nemo_text_processing) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->nemo_text_processing) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->nemo_text_processing) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->nemo_text_processing) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->nemo_text_processing) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->nemo_text_processing) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->nemo_text_processing) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->nemo_text_processing) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->nemo_text_processing) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->nemo_text_processing) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->nemo_text_processing) (2025.4.26)\n",
            "Downloading pynini-2.1.6.post1-cp311-cp311-manylinux_2_28_x86_64.whl (154.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nemo_text_processing, cdifflib, wget\n",
            "  Building wheel for nemo_text_processing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo_text_processing: filename=nemo_text_processing-1.1.0-py3-none-any.whl size=2883951 sha256=5db7d3c4aa8fa6c9d620824a9ea32c5c173f6f202750c7924abc21af0966c040\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pjfuub9w/wheels/a0/0e/ae/ba91ec55b7b9adaa595f3ce731f5c5ff0ad9072dfc41806ff6\n",
            "  Building wheel for cdifflib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdifflib: filename=cdifflib-1.2.9-cp311-cp311-linux_x86_64.whl size=28831 sha256=13b696cde6517ab89544a9b1d1e6cfbdc472ef54f3e2fd2e17e49a9074ba88e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/89/fa/9ac6480fd2400e5d7f4795b524c0d241eb8cdb921b72d5d102\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=86a4ad986502bec1e238c284960e6d9a3a43363d9fe8f2369c6e7769f91064d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built nemo_text_processing cdifflib wget\n",
            "Installing collected packages: wget, sacremoses, pynini, cdifflib, nemo_text_processing\n",
            "Successfully installed cdifflib-1.2.9 nemo_text_processing-1.1.0 pynini-2.1.6.post1 sacremoses-0.1.1 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "## Install NeMo-text-processing\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo-text-processing.git@$BRANCH#egg=nemo_text_processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ebPgJyddkSRa"
      },
      "outputs": [],
      "source": [
        "import pynini\n",
        "import nemo_text_processing\n",
        "from pynini.lib import pynutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pmo90YybkSRa"
      },
      "outputs": [],
      "source": [
        "from nemo_text_processing.text_normalization.en.graph_utils import GraphFst, NEMO_DIGIT, delete_space, NEMO_SIGMA, NEMO_NOT_QUOTE, delete_extra_space, NEMO_NON_BREAKING_SPACE\n",
        "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
        "\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.cardinal import CardinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.decimal import DecimalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.money import MoneyFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.ordinal import OrdinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.punctuation import PunctuationFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.time import TimeFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.whitelist import WhiteListFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.word import WordFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.cardinal import CardinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.decimal import DecimalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.money import MoneyFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.ordinal import OrdinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.time import TimeFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.whitelist import WhiteListFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.word import WordFst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0JxcvuPHvn9"
      },
      "source": [
        "NeMo 的文本处理模块使用加权有限状态转换器（WFST）来部署高效文本规范化（TN）和逆文本规范化（ITN）语法。在本教程中，您将学习如何从头开始构建规范化语法，以用于自己的文本处理任务。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMOxehxBkSRb"
      },
      "source": [
        "# Table of Contents\n",
        "- <a href='#wfsts'>WFSTs</a>\n",
        "- <a href='#nemo-inverse-text-processing'>NeMo Inverse Text Processing</a>\n",
        "- <a href='#getting-started'>Getting Started</a>\n",
        "- <a href='#cardinal-wfst'>Cardinal WFST</a>\n",
        "- <a href='#ordinal-wfst'>Ordinal WFST</a>\n",
        "- <a href='#decimal-wfst'>Decimal WFST</a>\n",
        "- <a href='#money-wfst'>Money WFST</a>\n",
        "- <a href='#time-wfst'>Time WFST</a>\n",
        "- <a href='#whitelist-wfst'>WhiteList WFST</a>\n",
        "- <a href='#word-and-punctuation-wfst'>Word and Punctuation WFST</a>\n",
        "- <a href='#other-classes'>Other Classes</a>\n",
        "- <a href='#tokenize-and-classify'>Tokenize and Classify</a>\n",
        "- <a href='#verbalize-and-verbalize-final'>Verbalize and Verbalize Final</a>\n",
        "- <a href='#deployment'>Deployment</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMUovcMsfXyI"
      },
      "source": [
        "# WFSTs <a id=\"wfsts\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ejNMLbH1jM"
      },
      "source": [
        "WFST 是[有限状态机](https://en.wikipedia.org/wiki/Finite-state_machine) 的一种形式，用于绘制正则表达式语言（或[正则表达式](https://en.wikipedia.org/wiki/Regular_expression)）之间的关系图。就我们的目的而言，它们可以由两个主要属性来定义：\n",
        "\n",
        "1. 用于文本替换的可接受输入和输出表达式之间的映射\n",
        "2. 路径加权以指导图的遍历"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNg45ZuaP_A8"
      },
      "source": [
        "例如，考虑一个简单的规范化任务：将单词 “cent”（法语中 “一百 ”的意思）映射为数字表示法 “100”。 我们将首先使用一个有限状态表示法来表示 regex `/cent/`："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxo7gUkW_XKT"
      },
      "source": [
        "![cent.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/cent.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fahsjMVFlbCa"
      },
      "source": [
        "然后创建文本字符串 `100` 的映射："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMJ-fNSk_jXC"
      },
      "source": [
        "![cent_to_100.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/cent_to_100.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPKW0I4yAGUb"
      },
      "source": [
        "*注：按照惯例，空字符用 `ε` 表示*。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0NK3aW5nG9C"
      },
      "source": [
        "这将为我们提供一个具有通用路径权重的 WFST。(默认情况下，“pynini ”使用[tropical semirings](https://en.wikipedia.org/wiki/Tropical_semiring)来表示弧，每个弧的默认权重为 “0”）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzBc9D3qTGJ-"
      },
      "source": [
        "现在，让我们考虑扩展我们的模型。为了表示 “100 ”和 “200 ”之间的数值，法语使用 “cent + digit ”的数字方案。例如，“120 ”的发音为 “cent-vingt”。为了创建适当的输出字符串，我们现在需要将 “分 ”映射为 “1”，并将字符串的其余部分映射为适当的数字表示。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRrKNQRjFDoL"
      },
      "source": [
        "![cent_vingt_to_120.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/cent_vingt_to_120.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLpm4mufAfUz"
      },
      "source": [
        "然而，这将使我们的图[非确定性](https://en.wikipedia.org/wiki/Nondeterministic_algorithm) - 它将有多种终止的可能性。现在，输入 “cent-vingt ”的结果可能是 “100 ”或 “10020”，而其中只有一个是正确的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvK92YIPkSRc"
      },
      "source": [
        "![cent_vingt_bad.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/cent_vingt_bad.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-GJTpgIAf7S"
      },
      "source": [
        "为了纠正这种情况，我们可以在接受不含 `s` 的输入的路径上添加一个新的结束状态和权重："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GJcsdttGg_S"
      },
      "source": [
        "![cent_vingt_good.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/cent_vingt_good.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHft1gzsAipc"
      },
      "source": [
        "现在，我们可以依靠最短路径（最小权重）启发式来保证理想的映射：图的遍历将优先考虑较长的输入，只有在没有较大输入时才将 “分 ”转换为 “100”。因此，我们现在删除了不需要的输出 `10020`，同时保留了我们所需的字符串映射覆盖率。\n",
        "\n",
        "通过使用权重来确保行为的可预测性，WFST 可以利用标准图遍历算法的效率，同时还能保持通用性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ik4PBXafSSB"
      },
      "source": [
        "# NeMo 逆文本处理 <a id=\"nemo-inverse-text-processing\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2fcWKhqYVF5"
      },
      "source": [
        "按照 [Google's Kestrel](https://www.researchgate.net/publication/277932107_The_Kestrel_TTS_text_normalization_system) 框架，NeMo 部署了两个复合 WFST 用于文本规范化。它们如下\n",
        "1. 一个*分类器*（或标签），按 “符号类别”（如货币、序号、街道地址）标记潜在标记符\n",
        "2. 将标记符转换成常规书面形式的*语言转换器*。\n",
        "\n",
        "例如，请看下面这个句子 <<le premier juillet il a mangé trente-cinq pommes>> 句子\n",
        "\n",
        "对于 ITN 任务，标记符生成器将识别以下标记符：\n",
        "\n",
        "`[“le” , “premier”, “juillet”, “il”, “a”, “mangé”, “trente-cinq”,\n",
        "‘pommes’]`\n",
        "\n",
        "并为每个类提供一个标记：\n",
        "\n",
        "- `tokens { name: “le” }`\n",
        "- `tokens { date { day： “1” month： “juillet” } } `\n",
        "- `tokens { name: “il” }`\n",
        "- `tokens { name: “a” }`\n",
        "- `tokens { name: “mangé” }`\n",
        "- `tokens { cardinal { integer： “35” } }`\n",
        "- `tokens { name: “pommes” }`\n",
        "\n",
        "然后，这些标记将被传递给 “语言化器 ”WFST，后者将以常规的书面形式呈现每个标记：\n",
        "\n",
        "- `tokens { name: “le” }` -> `le`\n",
        "- `tokens { date { day： “1” month： “juillet” } } ` -> `1ᵉʳ`\n",
        "- `tokens { name: “il” }` -> `juillet`\n",
        "- `tokens { name: “il” }` -> `il`\n",
        "- `tokens { name: “a” }` -> `a`\n",
        "- `tokens { name: “mangé” }` -> `mangé`\n",
        "- `tokens { cardinal { integer： “35” } }` -> `35`\n",
        "- `tokens { name: “pommes” }` -> `pommes`\n",
        "\n",
        "并合并为一个规范化字符串：\n",
        "\n",
        "Le 1ᵉʳ juillet il a mangé 35 pommes\n",
        "\n",
        "与之对应的 TN 任务则是相反的过程。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n-5JExAbvwr"
      },
      "source": [
        ">**_Note:_**\n",
        "> 关于反向文本规范化，有几点需要注意：\n",
        ">- 每个类标记都有一组独特的字段名称，分类器必须对其进行解析。NeMo 的默认字段名是按照 [Sparrowhawk](https://github.com/google/sparrowhawk) 中的语法选择的，以便进行部署。如果这些字段不准确，将无法使用 Sparrowhawk。\n",
        ">- NeMo 假定不使用标点符号（除非语法中明确提供）和所有小写字母，以方便与上游 ASR 集成。\n",
        ">- `name`类标记是任何不需要处理的标记的默认值。它将保持 “原样”。\n",
        ">- 您可能会注意到标记符是如何将 `premier` 转换为 `1` 的，而动词符则将 `1` 归一化为 `1ᵉʳ` 。这种决定取决于执行情况，并会因偏好和语言而异。(也就是说，从 `premier` -> `1ᵉʳ` 的规范化可能是一个标记化步骤）。\n",
        ">- 默认情况下，NeMo 会在一个标记中创建多个关键值的排列组合，以方便规范化。也就是说，给定的标记是 `tokens { date { day： “1” month： “juillet” } }` 时，它也会为 `tokens { date { month： \"juillet\" day： \"1\" } }`. 为了避免这种情况，并避免动词化器输入中的歧义，可以为标记分配一个 `preserve_order` 属性，以防止出现变序。(例如 `tokens { date { day: \"1\" month: \"juillet\" preserve_order: true }` ）（我们将在[教程的稍后部分](#verbalizer)讨论这个问题）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fxI20GZkSRd"
      },
      "source": [
        "## WFST Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBa8QHhAkSRd"
      },
      "source": [
        "NeMo 文本处理的基础语言支持一系列符号类，以便与 Sparrowhawk 集成。\n",
        "在本教程中，我们将重点介绍以下类：\n",
        "- 标号\n",
        "- 有位数\n",
        "- 小数\n",
        "- 货币\n",
        "- 时间\n",
        "- 白名单\n",
        "- 单词\n",
        "- 文法\n",
        "\n",
        "虽然并不全面，但这些课程将为您提供足够的基础，并让您接触到边缘案例，从而能够自如地构建其他案例。\n",
        "\n",
        "**注意**： *如果您只打算使用 NeMo 进行个人开发，您可以根据需要重新命名这些课程。但是，Sparrowhawk 集成\n",
        "只要求使用这些标记及其分配的属性。有关 Sparrowhawk 标记和属性列表，[请查阅 Sparrowhawk 存储库](https://github.com/yzhang123/sparrowhawk/blob/test/src/proto/semiotic_classes.proto)*。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kZkfOM7kSRd"
      },
      "source": [
        "## Further Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chrRqrFkSRd"
      },
      "source": [
        "If you wish to learn more about NeMo Text Processing, you may wish to consult the following:\n",
        "- [Y. Zhang, E. Bakhturina, K. Gorman, and B. Ginsburg, \"NeMo Inverse Text Normalization: From Development To Production\"](https://arxiv.org/pdf/2104.05055.pdf)\n",
        "- [NeMo's Text Normalization Documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/intro.html)\n",
        "- [NeMo's Text Normalization Deployment Documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/wfst_text_processing_deployment.html)\n",
        "- NeMo's [Text Normalization Introduction Tutorial](https://colab.research.google.com/github/NVIDIA/NeMo-text-processing/blob/main/tutorials/Text_(Inverse)_Normalization.ipynb)\n",
        "- [Sparrowhawk Documentation](https://github.com/google/sparrowhawk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1OvkXjIkSRd"
      },
      "source": [
        "For further information regarding WFSTs, please see:\n",
        "- [D. Jufasky and J. Martin, *Natural Language Processing*, Ch. 2](https://web.stanford.edu/~jurafsky/slp3/2.pdf)\n",
        "- [K. Gorman and R. Sproat, *Finite-State Text Processing*](http://www.morganclaypoolpublishers.com/catalog_Orig/product_info.php?products_id=1636)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFdXRcnUfI25"
      },
      "source": [
        "# Getting Started <a id=\"getting-started\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Zl3VwqdYqL"
      },
      "source": [
        "要开始开发标记程序，请确保您已[从源代码安装 NeMo](https://github.com/NVIDIA/NeMo)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGg7Bf13FXgc"
      },
      "source": [
        "在本教程中，我们将重点开发一个反向文本规范化系统，例如在下游 ASR 处理中可能会遇到的系统。因此，我们将导航到\n",
        "`nemo_text_processing/inverse_text_normalization`，并为我们的目标语言（法语）创建一个目录，以及为 `taggers` 和 `verbalizers` 创建子目录\n",
        "。您可能还希望创建一个 `data` 子目录，以方便导航。\n",
        "\n",
        "(注意，对于文本规范化，建议在 `nemo_text_processing/text_normalization` 文件夹中使用相同的目录结构。事实上，NeMo 的许多语法都是共享的）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T58E4pU4FN3A"
      },
      "source": [
        "```bash\n",
        "git clone https://github.com/NVIDIA/NeMo\n",
        "cd NeMo && ./reinstall.sh\n",
        "cd nemo_text_processing/inverse_text_normalization/\n",
        "export LANGUAGE=fr # Change this to your desired language\n",
        "mkdir $LANGUAGE\n",
        "mkdir $LANGUAGE/taggers\n",
        "mkdir $LANGUAGE/verbalizers\n",
        "mkdir $LANGUAGE/data\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vfz-bUFpwz"
      },
      "source": [
        "在 NeMo 中部署的所有 WFST 都继承自 “GraphFst ”类。\n",
        "在大多数情况下，你可以简单地从已有的`graph_utils.py`中导入，但有时你会发现在工作目录中保留一份\n",
        "，以便进行特定语言的编辑，这对部署很有帮助。(就我们而言，我们将使用 “nemo_text_processing.text_normalization.en.graph_utils”，它是 NeMo 语法的默认语法）。\n",
        "\n",
        "你也可以在工作目录中保留一份 `utils.py`（可在每个语言系统的目录中找到）\n",
        "，以协助路径定位。(请务必根据您的语言调整导入）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OME84EmOQ4h",
        "outputId": "6eea17f9-aae9-4176-ae35-3d1f0e94b4ea"
      },
      "source": [
        "```bash\n",
        "cp ../text_normalization/en/graph_utils.py $LANGUAGE/\n",
        "cp ../text_normalization/en/utils.py $LANGUAGE/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naHYVvbPkSRe"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtD_rC_5kSRe"
      },
      "source": [
        "在开发过程中，我们使用了 “nemo_text_processing ”和 ‘pynini’（一个用于高效构建和遍历 WFST 的 Python 库，默认与 “nemo ”一起安装）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXvvnghfkSRe"
      },
      "source": [
        "虽然本教程将尝试使 `pynini` 工具的使用透明化，但仍假定读者对其语法有一定的了解。如需更深入的指导，可参考以下功能概述：\n",
        "\n",
        "- [K. Gorman, Pynini： 用于加权有限状态语法编译的 Python 库](https://aclanthology.org/W16-2409.pdf)\n",
        "- [Pynini 文档](https://www.openfst.org/twiki/bin/view/GRM/PyniniDocs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjHutXatkSRh"
      },
      "source": [
        "我们还将导入 “pynutil ”模块，以访问一些额外的功能，并编写一个简单的辅助函数，通过之前讨论过的 “最短路径 ”启发式打印 “pynini ”图。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz18Ui8-8Kf4"
      },
      "outputs": [],
      "source": [
        "from pynini.lib import pynutil\n",
        "\n",
        "def apply_fst(text, fst):\n",
        "  \"\"\" Given a string input, returns the output string\n",
        "  produced by traversing the path with lowest weight.\n",
        "  If no valid path accepts input string, returns an\n",
        "  error.\n",
        "  \"\"\"\n",
        "  try:\n",
        "     print(pynini.shortestpath(text @ fst).string())\n",
        "  except pynini.FstOpError:\n",
        "    print(f\"Error: No valid output with given input: '{text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNsxgNlkSRi"
      },
      "source": [
        "# Cardinal WFST <a id=\"cardinal-wfst\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOyLZb9DgLoh"
      },
      "source": [
        "绝大多数 ITN 任务都要求能够识别数字并对其进行去规范化处理。因此，我们将首先开发一个红心（整数）数分类器和动词化器。(e.g. `-3,-2,-1,0,1,2,3,4,5....99,100,101...`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZQkH1V89kh"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h28tt1dQkSRi"
      },
      "source": [
        "首先，我们将以法语为例，构建一个 Cardinal WFST。虽然您的目标语言显然会与我们的示例语言有很大不同，但您很可能会发现一些相似之处，例如\n",
        "- 使用（半）规则十进制（基数为 10）计数系统。(这是自然语言的一个共同特征，但并不普遍）。\n",
        "- 在我们的 WFST 结构中加入了一些需要应急的不规则情况。(例如，伪十进制（基数 20）数列）。\n",
        "- 在枚举中使用性别和数字一致。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seCEx2KpkSRi"
      },
      "source": [
        "### Digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzJ2DIwc_TT3"
      },
      "source": [
        "我们将从小数点后第一位开始。由于这些数字是 WFST 其他部分的基石，我们将首先使用 `pynini.string_map`明确调用它们的 WFST 映射："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0H4qg4BjYfB"
      },
      "outputs": [],
      "source": [
        "zero = pynini.string_map([(\"zéro\",\"0\")]) # French only pronounces zeroes as stand alone\n",
        "digits = pynini.string_map([ # pynini function that creates explicit input-output mappings for a WFST\n",
        "\t\t\t\t(\"un\",\"1\"),\n",
        "\t\t\t\t(\"une\",\"1\"),\n",
        "\t\t\t\t(\"deux\",\"2\"),\n",
        "\t\t\t\t(\"trois\",\"3\"),\n",
        "\t\t\t\t(\"quatre\",\"4\"),\n",
        "\t\t\t\t(\"cinq\",\"5\"),\n",
        "\t\t\t\t(\"six\",\"6\"),\n",
        "\t\t\t\t(\"sept\",\"7\"),\n",
        "\t\t\t\t(\"huit\",\"8\"),\n",
        "\t\t\t\t(\"neuf\",\"9\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nHjY-NNjdWQ"
      },
      "source": [
        "我们也可以简单地在单独的数据文件夹中写入一个 `tsv` 文件"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvgDWO1UkSRi"
      },
      "source": [
        "- zéro\t0\n",
        "- un\t1\n",
        "- une\t1\n",
        "- deux\t2\n",
        "- trois\t3\n",
        "- quatre\t4\n",
        "- cinq\t5\n",
        "- six\t6\n",
        "- sept\t7\n",
        "- huit\t8\n",
        "- neuf\t9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xicKcZLEzQTg"
      },
      "source": [
        "并用 `string_file` 导入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw4tlhFzkSRi"
      },
      "source": [
        "`digits = pynini.string_file(\"data/digits.tsv\")`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWiinjtKkSRi"
      },
      "source": [
        "如果 utils.py 在工作目录中，也可以使用 `get_abs_path`，它总是调用相对于 {LANGUAGE} 目录的路径：\n",
        "\n",
        "`from nemo_text_processing.inverse_normalization.{LANGUAGE}.utils import get_abs_path`\n",
        "\n",
        "`digits = pynini.string_file(get_abs_path(\"data/digits.tsv\"))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPccmicQkYAB"
      },
      "source": [
        "While we will use `string_map` throughout this tutorial, please note that NeMo employs the later option for maintainability and recommends its use instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIlAMlDfkSRi"
      },
      "source": [
        "### Teens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQJiJcVMrNmC"
      },
      "source": [
        "让我们看看下一组数字：\n",
        "- 10 - dix\n",
        "- 11 - onze\n",
        "- 12 - douze\n",
        "- 13 - treize\n",
        "- 14 - quatorze\n",
        "- 15 - quinze\n",
        "- 16 - seize\n",
        "- 17 - dix-sept\n",
        "- 18 - dix-huit\n",
        "- 19 - dix-neuf\n",
        "\n",
        "像以前一样，我们可以简单地使用 `string_map` 为它们组成一个 WFST。但要注意的是，数字集中存在一些冗余： 17、18 和 19 都是 `dix + digit` 的形式。在这些情况下，重用我们之前的 WFST 会比简单地创建新弧、状态和权重更有效。\n",
        "\n",
        "我们可以使用 pynini 的字符串连接功能来扩展可接受的输入字符串。首先，我们将为 `11-16` 创建一个 WFST。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orSgBwyXsfY5"
      },
      "outputs": [],
      "source": [
        "teens = pynini.string_map([\n",
        "\t\t\t\t(\"onze\",\"11\"),\n",
        "\t\t\t\t(\"douze\",\"12\"),\n",
        "\t\t\t\t(\"treize\",\"13\"),\n",
        "\t\t\t\t(\"quatorze\",\"14\"),\n",
        "\t\t\t\t(\"quinze\",\"15\"),\n",
        "\t\t\t\t(\"seize\",\"16\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1yIgigdtriQ"
      },
      "source": [
        "现在，我们将创建一个 `tens` WFST，负责映射所有的 “dix ”实例，并与之前的 `digits` WFST 连接（使用重载的 `+` 操作符完成）。(使用内置的 `delete_hyphen` 删除中间可能存在的连字符）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzwZrFCkt87W"
      },
      "outputs": [],
      "source": [
        "tens = pynini.string_map([(\"dix\", \"1\")])\n",
        "delete_hyphen = pynini.closure(pynutil.delete(\"-\"), 0, 1) # Applies a closure from 0-1 of operation. Equivalent to regex /?/\n",
        "\n",
        "graph_tens = tens + delete_hyphen + digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2knCwybmuTDn"
      },
      "source": [
        "现在，我们可以通过联合操作（使用重载的 `|`操作符）将 `teens` 和 `graph_tens` WFST 结合在一起，从而可以选择任一图形。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIRJ4PE7uRrl"
      },
      "outputs": [],
      "source": [
        "graph_tens_and_teens = graph_tens | teens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGkzKoeuxbeA"
      },
      "source": [
        "让我们看看它是否能通过字符串函数运行："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2iD0_HnxdUV"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"dix-huit\", graph_tens_and_teens)\n",
        "apply_fst(\"seize\", graph_tens_and_teens)\n",
        "apply_fst(\"dix\", graph_tens_and_teens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh2f-3rux8_2"
      },
      "source": [
        "前两个都成功了，但为什么 \"dix \"会出错呢？如果你回过头来看，就会发现虽然我们的图有一个从 “dix ”到 “1 ”的映射，但与 “digits ”的连接假定了这些字符串中的某些输入会紧随其后。也就是说，我们没有为 “数字 ”的 “遗漏 ”留下任何机会。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM_eJYlV1UVp"
      },
      "source": [
        "![dix_to_digits.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/dix_to_digits.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xCMKRA1Wzw"
      },
      "source": [
        "您可能还会注意到，如果我们只想对数字进行标准化处理，这个问题也会存在--我们的图表会出错，因为它首先需要一个 “十 ”或输入。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJHnlJCm1dPv"
      },
      "source": [
        "我们可以通过一个选项来解决这两个问题，即直接插入一个零，而不需要任何额外的输入。(就像我们的 “美分 ”例子一样）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_vvJ9Bl1dYQ"
      },
      "source": [
        "![dix_to_digits_with_insert.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/dix_to_digits_with_insert.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJq3uoMN2OcC"
      },
      "source": [
        "这可以通过使用`pynutil.insert`函数来实现："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h9xuNfA081P"
      },
      "outputs": [],
      "source": [
        "graph_digits = digits | pynutil.insert(\"0\") # inserts zero if no digit follows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA_L_6Ky2SHm"
      },
      "source": [
        "And for `graph_tens`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jelVA81o2RXu"
      },
      "outputs": [],
      "source": [
        "tens = tens | pynutil.insert(\"0\") | tens + delete_hyphen\n",
        "graph_tens = tens + graph_digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb5uhpGr3I4X"
      },
      "source": [
        "Bringing everything together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLkDddkA3Stu"
      },
      "outputs": [],
      "source": [
        "graph_teens_and_tens = graph_tens | teens\n",
        "graph_all = graph_teens_and_tens | zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DESDKScv3r3P"
      },
      "source": [
        "Let us now check our tests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wrDNXuD3oh9"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"dix-huit\", graph_all)\n",
        "apply_fst(\"seize\" , graph_all)\n",
        "apply_fst(\"dix\" , graph_all)\n",
        "apply_fst(\"une\" , graph_all)\n",
        "apply_fst(\"trois\" , graph_all)\n",
        "apply_fst(\"quatre\" , graph_all)\n",
        "apply_fst(\"zéro\" , graph_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz_k3NoB66Bv"
      },
      "source": [
        "现在我们不再出错了--尽管代价是前导零。(我们将在本节稍后部分解决这个问题）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCdPqKfwkSRk"
      },
      "source": [
        "### Tens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dJZAhE57an3"
      },
      "source": [
        "既然我们已经解决了青少年的问题，那么就可以继续进行其余的十进制了。和许多语言一样，法语也采用（相当）规则的模式： 十位数+个位数 \"表示 20-100。事实上，我们可以用下面的模板概括 20-69 个数字：\n",
        "\n",
        "- 20 - vingt\n",
        "- 21 - vingt-et-un\n",
        "- 22 - vingt-deux\n",
        "- 23 - vingt-trois\n",
        "- 24 - vingt-quatre\n",
        "- 25 - vingt-cinq\n",
        "- 26 - vingt-six\n",
        "- 27 - vingt-sept\n",
        "- 28 - vingt-huit\n",
        "- 29 - vingt-neuf\n",
        "- 30 - trente\n",
        "- 31 - trente-et-un\n",
        "- 32 - trente-deux\n",
        "- 33 - trente-trois\n",
        "...\n",
        "- 40 - quarante\n",
        "...\n",
        "- 50 - cinquante\n",
        "...\n",
        "- 60 - soixante\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuaxVG35UKcs"
      },
      "source": [
        "为了适应这个模板，扩展 `tens` 是相当容易的：我们只需为 `tens place` 中的新术语扩展我们先前的 `string_map` 即可。在此基础上，我们再次将 `digits` WFST 连接起来（同时使用简单的 WFST 删除偶尔出现的“-et-”术语）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAnXlRkR32wt"
      },
      "outputs": [],
      "source": [
        "tens = pynini.string_map([\n",
        "\t\t\t\t(\"dix\", \"1\"),\n",
        "\t\t\t\t(\"vingt\",\"2\"),\n",
        "\t\t\t\t(\"trente\",\"3\"),\n",
        "\t\t\t\t(\"quarante\",\"4\"),\n",
        "\t\t\t\t(\"cinquante\",\"5\"),\n",
        "\t\t\t\t(\"soixante\",\"6\"),\n",
        "\t\t])\n",
        "\n",
        "graph_et = pynutil.delete(\"-et-\")\n",
        "\n",
        "tens = tens | pynutil.insert(\"0\") | tens + pynutil.delete(\"-\") | tens + graph_et\n",
        "\n",
        "graph_tens = tens + graph_digits\n",
        "graph_teens_and_tens = graph_tens | teens\n",
        "graph_all = graph_teens_and_tens | zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hJwqPDx8I2R"
      },
      "source": [
        "#### Special Cases: 70-99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvBLvJdY9XPA"
      },
      "source": [
        "然而，一旦超过 60 年代，情况就变得棘手了。在这里，标准法语采用的是一种臭名昭著的假十六进制（基数 20）系统。数字 70-99:\n",
        "\n",
        "- 70 - soixante-dix <- Literally in English: \"sixty-ten\"\n",
        "- 71 - soixante-et-onze <- Literally in English: \"sixty-and-eleven\"\n",
        "- 72 - soixante-douze\n",
        "- 73 - soixante-treize\n",
        "- 74 - soixante-quatorze\n",
        "- 75 - soixante-quinze\n",
        "- 76 - soixante-seize\n",
        "- 77 - soixante-dix-sept\n",
        "- 78 - soixante-dix-huit\n",
        "- 79 - soixante-dix-neuf\n",
        "- 80 - quatre-vingts <- Literally in English: \"four-twenties\"\n",
        "- 81 - quatre-vingt-un\n",
        "- 82 - quatre-vingt-deux\n",
        "- 83 - quatre-vingt-trois\n",
        "- 84 - quatre-vingt-quatre\n",
        "- 85 - quatre-vingt-cinq\n",
        "- 86 - quatre-vingt-six\n",
        "- 87 - quatre-vingt-sept\n",
        "- 88 - quatre-vingt-huit\n",
        "- 89 - quatre-vingt-nuef\n",
        "- 90 - quatre-vingt-dix <- Literally in English: \"four-twenties-ten\"\n",
        "- 91 - quatre-vingt-onze\n",
        "- 92 - quatre-vingt-douze\n",
        "- 93 - quatre-vingt-treize\n",
        "- 94 - quatre-vingt-quatorze\n",
        "- 95 - quatre-vingt-quinze\n",
        "- 96 - quatre-vingt-seize\n",
        "- 97 - quatre-vingt-dix-sept\n",
        "- 98 - quatre-vingt-dix-huit\n",
        "- 99 - quatre-vingt-dix-neuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQNiwFDyVV_3"
      },
      "source": [
        "和以前一样，我们希望尽可能多地利用冗余，同时又不产生额外的歧义，以免妨碍图的遍历。\n",
        "\n",
        "我们首先注意到，尽管重复了先前的词语，但 “quatre-vingt ”可以映射为 “8”，而不会产生歧义。这是因为，尽管 “quatre ”和 “vingt ”出现在我们之前的图中，但我们的 WFST 并没有按照这个确切的顺序为它们设置路径。因此，我们只需将其添加到 “tens ”中，就能立即提高 81-89 的覆盖率。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvJqaHhE9Wbd"
      },
      "outputs": [],
      "source": [
        "tens = pynini.string_map([\n",
        "\t\t\t\t(\"dix\", \"1\"),\n",
        "\t\t\t\t(\"vingt\",\"2\"),\n",
        "\t\t\t\t(\"trente\",\"3\"),\n",
        "\t\t\t\t(\"quarante\",\"4\"),\n",
        "\t\t\t\t(\"cinquante\",\"5\"),\n",
        "\t\t\t\t(\"soixante\",\"6\"),\n",
        "        (\"quatre-vingt\", \"8\")\n",
        "\t\t])\n",
        "tens = tens | pynutil.insert(\"0\") | tens + delete_hyphen | tens + graph_et\n",
        "graph_tens = tens + graph_digits\n",
        "graph_teens_and_tens = graph_tens | teens\n",
        "graph_all = graph_teens_and_tens | zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_DtcpZxZTzX"
      },
      "source": [
        "当然，现在我们允许出现："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2leANlDhCvj"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"quatre-vingt\", graph_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X_ef3sihCHH"
      },
      "source": [
        "这是无效的（法语在此使用复数 “quatre-vingt**s**”）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgKT903Y6rIQ"
      },
      "source": [
        "我们是否应该因此改变语法？这样的决定在很大程度上取决于您的预期实现和设计目标。如果您认为 “合法 ”标记的问题是上游模型的责任，那么就没有必要进行任何修改： “quatre-vingt ”作为一个独立的标记不会出现，因此不需要考虑输入问题。\n",
        "\n",
        "但是，如果您的 ITN 语法是为低保真 ASR 环境和/或错误转录会造成严重损失的环境（例如，驾驶方向、电话号码、银行业务的 ASR）而开发的，那么您可能需要谨慎行事。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_FghLT7jdY"
      },
      "source": [
        "如果我们想采用后者，就需要标明 “quatre-vingts ”**只**映射到 “80”。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JliFTF3mZSsJ"
      },
      "outputs": [],
      "source": [
        "quatre_vingt_plural = pynini.string_map([\n",
        "        (\"quatre-vingts\", \"80\")\n",
        "\t\t])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81_b3XPbicT1"
      },
      "source": [
        "而 “quatre vingt ”只能是非零的数字："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4_dmg6uin2j"
      },
      "outputs": [],
      "source": [
        "quatre_vingt_singular = pynini.string_map([\n",
        "        (\"quatre-vingt-\", \"8\") # Note that the hyphen can be assumed now\n",
        "\t\t])\n",
        "graph_digits_without_zero = pynini.string_map([\n",
        "\t\t\t\t(\"un\",\"1\"),\n",
        "\t\t\t\t(\"une\",\"1\"),\n",
        "\t\t\t\t(\"deux\",\"2\"),\n",
        "\t\t\t\t(\"trois\",\"3\"),\n",
        "\t\t\t\t(\"quatre\",\"4\"),\n",
        "\t\t\t\t(\"cinq\",\"5\"),\n",
        "\t\t\t\t(\"six\",\"6\"),\n",
        "\t\t\t\t(\"sept\",\"7\"),\n",
        "\t\t\t\t(\"huit\",\"8\"),\n",
        "\t\t\t\t(\"neuf\",\"9\")\n",
        "])\n",
        "graph_eighties = (quatre_vingt_singular + graph_digits_without_zero) | quatre_vingt_plural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL7jpekV8VgP"
      },
      "source": [
        "对于 `70`s'和 `90`s'，我们同样需要对它们的数列进行排他性配置，将识别 “onze”、“douze”、‘treize’......的数字改写为 `1,2,3....`（注意，我们必须将 `71` 和 `91` 分开，以管理 “soixante-**et**-onze ”和 “quatre-vingt-onze”）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3dYkwK29zCX"
      },
      "outputs": [],
      "source": [
        "seventy_and_ninety = pynini.string_map([\n",
        "        (\"soixante-dix\", \"70\"),\n",
        "        (\"quatre-vingt-dix\", \"90\"),\n",
        "\t\t])\n",
        "\n",
        "seventy_and_ninety_tens = pynini.string_map([\n",
        "        (\"soixante-\", \"7\"),\n",
        "        (\"quatre-vingt-\", \"9\"),\n",
        "\t\t])\n",
        "\n",
        "seventy_and_ninety_one = pynini.string_map([\n",
        "        (\"soixante-et-onze\", \"71\"),\n",
        "        (\"quatre-vingt-onze\", \"91\"),\n",
        "\t\t])\n",
        "\n",
        "seventy_and_ninety_digits = digits = pynini.string_map([\n",
        "\t\t\t\t(\"douze\",\"2\"),\n",
        "\t\t\t\t(\"treize\",\"3\"),\n",
        "\t\t\t\t(\"quatorze\",\"4\"),\n",
        "\t\t\t\t(\"quinze\",\"5\"),\n",
        "\t\t\t\t(\"seize\",\"6\"),\n",
        "\t\t\t\t(\"dix-sept\",\"7\"), # For 97-99, digits are used as normal.\n",
        "\t\t\t\t(\"dix-huit\",\"8\"),\n",
        "\t\t\t\t(\"dix-neuf\",\"9\")\n",
        "])\n",
        "\n",
        "graph_seventies_and_nineties = (seventy_and_ninety_tens + seventy_and_ninety_digits) | seventy_and_ninety | seventy_and_ninety_one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NCrCwEH9HVg"
      },
      "source": [
        "现在，我们将它们与最初的 “tens ”系列结合起来："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psGCgxaH-btn"
      },
      "outputs": [],
      "source": [
        "tens = pynini.string_map([\n",
        "\t\t\t\t(\"dix\", \"1\"),\n",
        "\t\t\t\t(\"vingt\",\"2\"),\n",
        "\t\t\t\t(\"trente\",\"3\"),\n",
        "\t\t\t\t(\"quarante\",\"4\"),\n",
        "\t\t\t\t(\"cinquante\",\"5\"),\n",
        "\t\t\t\t(\"soixante\",\"6\"),\n",
        "\t\t])\n",
        "tens = tens | pynutil.insert(\"0\") | tens + delete_hyphen | tens + graph_et\n",
        "\n",
        "graph_tens = tens + graph_digits\n",
        "graph_tens_with_special_cases = graph_tens | graph_seventies_and_nineties | graph_eighties\n",
        "graph_teens_and_tens = graph_tens_with_special_cases | teens\n",
        "graph_all = graph_teens_and_tens | zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWjSAGRX_s0H"
      },
      "source": [
        "确保测试用例有效："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kapWmgos-xcn"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"quatre-vingt-treize\" , graph_all)\n",
        "apply_fst(\"quatre-vingts\", graph_all)\n",
        "apply_fst(\"quatre-vingt-deux\", graph_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNUepfKZ_vS_"
      },
      "source": [
        "而其他情况则如期失败："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo2pCOXGAgYn"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"quatre-vingt\", graph_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VPuCTTtigh-"
      },
      "source": [
        "当然，我们还可以通过其他方式重新配置语法：我们可以简单地为十的倍数（“10,20,30...”）和所有出现“-et-”的情况（“21,31,41,51...91”）制作特定的图形。\n",
        "\n",
        "但这忽略了一个更重要的问题：这些东西有必要存在吗？这些额外的语法所做的只是扩大了覆盖范围，增加了 30 个红心。而且它们仍然没有排除所有错误输入！请注意以下情况："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KICvpeewCFyH"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"dix-une\", graph_all)  # supposed to be \"onze\"\n",
        "apply_fst(\"dix-deux\", graph_all) # supposed to be \"douze\"\n",
        "apply_fst(\"vingt-un\", graph_all)  # supposed to be \"vingt-et-un\"\n",
        "apply_fst(\"trente-un\", graph_all) # supposed to be \"trente-et-un\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D130jIVCLp2"
      },
      "source": [
        "我们*还*需要解决可能出现的边缘情况！\n",
        "\n",
        "所有这些都说明，在构建之前了解输入域是非常必要的，因为一些小的决定很容易决定以后的输出范围。\n",
        "\n",
        "事实上，如果你特别在意限制输入的可能性，那么只需将所有唯一选项写入 “字符串映射 ”即可。这样做虽然略显笨拙，但无疑有助于控制输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSp9w5ayA9ii"
      },
      "outputs": [],
      "source": [
        "graph_tens_special = pynini.string_map([\n",
        "\t\t\t\t(\"soixante-dix\", \"70\"),\n",
        "\t\t\t\t(\"soixante-et-onze\",\"71\"),\n",
        "        (\"soixante-douze\",\"72\"),\n",
        "\t\t\t\t(\"soixante-treize\",\"73\"),\n",
        "\t\t\t\t(\"soizante-quatorze\",\"74\"),\n",
        "\t\t\t\t(\"soixante-quinze\",\"75\"),\n",
        "\t\t\t\t(\"soixante-seize\",\"76\"),\n",
        "        (\"soixante-dix-sept\",\"77\"),\n",
        "        (\"soixante-dix-huit\",\"78\"),\n",
        "\t\t\t\t(\"soixante-dix-neuf\",\"79\"),\n",
        "        (\"quatre-vingts\", \"80\"),\n",
        "        (\"quatre-vingt-un\", \"81\"),\n",
        "        (\"quatre-vingt-une\", \"81\"),\n",
        "\t\t\t\t(\"quatre-vingt-deux\",\"82\"),\n",
        "        (\"quatre-vingt-trois\",\"83\"),\n",
        "        (\"quatre-vingt-quatre\",\"84\"),\n",
        "        (\"quatre-vingt-cinq\",\"85\"),\n",
        "        (\"quatre-vingt-six\",\"86\"),\n",
        "        (\"quatre-vingt-sept\",\"87\"),\n",
        "        (\"quatre-vingt-huit\",\"88\"),\n",
        "        (\"quatre-vingt-neuf\",\"89\"),\n",
        "        (\"quatre-vingt-dix\",\"90\"),\n",
        "        (\"quatre-vingt-onze\",\"91\"),\n",
        "        (\"quatre-vingt-douze\",\"92\"),\n",
        "        (\"quatre-vingt-treize\",\"93\"),\n",
        "        (\"quatre-vingt-quatorze\",\"94\"),\n",
        "        (\"quatre-vingt-quinze\",\"95\"),\n",
        "        (\"quatre-vingt-sieze\",\"96\"),\n",
        "        (\"quatre-vingt-dix-sept\",\"97\"),\n",
        "        (\"quatre-vingt-dix-huit\",\"98\"),\n",
        "        (\"quatre-vingt-dix-neuf\",\"99\"),])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUPs1qOUg-hE"
      },
      "source": [
        "哪个效率更高？这还是取决于你的语言和实现。如果我们简单地将每个图形及其状态数可视化，那么："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ9GsIkNzxsU"
      },
      "outputs": [],
      "source": [
        "constructed_version = (graph_seventies_and_nineties | graph_eighties)\n",
        "constructed_version.num_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsgdu5TYx09_"
      },
      "outputs": [],
      "source": [
        "string_map_version = graph_tens_special\n",
        "string_map_version.num_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jzn_U7s0Sit"
      },
      "source": [
        "我们可以看到，它们的状态数（图顶点）几乎相等。然而，如果我们使用 “pynini.optimize”--一种调用一套 WFST 最小化算法的方法："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YtqhOY90iF0"
      },
      "outputs": [],
      "source": [
        "constructed_version.optimize()\n",
        "constructed_version.num_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y93SqnOf0qa8"
      },
      "outputs": [],
      "source": [
        "string_map_version.optimize()\n",
        "string_map_version.num_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cTdQj9L0xhl"
      },
      "source": [
        "我们看到后者的图形顶点数量要大得多。\n",
        "\n",
        "因此，决定将取决于您的 ITN 需求、语言、对效率的关注以及设计理念。此外，即使是语言方言的决定也会产生影响。\n",
        "(例如，比利时、加拿大和瑞士的法语方言将放弃十进制模式中的维格小数系统元素）。\n",
        "\n",
        ">**_Note:_**\n",
        ">虽然 “nemo_text_processing ”语法旨在尽量减少无效生成，但它们假定输入标记是目标语言的有效字符串。(例如，将 \"quatre-vingt \"映射为 \"80 \"是允许的，因为它不可能出现在有效的法语字符串中。）  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1djCnvY3CjW"
      },
      "source": [
        "For more information on optimization algorithms for WFSTs, please see:\n",
        "\n",
        "- [M. Mohri,\"Generic epsilon-removal and input epsilon-normalization algorithms for weighted transducers\"](https://cs.nyu.edu/~mohri/pub/ijfcs.pdf)\n",
        "- [M. Mohri, \"Weighted automata algorithms\"](https://cs.nyu.edu/~mohri/pub/hwa.pdf)\n",
        "- [K. Thompson, \"Programming techniques: regular expression search algorithm\"](http://www.oilshell.org/archive/Thompson-1968.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkHNtpvjkSRn"
      },
      "source": [
        "### Hundreds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqPUdVBbi6gU"
      },
      "source": [
        "\n",
        "如果是三位数的红心数（“百”），您所选择的语言可能会变得更有规律。例如，几乎所有法语数字“>100 ”都遵循以下规则：\n",
        "\n",
        "- 数字从_1_到_9 + “百 ”字 + 数字从_1_到_99\n",
        "\n",
        "例如\n",
        "- `203` - “deux-cent-trois” (“二分三”)\n",
        "- `530` - “cinq-cent-trente” (五分三)\n",
        "- `880` - “huit-cent-quatre-vingt”\n",
        "\n",
        "因此，我们可以将简单的 `hundreds` WFST 写成这样："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOt-gc-FiF-X"
      },
      "outputs": [],
      "source": [
        "hundreds = graph_digits + delete_hyphen + pynutil.delete(\"cent\") + delete_hyphen + graph_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fyn1uL_NoEiz"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"deux-cent-trois\", hundreds)\n",
        "apply_fst(\"huit-cent-quatre-vingts\", hundreds)\n",
        "apply_fst(\"cinq-cent-trente\" , hundreds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDjq_KfnoD5C"
      },
      "source": [
        "Indeed, the use of French only presents two complications:\n",
        "- French uses *only* the word \"cent\" for `100`. (Instead of \"un cent\".)\n",
        "- 'Pure' multiples of a hundred (`200,300,400....`) use the plural \"cents\".\n",
        "\n",
        "The second one is the easier of the two so let's start there. There are actually two options open to us. First, we could treat \"cents\" the same way as we did \"cent\" in the base case and simply delete it. From there, the lack of any following inputs will allow the WFST to insert the trailing zeroes as appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2F-sumbxqLE"
      },
      "outputs": [],
      "source": [
        "cents = pynini.accep(\"cent\") | pynini.accep(\"cents\") # Creates a Finite State (Accep)tor, mapping inputs back to themselves\n",
        "hundreds = graph_digits + delete_hyphen + pynutil.delete(cents) + delete_hyphen + graph_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VisQu_Etx-QB"
      },
      "source": [
        "Or we can use it as a cue to 'shortcut' the WFST to immediately insert zeroes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VspiTN5Vxxjl"
      },
      "outputs": [],
      "source": [
        "graph_cents = pynini.cross(\"cents\", \"00\") # Creates a single input-output mapping\n",
        "hundreds = graph_digits + delete_hyphen + ((pynutil.delete(\"cent\") + delete_hyphen + graph_all) | graph_cents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meVn5BiyyX5v"
      },
      "source": [
        "For the case of solitary \"cent\", we need to make sure our output is `1` only in the case that no digit precedes the occurrence. Here we need to be confident in the structure of our WFST and that any possible ambiguity has been dealt with by this point. (Something to keep in mind as we move to the thousands.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "277Z-zLWyWAf"
      },
      "outputs": [],
      "source": [
        "graph_cent = pynini.cross(\"cent\", \"1\")\n",
        "graph_hundreds_first_digit = (graph_digits + delete_hyphen + pynutil.delete(cents)) | graph_cent\n",
        "graph_hundreds = graph_hundreds_first_digit + delete_hyphen + graph_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNZlJsvS_Yvt"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"trois-cents\", graph_hundreds)\n",
        "apply_fst(\"cent\", graph_hundreds)\n",
        "apply_fst(\"cent-trois\", graph_hundreds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RWNi6jxkSRn"
      },
      "source": [
        "### Thousands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Dy5slLzp-K"
      },
      "source": [
        "For quite a few languages, managing the WFST for the thousands place is the last aspect to figure out, as the higher powers of ten reuse the same schema. (For those working with counting systems that reserve special terms for \"ten-thousand\" (e.g. Chinese derived counting systems), you may need to extend unique coverage to the next power of ten.)\n",
        "\n",
        "For French, the question of thousands is rather simple: `digits_from_1_to_999 + mille + digits_from_1_to_999`\n",
        "\n",
        "With only the exception that any expression of one thousand drops a leading digit.\n",
        "- `1,000` -> \"mille\"\n",
        "- `1,001` ->  \"mille-un\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvsnAAiPzlu_"
      },
      "outputs": [],
      "source": [
        "graph_one_thousand = pynini.cross(\"mille\", \"1\")\n",
        "graph_many_thousand = graph_hundreds + delete_hyphen + pynutil.delete(\"mille\")\n",
        "\n",
        "graph_thousands = (graph_one_thousand | graph_many_thousand) + delete_hyphen + graph_hundreds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3m9TG7Y4tkl"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"cent-mille-deux-cents\", graph_thousands)\n",
        "apply_fst(\"deux-cent-mille-deux-cents\", graph_thousands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoevSTZGGT17"
      },
      "source": [
        "### Weighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2gcVIZM0-iv"
      },
      "source": [
        "Question: will this cover all our grammar so far? (Hint: what assumptions were made about \"cent\"/\"cents\"?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCFtPhr1BjAc"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"deux-mille-un\", graph_thousands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne-7L9Cd4t-8"
      },
      "source": [
        "Once again, we need to introduce the possibility of the prior power of ten not occurring in the string. There must be an option for simply inserting a string of `0` in place of the omitted \"cent\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iockqXdn-aG4"
      },
      "source": [
        "Further, we want to be careful with how cavalier we have been with insertions. Consider the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxJlSnj2-Xw3"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"mille-cent-un\", graph_thousands) # Should be 1101\n",
        "apply_fst(\"mille-cent\", graph_thousands) # 1100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq5zEayA-kOx"
      },
      "source": [
        "It appears that our WFST has developed a tendency to simply 'ignore' some of these higher powers. Let us return to our code for `graph_hundreds` and `graph_thousands`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2aV1KQ4-1iP"
      },
      "outputs": [],
      "source": [
        "graph_cents = pynini.cross(\"cents\", \"00\")\n",
        "graph_cent = pynini.cross(\"cent\", \"1\")\n",
        "graph_hundreds_first_digit = (graph_digits + delete_hyphen + pynutil.delete(cents))  | graph_cent\n",
        "graph_hundreds = (graph_hundreds_first_digit + delete_hyphen | pynutil.insert(\"0\")) + graph_all\n",
        "\n",
        "graph_one_thousand = pynini.cross(\"mille\", \"1\")\n",
        "graph_many_thousand = graph_hundreds + delete_hyphen + pynutil.delete(\"mille\")\n",
        "graph_thousands = (graph_one_thousand | graph_many_thousand) + delete_hyphen + graph_hundreds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9avwOIkk-9qt"
      },
      "source": [
        "Recall that throughout we have provided options for simply inserting zeroes in the case of omitted numbers? That tendency has finally caught up with us. The use of our previous `graph_hundreds` in `graph_many_thousands` now allows our graph to insert a string of `0`'s without penalty.\n",
        "\n",
        "You may note that this is very similar to the \"cents\" example brought up at the beginning, presenting a similar solution. We can control this output by making it too costly to traverse unless absolutely necessary for the graph. This can be accomplished simply by appending a weight to the insertion for hundreds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQG3j0U8CUAQ"
      },
      "outputs": [],
      "source": [
        "graph_hundreds = (graph_hundreds_first_digit + delete_hyphen | pynutil.insert(\"0\", weight=.1)) + graph_all\n",
        "\n",
        "graph_one_thousand = pynini.cross(\"mille\", \"1\")\n",
        "graph_many_thousand = graph_hundreds + delete_hyphen + pynutil.delete(\"mille\")\n",
        "graph_thousands = (graph_one_thousand | graph_many_thousand) + delete_hyphen + graph_hundreds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNHhrYZ7Ca58"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"mille-cent-un\", graph_thousands)\n",
        "apply_fst(\"mille-cent\", graph_thousands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51yPEaf2EkbD"
      },
      "source": [
        "Why choose a weight of `.1`? Quite simply: it's arbitrary. As mentioned earlier, the default graph in `pynini` is a tropical semiring, which uses the `min` function to select among two arcs for path traversal. Since all our paths so far are weight `0`, any positive value will ensure that it is a last option among path traversal. (Note, this conversely entails any negative weight path will be prioritized.)\n",
        "\n",
        "That we chose this number as a small value comes from a place of caution: the tropical semiring uses an additive function to calculate the total weight of an entire path to traverse a WFST. As our grammars can easily become massive, this means that small weights can have major impact down the line. Further, by constraining path weights to small values, we can have general certainty towards the maximum weight of any individual graph, allowing us to add constraints regarding maximum token length and token hierarchy. (As explained in [later sections](#classifyweights).) As such, when using weights in a localized setting, it is best to use small values to avoid unforeseen escalation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iScKgvRxGt-B"
      },
      "source": [
        "### Higher Powers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtHEd6OE2WSg"
      },
      "source": [
        "At this point, we can propose a general heuristic with escalating to higher powers of ten: they always need a way for their absence to be accommodated in the WFST. Further, they require some weighting to prevent this absence from developing into a string of omitted values. To avoid further bumps, we'll take care of this now with `graph_thousands`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZMN7wcE2lH5"
      },
      "outputs": [],
      "source": [
        "graph_one_thousand = pynini.cross(\"mille\", \"1\")\n",
        "graph_many_thousand = graph_hundreds + delete_hyphen + pynutil.delete(\"mille\")\n",
        "graph_thousands = (graph_one_thousand | graph_many_thousand | pynutil.insert(\"000\", weight=.001)) + delete_hyphen + graph_hundreds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkc3LIH824P7"
      },
      "source": [
        "\n",
        "For the rest of French (and many other languages), the rest of the work is simply repeating the prior pattern for the thousands element:\n",
        "`hundreds + word_for_higher_power + hundreds.....` Of course there will be some variation in this schema, but the recursion should be regular. (It is rather rare that languages appropriate unique forms for these higher counts.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGnK4ARX4Nay"
      },
      "source": [
        "To finish French, we can list off the following equivalent for higher powers of ten:\n",
        "- `million` - \"million/millions\"\n",
        "- `billion` - \"milliard/milliards\"\n",
        "- `trillion` - \"billion/billions\"\n",
        "\n",
        "Like the \"cent/cents\" rule, these values alternate with a plural form in the case of multiples of the value. Writing them out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBu7-dub4vxz"
      },
      "outputs": [],
      "source": [
        "millions = pynini.accep(\"million\") | pynini.accep(\"millions\")\n",
        "graph_millions = ((graph_hundreds + delete_hyphen + pynutil.delete(millions) + delete_hyphen) | pynutil.insert(\"000\", weight=.1) # We need three zeroes now\n",
        " ) + graph_thousands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmMeCHXr5Bb5"
      },
      "outputs": [],
      "source": [
        "billions = pynini.accep(\"milliards\") | pynini.accep(\"milliard\")\n",
        "graph_billions = ((graph_hundreds +  delete_hyphen + pynutil.delete(billions) + delete_hyphen)| pynutil.insert(\"000\",weight=.1) # We need three zeroes now\n",
        " ) + graph_millions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIRIeQEg5B0J"
      },
      "outputs": [],
      "source": [
        "trillions = pynini.accep(\"billion\") | pynini.accep(\"billions\")\n",
        "graph_trillions = ((graph_hundreds + delete_hyphen + pynutil.delete(trillions) + delete_hyphen) | pynutil.insert(\"000\",weight=.1) # We need three zeroes now\n",
        " ) + graph_billions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRNUPx-15J1v"
      },
      "source": [
        "Bringing all together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dLOWm_B5SwQ"
      },
      "outputs": [],
      "source": [
        "graph = graph_trillions | zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBFE3BrN6IPR"
      },
      "source": [
        "Let's try it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lWwtR1S6LI4"
      },
      "outputs": [],
      "source": [
        "example = \"deux-cent-milliard-quatre-million-deux-cent-quatre-vingt-onze\"\n",
        "apply_fst(example, graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML4U8dqSkSRp"
      },
      "source": [
        "### Finishing Touches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w3KgX6C6mff"
      },
      "source": [
        "Now that we have our cardinal in place, we can take care of that stylistic issue of the leading zeroes. For this, we want to develop a 'filter' that deletes all zeroes preceding the first non-zero in the string, and leave the rest 'as is.'\n",
        "\n",
        "First let us create the filter by calling on `NEMO_DIGIT`- a `graph_util` WFST that only permits digits as input. With it, we'll create a WFST that will delete all leading zeroes in a sting. We then compose this (using `@`) onto our original graph, creating a new graph that accepts inputs from our original but produces only the outputs of `clean_cardinal`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA4VnRe6FO-2"
      },
      "outputs": [],
      "source": [
        "delete_leading_zeroes = pynutil.delete(pynini.closure(\"0\")) # will delete all zeroes under closure. Equivalent to regex * operator\n",
        "stop_at_non_zero = pynini.difference(NEMO_DIGIT, \"0\") # creates a graph that accepts all input-outputs from NEMO_DIGIT except 0\n",
        "rest_of_cardinal = pynini.closure(NEMO_DIGIT) # accepts all digits that may follow\n",
        "\n",
        "clean_cardinal = delete_leading_zeroes + stop_at_non_zero + rest_of_cardinal\n",
        "clean_cardinal = clean_cardinal | \"0\" # We don't want to ignore the occurrence of zero\n",
        "\n",
        "graph = graph @ clean_cardinal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piP9nqQkHpo3"
      },
      "source": [
        "Now our WFST will output our numbers as normal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnQ9odSpIAB7"
      },
      "outputs": [],
      "source": [
        "apply_fst(example, graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M8e0xd8kSRp"
      },
      "source": [
        "### Final Notes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7zt8lVsK2rY"
      },
      "source": [
        "We have finally formulated a grammar that will process French cardinals into numeric representation. Of course, not every grammar you write will be for French. But several of the principles we've worked through will be invaluable in your own development. Before moving on, here's a quick summary of (almost) universal points to take away for WFST construction.\n",
        "- Decide at the beginning of construction the level of constraint you wish for your grammar. Is it necessary to have a specific domain or can you rely on upstream models to narrow your input possibilities for you?\n",
        "- Work iteratively upwards from the smallest place value of your numeric system. This will assist you in forming building blocks for larger values.\n",
        "- Always allow for the possibility of omission of previous place values. (Not every number in the thousands will contain mention of the hundreds place.)\n",
        "- For each place value, consider how the sub-grammar will affect the preceding and following place values. Are there exceptions that you've built into the grammar that may become problematic later on?\n",
        "- Utilize weights for default insertions to limit path traversal to only final options. When doing so, use small values to avoid escalating problems in your larger grammar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvyHg1bQIIHD"
      },
      "source": [
        "With that handled, we can move on to converting this grammar into a Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ1YJUvhIZwm"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2L2x0crIeXQ"
      },
      "source": [
        "Now that we have a grammar that will convert individual tokens into number strings, we now want to focus on building it into a classifier to properly tag candidate tokens. This requires a couple of properties:\n",
        "- It recognizes any valid token and permits traversal through the WFST graph\n",
        "- Conversely, it does not allow invalid tokens to traverse the WFST graph\n",
        "- It properly disambiguates overlap among ambiguous cases\n",
        "- It attributes the proper attributes to a classified token\n",
        "\n",
        "While this seems like a lot, in practice this just means that your grammar will need a few more tweaks to improve exclusivity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArEYn7RWKcYI"
      },
      "source": [
        "NeMo ITN performs token classification through a series of `GraphFst` classes and assumes deployment of your grammars through an object that inherits from this class. As such, you will need to instantiate your grammar as a `CardinalFST`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWgMSybqLqiS"
      },
      "outputs": [],
      "source": [
        "class CardinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"cardinal\", kind=\"classify\")\n",
        "        # Rest of the grammar here\n",
        "        # .......\n",
        "        #........."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIE8dNQlL52G"
      },
      "source": [
        "While the naming convention may vary, the `name` and `kind` properties must be set accordingly to permit Sparrowhawk integration.\n",
        "\n",
        "Further, the resulting graph must produce the classified token within the following format:\n",
        "`token { cardinal { integer: \"DIGIT_STRING\" } }`\n",
        "\n",
        "This is accomplished by a series of string insertions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC_c64KSNTCg"
      },
      "outputs": [],
      "source": [
        "class CardinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"cardinal\", kind=\"classify\")\n",
        "        # Rest of the grammar here\n",
        "        # .......\n",
        "        #.........\n",
        "        self.fst = pynutil.insert(\"integer: \\\"\") + graph + pynutil.insert(\"\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLQxOSzOK1F"
      },
      "source": [
        "Followed by a call of the parent `GraphFst.add_tokens()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz-UXFipORps"
      },
      "outputs": [],
      "source": [
        "class CardinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"cardinal\", kind=\"classify\")\n",
        "        # Rest of the grammar here\n",
        "        # .......\n",
        "        #.........\n",
        "        self.fst = pynutil.insert(\"integer: \\\"\") + graph + pynutil.insert(\"\\\"\")\n",
        "        final_graph = self.add_tokens(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh23S7BHOY0r"
      },
      "source": [
        "Which will insert the appropriate formatting. Note that this formatting must be exact: a single space must follow each field name and each value must be within escaped double quotes.\n",
        "\n",
        "In the event that you also wish for `CardinalFst` to indicate negative values, the optional `negative: ` property may be used.\n",
        "\n",
        "For instance, French indicates negative values by prefacing the quantity with \"moins.\" As such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JbTn35cOx0k"
      },
      "outputs": [],
      "source": [
        "optional_minus_graph = pynini.closure(\n",
        "    pynutil.insert(\"negative: \") + pynini.cross(\"moins\", \"\\\"-\\\"\") + \" \", 0, 1 # Note the extra space to separate the value from the integer field\n",
        ")\n",
        "\n",
        "final_graph = optional_minus_graph + pynutil.insert(\"integer: \\\"\") + graph + pynutil.insert(\"\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCs1048v6N0K"
      },
      "source": [
        "All together, your `CardinalFst` ultimately serves as a wrapper for your grammar, save with the addition of a few insertions to assist processing:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo6uEz1s5TJY"
      },
      "outputs": [],
      "source": [
        "class CardinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"cardinal\", kind=\"classify\")\n",
        "\n",
        "        ### Cardinal Grammar....\n",
        "        ### .....\n",
        "        graph = graph_trillions | zero\n",
        "\n",
        "        ### Formatting grammar....\n",
        "        ### .....\n",
        "        graph = graph @ clean_cardinal\n",
        "\n",
        "        ### Token insertion\n",
        "        optional_minus_graph = pynini.closure(\n",
        "        pynutil.insert(\"negative: \") + pynini.cross(\"moins\", \"\\\"-\\\"\") + \" \", 0, 1\n",
        "          )\n",
        "\n",
        "        final_graph = optional_minus_graph + pynutil.insert(\"integer: \\\"\") + graph + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        final_graph = self.add_tokens(final_graph) # inserts the cardinal tag\n",
        "\n",
        "        self.fst = final_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFIMdLCoZzLK"
      },
      "source": [
        "Let's see a demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CF6Iz9NZ7R_"
      },
      "outputs": [],
      "source": [
        "cardinal = CardinalFst().fst\n",
        "\n",
        "example = \"moins deux-cent-quatre\"\n",
        "\n",
        "apply_fst(example, cardinal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a22ruTj5kSRq"
      },
      "source": [
        "## Verbalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvUqpC_Q8FSt"
      },
      "source": [
        "The verbalizer can be both the most crucial and simplest part of building each grammar. On one hand, it is the component that finalizes all of your previous work. If it is unable to properly normalize your text, everything has been for naught.\n",
        "\n",
        "On the other hand, your previous work has vastly limited the unpredictability of your input. Recall from our initial demonstration of the classifier-verbalizer system that and input like <<le premier juillet il a mangé trente-cinq pommes>> becomes:\n",
        "\n",
        "- `tokens { name: \"le\" }`\n",
        "- `tokens { date { day: \"1\" month: \"juillet\" }`  \n",
        "- `tokens { name: \"il\" }`\n",
        "- `tokens { name: \"a\" }`\n",
        "- `tokens { name: \"mangé\" }`\n",
        "- `tokens { cardinal { integer: \"35\" } }`\n",
        "- `tokens { name: \"pommes\" }`\n",
        "\n",
        "Part of the purpose of the two stage set-up is that the input space for each verbalizer is obvious: it's simply the name of its semiotic class. As such, we only need to write our grammar to recognize its class, remove tokens accordingly, and then manage the attributes of each semiotic token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et1GgmBuAWzY"
      },
      "source": [
        "We will begin as we did with our classifier and create a class to inherit from the `GraphFST` utility class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNKpgWtkAgEW"
      },
      "outputs": [],
      "source": [
        "class CardinalFst(GraphFst):\n",
        "  def __init__(self):\n",
        "    super().__init__(name=\"cardinal\", kind=\"verbalize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyAV39NsAqSN"
      },
      "source": [
        "One of the useful aspects of the `GraphFst` utility is that it already possesses a built in graph that will recognize and remove semiotic tokens: `delete_tokens`. As such we need only concern ourselves with managing the properties of the Cardinal class:\n",
        "- `integers`\n",
        "- `negative`\n",
        "\n",
        "Here, the desired written format of your chosen language will dictate how you proceed. For French, we have the following rules for Cardinal numbers:\n",
        "- A negative sign is written before the numeral.\n",
        "- Cardinal numbers representing quantities (e.g. \"mille euros\"/ \"one thousand dollars\") are written with spaces in-between every three digits. (e.g. `1 000`)\n",
        "- Cardinal numbers representing place in a sequence or addresses (\"page mille\"/\"page one thousand\") are written without spacing. (`1000`)\n",
        "\n",
        "The first property seems easy enough to handle: write a grammar that simply removes the `negative` formatting, leaving only `-`. (Recall that our Classifier only inserted the string if it was present.)\n",
        "\n",
        "For the final two, we may note that our intention to develop WFSTs for the Decimal, Measure, and Money classes already will cover most desired quantities. As such, we can leave the issue of spacing to those instances and let the Cardinal WFST default to the non-spacing case. (Note that this will be helpful with Time, Date, Telephone, Electronic, and Ordinal classes as they will not use the spacing format either. It is usually better to reserve specific formatting rules to other classes and let the Cardinal serve as a default.)\n",
        "\n",
        "As such, we just need our WFST to remove the `integer` property and `negative` property (if it occurs). These can be managed through the `pynutil.delete` function, as seen in the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MF2I6SLU7nf"
      },
      "outputs": [],
      "source": [
        "class CardinalFst(GraphFst):\n",
        "  def __init__(self):\n",
        "    super().__init__(name=\"cardinal\", kind=\"verbalize\")\n",
        "\n",
        "    # Removes the negative attribute and leaves the sign if occurs\n",
        "    optional_sign = pynini.closure(\n",
        "            pynutil.delete(\"negative:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.accep(\"-\")\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + delete_space,\n",
        "            0,\n",
        "            1,\n",
        "        )\n",
        "\n",
        "    # removes integer aspect\n",
        "    graph = (\n",
        "            pynutil.delete(\"integer:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_DIGIT, 1) # Accepts at least one digit\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "\n",
        "    graph = optional_sign + graph # concatenates two properties\n",
        "\n",
        "    delete_tokens = self.delete_tokens(graph) # removes semiotic class tag\n",
        "\n",
        "    self.fst = delete_tokens.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSX2KlZJbRAA"
      },
      "source": [
        "Let's see if it will properly render a given token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxaLm2k0bYIJ"
      },
      "outputs": [],
      "source": [
        "cardinal = CardinalFst().fst\n",
        "example = 'cardinal { negative: \"-\" integer: \"204\" }'\n",
        "\n",
        "apply_fst(example, cardinal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc0-QCBHWg-8"
      },
      "source": [
        "That's it! We've now completed all aspects of our `CardinalFst` from grammar writing to Verbalization. While we still have quite a few semiotic classes left, you will find that they build off the `CardinalFst` quite easily, making progression much simpler and straightforward.\n",
        "\n",
        ">**_Note:_**\n",
        ">- `delete_tokens` is called on the completed graph, despite the token class occurring first in the tokenized string. This is because the function intersects with an initial WFST that deletes the tags. As such, the function must be passed a completed graph.\n",
        ">- In our initial example, all tokens were enclosed within a `token` category. Insertion and deletion of this category is managed by the main [Classifier](#tokenize-and-classify) and [Verbalizer](#verbalize-and-verbalize-final) respectively and is not a concern during individual class grammar development.\n",
        ">- Earlier in the tutorial we noted that NeMo ITN permutates all WFSTs unless the `preserve_order` tag is passed as part of the Classifier. This allows you to ignore possible variation in designing the verbalizer and focus on whatever form of processing is easiest for the grammar. That is, the decision to process the `negative` property before the `integer` property is not chosen because of a consequence of the French language but instead because it is easier to write out with `pynini`.\n",
        ">- Conversely, if your language is completely invariant in this regard, it may be more efficient to pass `preserve_order` through the Classifier and manage the property here in the Verbalizer. This allows NeMo ITN to avoid building states and arcs for each permutation, reducing graph size and compiling time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFUrbSdJ8Wk7"
      },
      "source": [
        "# Ordinal WFST <a id=\"ordinal-wfst\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1b0Z7f5Z9Ar"
      },
      "source": [
        "Ordinals is the class of numbers used for enumerating order or placement of entities in a series. In some languages, they are simply derivations of cardinal numbers. For instance, English enumerates order as `first, second, third, fourth, fifth....` After the third ordinal, they become a regular pattern of `cardinal + 'th'`.\n",
        "\n",
        "Meanwhile, other languages may reserve specific counting systems for ordinals. For example, while Korean uses a Chinese derived counting system for several Cardinal related tasks, it uses derivations from a native counting system for ordering:\n",
        "\n",
        "**Cardinal**/**Ordinal** = **English**\n",
        "- il/cheot-jae = \"First\"\n",
        "- i/dul-jae = \"Second\"\n",
        "- sam/set-jae = \"Third\"\n",
        "- sa/net-jae = \"Fourth\"\n",
        "- o/daseot-jae = \"Fifth\"\n",
        "\n",
        "If your language is of the latter variety, you will likely need to begin development of Ordinal WFST by repeating Cardinal WFST development before proceeding. (Or make it part of your previous Cardinal WFST and combining with a `union` operation.) While you can extend coverage to the level of Cardinal WFST, you will find most Ordinals to be sufficiently covered by only enumerating to a few hundreds. (e.g. Is it common in your language to speak of the \"one millionth\" in an order and/or write out `1,000,000th`?)\n",
        "\n",
        "For this portion of the tutorial, we will focus on the first type of ordinals - those that primarily derived by altering Cardinals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq_xA8NPiANw"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhjcQS6oiD_w"
      },
      "source": [
        "Continuing with our example language, we first begin by laying out our expected inputs and pinpointing a regular pattern to guide our WFSTs. We note the following examples:\n",
        "\n",
        "  **English = French**\n",
        "  - \"first\" = \"premier/première\"\n",
        "  - \"second\" = \"second/seconde/deuxième\"\n",
        "  - \"third\" = \"troisième\"\n",
        "  - \"fourth\" = \"quatrième\"\n",
        "  - \"fifth\" = \"cinquième\"\n",
        "  - \"sixth\" = \"sixième\"\n",
        "  - \"seventh\" = \"septième\"\n",
        "\n",
        "From our examples inputs, it appears that spelling of French Ordinals follows a general format of: `cardinal + ième`. The only exceptions appear to be in the case of the first and second Ordinals - for which completely different roots appear - and the fourth and the fifth Ordinals - where the former drops the \"e\" at the end of the root (`quatre -> quatr`) and the latter appends a \"u\" (`cinq -> cinqu`).\n",
        "\n",
        "For the expected outputs, we observe the following examples:\n",
        "  - \"premier/première\" -> `1ᵉʳ/1ʳᵉ`\n",
        "  - \"second/seconde\" -> `2ᵈ/2ᵈᵉ`\n",
        "  - \"deuxième\" -> `2ᵉ`\n",
        "  - \"troisième\" -> `3ᵉ`\n",
        "  - \"quatrième\" -> `4ᵉ`\n",
        "  - \"cinquième\" -> `5ᵉ`\n",
        "  - \"sixième\" -> `6ᵉ`\n",
        "  - \"septième\" -> `7ᵉ`\n",
        "\n",
        "It appears that the output is simply the cardinal number of the root with an associated superscript. Since we have already constructed the Cardinal WFST, this means that the job of constructing an Ordinal WFST is simply a case of recognizing the cardinal root for the input and then utilizing a preconstructed Cardinal grammar to render the proper form alongside an associated superscript. That is, our tasks are to:\n",
        "- Identify the proper superscript for the ordinal\n",
        "- Change the ordinal back into a cardinal\n",
        "- Use the Cardinal WFST to transform the cardinal into normalized form\n",
        "- Properly render the ordinal using the normalized cardinal and proper superscript\n",
        "\n",
        "As information regarding the superscript will need to be conveyed through development of the Classifier, we will begin with creating the grammar necessary for rendering the ordinal as its cardinal root.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUVZhiwT7hE"
      },
      "source": [
        "### Stripping Suffixes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nw0_lOTsEik"
      },
      "source": [
        "Since French forms Ordinals by appending a suffix to Cardinals, we should start by creating a WFST to remove the suffix. Assuming that our grammar processes one token at a time, this means that we just need an WFST that will accept all tokens that end with \"ième\" and then delete the suffix from that token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk89LhsxsHTO"
      },
      "outputs": [],
      "source": [
        "strip_morpheme = pynutil.delete(\"ième\") # deletes suffix\n",
        "graph_strip_morpheme = NEMO_SIGMA + strip_morpheme # accepts all strings until passed suffix, then deletes suffix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLg-PzdntV4N"
      },
      "source": [
        "Now we can create a graph that permits all characters in a word token and deletes the ordinal suffix. (Note that this also means that the graph won't accept tokens without the suffix, helping us avoid false inputs.)\n",
        "\n",
        "We can now intersect this graph with our Cardinal WFST to now strip the suffixes from ordinals and treat them as cardinals. However, recall that our `CardinalFst` also inserted its own class tag. Obviously, we do not want to do this here as it will disrupt the formatting of the token. Instead, we should create a new subgraph *within* the `CardinalFst` class that will only produce the cardinals without tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWnswIA1kSRs"
      },
      "outputs": [],
      "source": [
        "class CardinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"cardinal\", kind=\"classify\")\n",
        "\n",
        "        ### Cardinal Grammar....\n",
        "        ### .....\n",
        "        graph = graph_trillions | zero\n",
        "\n",
        "        ### Formatting grammar....\n",
        "        ### .....\n",
        "        graph = graph @ clean_cardinal\n",
        "\n",
        "        ### NEW GRAPH\n",
        "        self.just_cardinals = graph # will produce cardinals without formatting\n",
        "\n",
        "        ### Token insertion\n",
        "        optional_minus_graph = pynini.closure(\n",
        "        pynutil.insert(\"negative: \") + pynini.cross(\"moins\", \"\\\"-\\\"\") + \" \", 0, 1\n",
        "          )\n",
        "\n",
        "        final_graph = optional_minus_graph + pynutil.insert(\"integer: \\\"\") + graph + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        final_graph = self.add_tokens(final_graph)\n",
        "\n",
        "        self.fst = final_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hCzEfKlkSRs"
      },
      "source": [
        "Now we call it for our graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxDgBa4_t1nD"
      },
      "outputs": [],
      "source": [
        "graph_cardinal = CardinalFst().just_cardinals\n",
        "graph_ordinal_regular_suffix = graph_strip_morpheme @ graph_cardinal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSpk5M7BuXRz"
      },
      "source": [
        "Let's see if it works and gives us the desired cardinal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cJ7fieouY2r"
      },
      "outputs": [],
      "source": [
        "example = \"sixième\" # dervied from six/6\n",
        "apply_fst(example, graph_ordinal_regular_suffix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtEuV7sOuxek"
      },
      "source": [
        "Now we can consider the edge cases. Beyond the first and second ordinals, French exhibits irregular behavior in the following cases:\n",
        "- If the cardinal root ends with an \"e\", the \"e\" is dropped before adding the suffix (e.g. \"quatrième\").\n",
        "- Cardinals ending with \"cinq\", \"neuf\", and \"dix\" change their endings to \"cinqu\", \"neuv\" , and \"diz\" before appending the suffix, respectively.\n",
        "\n",
        "We could start by proposing a WFST that replaces the suffix \"ième\" with \"e\" and then compose this onto the Cardinal WFST. If it is a legitimate cardinal, then there will be a path through CardinalFST and the integer will be rendered as normal.\n",
        "\n",
        "Meanwhile, the case of \"dix\", \"cinq\", and \"neuf\" would each require a distinct WFST as they are each a consequence of different rules of orthography and phonology. Like the case with \"e\", we could change each back to its root and then see if the CardinalWFST will permit a path with the new input.\n",
        "\n",
        "It is at this point that we can do a cost-benefit analysis and realize that all these cases can be managed by an explicit `string_map/string_file`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9KTNQeIw4sq"
      },
      "outputs": [],
      "source": [
        "graph_root_change = pynini.string_map([(\"quatrième\", \"quatre\"),\n",
        "                                  (\"cinquième\",\t\"cinq\"),\n",
        "                                  (\"neuvième\",\t\"neuf\"),\n",
        "                                  (\"onzième\",\t\"onze\"),\n",
        "                                  (\"douzième\",\t\"douze\"),\n",
        "                                  (\"treizième\",\t\"treize\"),\n",
        "                                  (\"quatorzième\",\t\"quatorze\"),\n",
        "                                  (\"quinzième\",\t\"quinze\"),\n",
        "                                  (\"seizième\",\t\"seize\"),\n",
        "                                  (\"trentième\",\t\"trente\"),\n",
        "                                  (\"quarantième\",\t\"quarante\"),\n",
        "                                  (\"cinquantième\",\t\"cinquante\"),\n",
        "                                  (\"soixantième\",\t\"soixante\"),\n",
        "                                  (\"millième\",\t\"mille\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2_keFVqaY4"
      },
      "source": [
        "We could then concatenate these with a WFST that accepts all tokens with these endings and then change the endings as desired. These will provide the cardinal roots just as effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7I29ezmxylx"
      },
      "source": [
        "The same can be said for \"premier/première\" and \"second/seconde\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JZoz51VyGS6"
      },
      "outputs": [],
      "source": [
        "graph_firsts = pynini.string_map([(\"premier\", \"un\"),(\"première\", \"un\")])\n",
        "graph_seconds = pynini.string_map([(\"second\", \"deux\"),(\"seconde\", \"deux\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ9BGGAwyTQ5"
      },
      "source": [
        "*Note: We graph separately to manage their different superscripts later on.*\n",
        "\n",
        "Depending on your language of focus, the choice of implicitly reversing the root token or explicitly mapping back to root will be the most efficient, but it is worth considering both options if only to check your understanding of the language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgVwDRRq9gr"
      },
      "source": [
        "Putting our grammar together, we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko2kAeKwrRSH"
      },
      "outputs": [],
      "source": [
        "strip_morpheme = pynutil.delete(\"ième\") # deletes suffix\n",
        "\n",
        "graph_root_change = pynini.string_map([(\"quatrième\", \"quatre\"),\n",
        "                                  (\"cinquième\",\t\"cinq\"),\n",
        "                                  (\"neuvième\",\t\"neuf\"),\n",
        "                                  (\"onzième\",\t\"onze\"),\n",
        "                                  (\"douzième\",\t\"douze\"),\n",
        "                                  (\"treizième\",\t\"treize\"),\n",
        "                                  (\"quatorzième\",\t\"quatorze\"),\n",
        "                                  (\"quinzième\",\t\"quinze\"),\n",
        "                                  (\"seizième\",\t\"seize\"),\n",
        "                                  (\"trentième\",\t\"trente\"),\n",
        "                                  (\"quarantième\",\t\"quarante\"),\n",
        "                                  (\"cinquantième\",\t\"cinquante\"),\n",
        "                                  (\"soixantième\",\t\"soixante\"),\n",
        "                                  (\"millième\",\t\"mille\"),\n",
        "])\n",
        "\n",
        "# Component will accept all tokens that end with desired strings\n",
        "graph_get_cardinal = NEMO_SIGMA + (strip_morpheme | graph_root_change)\n",
        "\n",
        "graph_firsts = pynini.string_map([(\"premier\", \"un\"),(\"première\", \"un\")])\n",
        "graph_seconds = pynini.string_map([(\"second\", \"deux\"),(\"seconde\", \"deux\")])\n",
        "\n",
        "graph_get_cardinal = pynini.union(graph_firsts, graph_seconds, graph_get_cardinal)\n",
        "\n",
        "graph_cardinal = CardinalFst().just_cardinals\n",
        "\n",
        "graph_ordinal = graph_get_cardinal @ graph_cardinal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESxY3LsCdE8q"
      },
      "outputs": [],
      "source": [
        "apply_fst(\"sixième\", graph_ordinal)\n",
        "apply_fst(\"première\", graph_ordinal)\n",
        "apply_fst(\"seconde\", graph_ordinal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_g8UdoUFJB"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kemhdKAjzEIa"
      },
      "source": [
        "Now that we've found a way to pass the work of the Ordinal grammar back onto the Cardinal grammar, we can move onto the Classifier. Like before, we need to inherit from `GraphFst` to properly insert token formatting and required attributes. As well, we will again use the `integer` property to tag our digit string.\n",
        "\n",
        "Indeed, the only major difference between the Ordinal Classifier and the Cardinal Classifier is the replacement of optional `negative` attribute with the `morphosyntactic_feature` attribute to indicate the superscript function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHM4Y3TW2nXT"
      },
      "source": [
        "Since we are relying on the `CardinalFst` class in our grammar, we want to consider how to instantiate an instance of it. Since our ultimate goal is to build a Classifier that unites all semiotic classes, it makes sense to simply use the `CardinalFst` that we will need to call for our ITN and pass it as an argument to our new class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsmPhWSa3LF_"
      },
      "outputs": [],
      "source": [
        "def __init__(self, cardinal: GraphFst):\n",
        "        super().__init__(name=\"ordinal\", kind=\"classify\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtBQ-udB3S5Q"
      },
      "source": [
        "To clear up the namespace, we will now be importing from the NeMo implementation of `CardinalFst` for French."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-JAcidf4QQg"
      },
      "outputs": [],
      "source": [
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.cardinal import CardinalFst\n",
        "\n",
        "class OrdinalFst(GraphFst):\n",
        "  def __init__(self, cardinal: GraphFst):\n",
        "          super().__init__(name=\"ordinal\", kind=\"classify\")\n",
        "          graph_cardinal = cardinal.graph_no_exception # NeMo equivalent to self.just_cardinals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQfkAqZavCAB"
      },
      "source": [
        "We now add in our grammar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUQ4BLuivGut"
      },
      "outputs": [],
      "source": [
        "class OrdinalFst(GraphFst):\n",
        "  def __init__(self, cardinal: GraphFst):\n",
        "      super().__init__(name=\"ordinal\", kind=\"classify\")\n",
        "      graph_cardinal = cardinal.graph_no_exception # may replace\n",
        "\n",
        "      strip_morpheme = pynutil.delete(\"ième\") # deletes suffix\n",
        "\n",
        "      graph_root_change = pynini.string_map([(\"quatrième\", \"quatre\"),\n",
        "                                (\"cinquième\",\t\"cinq\"),\n",
        "                                (\"neuvième\",\t\"neuf\"),\n",
        "                                (\"onzième\",\t\"onze\"),\n",
        "                                (\"douzième\",\t\"douze\"),\n",
        "                                (\"treizième\",\t\"treize\"),\n",
        "                                (\"quatorzième\",\t\"quatorze\"),\n",
        "                                (\"quinzième\",\t\"quinze\"),\n",
        "                                (\"seizième\",\t\"seize\"),\n",
        "                                (\"trentième\",\t\"trente\"),\n",
        "                                (\"quarantième\",\t\"quarante\"),\n",
        "                                (\"cinquantième\",\t\"cinquante\"),\n",
        "                                (\"soixantième\",\t\"soixante\"),\n",
        "                                (\"millième\",\t\"mille\"),\n",
        "        ])\n",
        "\n",
        "      # Component will accept all tokens that end with desired strings\n",
        "      graph_get_cardinal = NEMO_SIGMA + (strip_morpheme | graph_root_change)\n",
        "\n",
        "      graph_firsts = pynini.string_map([(\"premier\", \"un\"),(\"première\", \"un\")])\n",
        "      graph_seconds = pynini.string_map([(\"second\", \"deux\"),(\"seconde\", \"deux\")])\n",
        "\n",
        "      graph_get_cardinal = pynini.union(graph_firsts, graph_seconds, graph_get_cardinal)\n",
        "\n",
        "      graph_ordinal = graph_get_cardinal @ graph_cardinal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_6EXPRMvnp2"
      },
      "source": [
        "Now we come to the `morphosyntactic_features` property - a linguistic term for aspects of a word related to grammar. If intending to deploy your WFST through Sparrowhawk, this is the only ordinal property that is permitted (outside of the universal properties like `preserve_order`) and thus must carry all information regarding how to properly normalize the ordinal. (If Sparrowhawk deployment is not necessary, you may add additional properties to the tag.)\n",
        "\n",
        "How should we convey this information? Since the Verbalizer will be the main interface for our tags, it really does not matter - so long as we can reliably process the features. For the purposes of French, we just need `morphosyntactic_features` to decide the following:\n",
        "- Insert the specific superscripts for \"premier/première\" or \"second/seconde\"\n",
        "- Insert \"ᵉ\" otherwise\n",
        "\n",
        "We will also introduce another aspect of French Ordinals: they can be either plural or singular, identified by the suffix \"s\" on input and superscript \"ˢ\" on output. As such, our `morphosyntactic_features` should also decide the additional property:\n",
        "- Insert the plural superscript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atctz6p-2GtV"
      },
      "source": [
        "Since the default superscript is near universal, we will just specify this in our WFST and focus on the second and first ordinals as specific cases. We will create a `graph_morpheme` component that inserts the default superscript - indicated with a standard \"e\" to avoid possible encoding issues. We will then append a WFST that will graph any possible plural marker - \"s\" - as part the `morphosyntactic_features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui99osyP2UuQ"
      },
      "outputs": [],
      "source": [
        "graph_morpheme = pynutil.insert(\"e\") # Insert e superscript\n",
        "graph_plural = pynini.closure(pynini.accep(\"s\"), 0, 1) # We create an acceptor since we must process the possible \"s\"\n",
        "\n",
        "graph_morpheme_component = graph_morpheme + graph_plural\n",
        "\n",
        "graph_morphosyntactic_features = (pynutil.insert(\" morphosyntactic_features: \\\"\")\n",
        "      + graph_morpheme_component\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAlqubA25gq0"
      },
      "source": [
        "Introducing the `integer` feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs2TyIBc5la6"
      },
      "outputs": [],
      "source": [
        "graph_reg_ordinals = graph_get_cardinal @ graph_cardinal # Rewriting ordinals to remove the first and second ordinal.\n",
        "\n",
        "graph_ordinal = pynutil.insert(\"integer: \\\"\") + graph_reg_ordinals + pynutil.insert(\"\\\"\")\n",
        "graph_ordinal += graph_morphosyntactic_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoqk20Pi2gT8"
      },
      "source": [
        "For the first and second ordinals, we can explicitly state their mappings, as these occurrences are invariable. (First and second ordinals do not need to accommodate being the endings of other terms.) As such, we can just have mappings from the token to the superscripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54aqdH_P63Ea"
      },
      "outputs": [],
      "source": [
        "firsts = pynini.string_map([(\"premier\", \"er\"), (\"première\",\"re\")])\n",
        "firsts += graph_plural # Still accepts plural marker in superscript\n",
        "seconds = pynini.string_map([(\"second\", \"d\"),(\"seconde\", \"de\")])\n",
        "seconds += graph_plural\n",
        "\n",
        "graph_firsts = pynutil.insert(\"integer: \\\"1\\\" morphosyntactic_features: \\\"\") + firsts\n",
        "graph_seconds = pynutil.insert(\"integer: \\\"2\\\" morphosyntactic_features: \\\"\") + seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2vQ4m7o7p84"
      },
      "source": [
        "Placing them in our class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_JKT8JMf-Mz"
      },
      "outputs": [],
      "source": [
        "class OrdinalFst(GraphFst):\n",
        "  def __init__(self, cardinal: GraphFst):\n",
        "      super().__init__(name=\"ordinal\", kind=\"classify\")\n",
        "      graph_cardinal = cardinal.graph_no_exception # may replace\n",
        "\n",
        "      strip_morpheme = pynutil.delete(\"ième\") # deletes suffix\n",
        "\n",
        "      graph_root_change = pynini.string_map([(\"quatrième\", \"quatre\"),\n",
        "                                (\"cinquième\",\t\"cinq\"),\n",
        "                                (\"neuvième\",\t\"neuf\"),\n",
        "                                (\"onzième\",\t\"onze\"),\n",
        "                                (\"douzième\",\t\"douze\"),\n",
        "                                (\"treizième\",\t\"treize\"),\n",
        "                                (\"quatorzième\",\t\"quatorze\"),\n",
        "                                (\"quinzième\",\t\"quinze\"),\n",
        "                                (\"seizième\",\t\"seize\"),\n",
        "                                (\"trentième\",\t\"trente\"),\n",
        "                                (\"quarantième\",\t\"quarante\"),\n",
        "                                (\"cinquantième\",\t\"cinquante\"),\n",
        "                                (\"soixantième\",\t\"soixante\"),\n",
        "                                (\"millième\",\t\"mille\"),\n",
        "        ])\n",
        "\n",
        "      # Component will accept all tokens that end with desired strings\n",
        "      graph_get_cardinal = NEMO_SIGMA + (strip_morpheme | graph_root_change)\n",
        "\n",
        "      # Graph will map ordinals beyond second ordinal to their cardinals\n",
        "      graph_reg_ordinals = graph_get_cardinal @ graph_cardinal\n",
        "\n",
        "      # Graphing morphosyntactic_features\n",
        "      graph_morpheme = pynutil.insert(\"e\") # Insert e superscript\n",
        "      graph_plural = pynini.accep(\"s\").ques # ques is equivalent to pynini.closure(, 0, 1)\n",
        "\n",
        "      graph_morpheme_component = graph_morpheme + graph_plural\n",
        "\n",
        "      graph_morphosyntactic_features = (pynutil.insert(\" morphosyntactic_features: \\\"\")\n",
        "      + graph_morpheme_component\n",
        "        )\n",
        "\n",
        "      # Adding in the `integer` property:\n",
        "      graph_ordinal = pynutil.insert(\"integer: \\\"\") + graph_reg_ordinals + pynutil.insert(\"\\\"\")\n",
        "      graph_ordinal += graph_morphosyntactic_features\n",
        "\n",
        "      # Case of first and second ordinals\n",
        "      firsts = pynini.string_map([(\"premier\", \"er\"), (\"première\",\"re\")])\n",
        "      firsts += graph_plural # Still accepts plural marker in superscript\n",
        "      seconds = pynini.string_map([(\"second\", \"d\"),(\"seconde\", \"de\")])\n",
        "      seconds += graph_plural\n",
        "\n",
        "      graph_firsts = pynutil.insert(\"integer: \\\"1\\\" morphosyntactic_features: \\\"\") + firsts\n",
        "      graph_seconds = pynutil.insert(\"integer: \\\"2\\\" morphosyntactic_features: \\\"\") + seconds\n",
        "\n",
        "      # All together\n",
        "      graph_ordinal = pynini.union(graph_ordinal, graph_firsts, graph_seconds)\n",
        "      self.fst = graph_ordinal.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpGHVg6chmA0"
      },
      "source": [
        "Trying out on some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5DL3PZRhpc8"
      },
      "outputs": [],
      "source": [
        "cardinal = CardinalFst()\n",
        "ordinal = OrdinalFst(cardinal).fst\n",
        "\n",
        "apply_fst(\"premier\", ordinal)\n",
        "apply_fst(\"premiers\", ordinal)\n",
        "apply_fst(\"seconde\", ordinal)\n",
        "apply_fst(\"douzièmes\", ordinal)\n",
        "apply_fst(\"cent-cinquièmes\", ordinal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNQVgiv-UK29"
      },
      "source": [
        "### Special Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdiNAHGh71O9"
      },
      "source": [
        "If you are particularly astute, you may have noticed that we have not closed the quotations around the `morphosyntactic_features` throughout, despite doing so for `integer`. This is not a typo, as there is one more aspect of the Classifier that must be addressed: special cases.\n",
        "\n",
        "For your language, you may notice that there are occasional exceptions to writing rules that are signaled by a specific vocabulary token in a string. As this must be communicated to our Verbalizer, it is important that we signal this vocabulary through our Classifier.\n",
        "\n",
        "For French, this can occur in the normalization of centuries. When using Ordinals to indicate centuries, French commonly writes with Roman numerals. For example:\n",
        "- \"Fifth century\" -> \"cinquième siècle\" -> `Vᵉ siècle`  \n",
        "- \"Twentieth century\" -> \"vintième siècle\" -> `XXᵉ siècle`\n",
        "\n",
        "As such, we must allow our Classifier to pass on the information that \"siècle\" follows an ordinal to our Verbalizer, so it may normalize with Roman numerals. We accomplish this by appending a WFST that accepts special tokens that follow our Ordinals, adding them to our `morphosyntactic_features` attribute with a forward slash to delineate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsWnT4BfQKcC"
      },
      "outputs": [],
      "source": [
        "special_tokens = pynini.accep(\"siècle\")\n",
        "\n",
        "graph_special_tokens = delete_space + pynutil.insert(\"/\") + special_tokens # We need to delete the space in between this token and the following one.\n",
        "graph_special_tokens = pynini.closure(graph_special_tokens, 0, 1)\n",
        "\n",
        "graph_ordinal += graph_special_tokens + pynutil.insert(\"\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698_n5SFQ_jP"
      },
      "source": [
        "*Once again, it is advised to retain a tsv file in `data` to quickly append these key-words.*\n",
        "\n",
        "Having taken care of the special case, we may now call `add_tokens` and complete the graph (fully written out below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ1dkft0Riou"
      },
      "outputs": [],
      "source": [
        "class OrdinalFst(GraphFst):\n",
        "  def __init__(self, cardinal: GraphFst):\n",
        "      super().__init__(name=\"ordinal\", kind=\"classify\")\n",
        "      graph_cardinal = cardinal.graph_no_exception # may replace\n",
        "\n",
        "      strip_morpheme = pynutil.delete(\"ième\") # deletes suffix\n",
        "\n",
        "      graph_root_change = pynini.string_map([(\"quatrième\", \"quatre\"),\n",
        "                                (\"cinquième\",\t\"cinq\"),\n",
        "                                (\"neuvième\",\t\"neuf\"),\n",
        "                                (\"onzième\",\t\"onze\"),\n",
        "                                (\"douzième\",\t\"douze\"),\n",
        "                                (\"treizième\",\t\"treize\"),\n",
        "                                (\"quatorzième\",\t\"quatorze\"),\n",
        "                                (\"quinzième\",\t\"quinze\"),\n",
        "                                (\"seizième\",\t\"seize\"),\n",
        "                                (\"trentième\",\t\"trente\"),\n",
        "                                (\"quarantième\",\t\"quarante\"),\n",
        "                                (\"cinquantième\",\t\"cinquante\"),\n",
        "                                (\"soixantième\",\t\"soixante\"),\n",
        "                                (\"millième\",\t\"mille\"),\n",
        "        ])\n",
        "\n",
        "      # Component will accept all tokens that end with desired strings\n",
        "      graph_get_cardinal = NEMO_SIGMA + (strip_morpheme | graph_root_change)\n",
        "\n",
        "      # Graph will map ordinals beyond second ordinal to their cardinals\n",
        "      graph_reg_ordinals = graph_get_cardinal @ graph_cardinal\n",
        "\n",
        "      # Graphing morphosyntactic_features\n",
        "      graph_morpheme = pynutil.insert(\"e\") # Insert e superscript\n",
        "      graph_plural = pynini.accep(\"s\").ques # We create an acceptor since we must process the possible \"s\"\n",
        "\n",
        "      graph_morpheme_component = graph_morpheme + graph_plural\n",
        "\n",
        "      graph_morphosyntactic_features = (pynutil.insert(\" morphosyntactic_features: \\\"\")\n",
        "      + graph_morpheme_component\n",
        "        )\n",
        "\n",
        "      # Adding in the `integer` property:\n",
        "      graph_ordinal = pynutil.insert(\"integer: \\\"\") + graph_reg_ordinals + pynutil.insert(\"\\\"\")\n",
        "      graph_ordinal += graph_morphosyntactic_features\n",
        "\n",
        "      # Case of first and second ordinals\n",
        "      firsts = pynini.string_map([(\"premier\", \"er\"), (\"première\",\"re\")])\n",
        "      firsts += graph_plural # Still accepts plural marker in superscript\n",
        "      seconds = pynini.string_map([(\"second\", \"d\"),(\"seconde\", \"de\")])\n",
        "      seconds += graph_plural\n",
        "\n",
        "      graph_firsts = pynutil.insert(\"integer: \\\"1\\\" morphosyntactic_features: \\\"\") + firsts\n",
        "      graph_seconds = pynutil.insert(\"integer: \\\"2\\\" morphosyntactic_features: \\\"\") + seconds\n",
        "\n",
        "\n",
        "      # Special tokens\n",
        "      special_tokens = pynini.accep(\"siècle\")\n",
        "\n",
        "      graph_special_tokens = delete_space + pynutil.insert(\"/\") + special_tokens # We need to delete the space in between this token and the following one.\n",
        "      graph_special_tokens = pynini.closure(graph_special_tokens, 0, 1)\n",
        "\n",
        "      graph_ordinal += graph_special_tokens + pynutil.insert(\"\\\"\")\n",
        "\n",
        "      # Finishing\n",
        "      graph_ordinal = self.add_tokens(graph_ordinal)\n",
        "      self.fst = graph_ordinal.optimize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a4zBo-YS1QD"
      },
      "source": [
        "## Verbalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYbrcGyGS2rW"
      },
      "source": [
        "The initial part of the Ordinal Verbalizer is similar to the Cardinal WFST: we simply need to build a Verbalizer that inherits from `GraphFST` and removes the `integer` property tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUv99A_rYjb9"
      },
      "outputs": [],
      "source": [
        "class OrdinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"ordinal\", kind=\"verbalize\")\n",
        "        graph_integer = (\n",
        "            pynutil.delete(\"integer:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_DIGIT, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKCt_EapZXGW"
      },
      "source": [
        "Now we need to manage the `morphosyntactic_features` component. The first steps seem simple enough: delete the property tag and replace the superscript indicators with the actual superscripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoa_mXMLabrU"
      },
      "outputs": [],
      "source": [
        "        # Create mappings for all superscripts\n",
        "        superscript = pynini.union(\n",
        "            pynini.cross(\"e\", \"ᵉ\"),  # only delete first quote since there may be more features\n",
        "            pynini.cross(\"d\", \"ᵈ\"),\n",
        "            pynini.cross(\"r\", \"ʳ\"),\n",
        "            pynini.cross(\"s\", \"ˢ\"),\n",
        "        )\n",
        "\n",
        "        # Append to deletion of feature property. Note that we use plus closure for multiple superscripts.\n",
        "        graph_morphosyntactic_features = pynutil.delete(\" morphosyntactic_features: \\\"\") + superscript.plus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOA7_MsUrSJS"
      },
      "source": [
        "### Romanization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_SaG0DUa2t7"
      },
      "source": [
        "Now we come to the possible Romanization component. Since we need to graph the superscript components as following the number, we want to design our graph so that `morphosyntactic_features` is the last component of the graph. However, we do not know that we need Romanization until we see the `morphosyntactic_features` component. As such, we need to design our graph such that two options are available initially for an input, but only one allows full traversal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dalc-tablG-"
      },
      "source": [
        "![romanization.png](https://github.com/NVIDIA/NeMo-text-processing/blob/main/tutorials/images/romanization.PNG?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPTNCddNcEEE"
      },
      "source": [
        "In cases where your WFST decisions are dependent on latter parts of an input string, permitting the union of two separate paths when only one is valid usually assists, as a standard pathing heuristic will only choose the valid path.\n",
        "\n",
        "In the case of French, this would require us to separate our Verbalizer into two parts: one for Arabic numerals and one for Roman numerals. For the Arabic WFST, we simply conclude the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YSy1PYOcuyD"
      },
      "outputs": [],
      "source": [
        "graph_integer = (\n",
        "            pynutil.delete(\"integer:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_DIGIT, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "graph_Arabic = graph_integer + graph_morphosyntactic_features + pynutil.delete(\"\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnXjUU5Pf7Sh"
      },
      "source": [
        "For the Roman graph, things get a bit trickier. Ideally, we would want to build a WFST that maps each digit of `graph_Arabic` to a Roman equivalent. However, consider the following examples:\n",
        "- 1 -> I\n",
        "- 10 -> X\n",
        "- 11 -> XI\n",
        "- 100 -> C\n",
        "- 101 -> CI\n",
        "- 110 -> CX\n",
        "- 111 -> CXI\n",
        "\n",
        "Since Roman numerals do not preserve powers of ten through digit placement, we will need to design separate FSTs for each digit position and apply them accordingly. As this can quickly become intensive, we will only work to enumerate the Ordinals from 1 to 100. (Note: We are doing this to accommodate centuries; there is little likelihood that any century beyond the 99th will be used in regular strings.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-fQHMc2iQrz"
      },
      "source": [
        "First we design our graphs for converting from Arabic to Roman numerals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6PDySykiXTh"
      },
      "outputs": [],
      "source": [
        "digits = pynini.string_map([(\"1\", \"I\"),\n",
        "                          (\"2\",\t\"II\"),\n",
        "                          (\"3\",\t\"III\"),\n",
        "                          (\"4\",\t\"IV\"),\n",
        "                          (\"5\",\t\"V\"),\n",
        "                          (\"6\",\t\"VI\"),\n",
        "                          (\"7\",\t\"VII\"),\n",
        "                          (\"8\",\t\"VIII\"),\n",
        "                          (\"9\",\t\"IX\"),\n",
        "          ])\n",
        "tens = pynini.string_map([(\"1\", \"X\"),\n",
        "                          (\"2\",\t\"XX\"),\n",
        "                          (\"3\",\t\"XXX\"),\n",
        "                          (\"4\",\t\"XL\"),\n",
        "                          (\"5\",\t\"L\"),\n",
        "                          (\"6\",\t\"LX\"),\n",
        "                          (\"7\",\t\"LXX\"),\n",
        "                          (\"8\",\t\"LXXX\"),\n",
        "                          (\"9\",\t\"XC\"),\n",
        "          ])\n",
        "zero = pynutil.delete(\"0\") # No Roman representation for zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb-LmwJdk59m"
      },
      "source": [
        "Now we build two separate filters: one will accept only single digit Arabic numerals and the other will accept two digit Arabic numerals. For this we can use `NEMO_DIGIT`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW3oD7Hbli2X"
      },
      "outputs": [],
      "source": [
        "map_one_digit = NEMO_DIGIT\n",
        "map_two_digits = NEMO_DIGIT ** 2 # pynini overloads the exponent function to allow self-concatenation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtYKLy9AmJZS"
      },
      "source": [
        "We now build mappings between two digit Arabic numerals and Roman numerals, composing them onto the filters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUy7uEUXmT_g"
      },
      "outputs": [],
      "source": [
        "graph_one_digit_romans = NEMO_DIGIT @ digits\n",
        "\n",
        "graph_two_digit_romans = tens + (digits | zero)\n",
        "graph_two_digit_romans = map_two_digits @ graph_two_digit_romans\n",
        "\n",
        "graph_romans = graph_one_digit_romans | graph_two_digit_romans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEinyAMdm7RJ"
      },
      "source": [
        "We now take care of the occurrence of \"siècle\" before composing onto `graph_integer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERO19BbynPNX"
      },
      "outputs": [],
      "source": [
        "graph_romans = (graph_integer @ graph_romans) + graph_morphosyntactic_features\n",
        "graph_romans += pynini.cross(\"/\", \" \") + \"siècle\" + pynutil.delete(\"\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN-fwrCGoToQ"
      },
      "source": [
        "We finalize with a union and calling `delete_tokens`, the complete Verbalizer now being::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr2wcToAofWB"
      },
      "outputs": [],
      "source": [
        "class OrdinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"ordinal\", kind=\"verbalize\")\n",
        "\n",
        "        # Maps integer and removes attribute\n",
        "        graph_integer = (\n",
        "            pynutil.delete(\"integer:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_DIGIT, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "\n",
        "        # Create mappings for all superscripts\n",
        "        superscript = pynini.union(\n",
        "            pynini.cross(\"e\", \"ᵉ\"),  # only delete first quote since there may be more features\n",
        "            pynini.cross(\"d\", \"ᵈ\"),\n",
        "            pynini.cross(\"r\", \"ʳ\"),\n",
        "            pynini.cross(\"s\", \"ˢ\"),\n",
        "        )\n",
        "\n",
        "        # Append to deletion of feature property. Note that we use plus closure for multiple superscripts.\n",
        "        graph_morphosyntactic_features = pynutil.delete(\" morphosyntactic_features: \\\"\") + superscript.plus\n",
        "\n",
        "        # Writing WFST for Arabic\n",
        "        graph_Arabic = graph_integer + graph_morphosyntactic_features + pynutil.delete(\"\\\"\")\n",
        "\n",
        "        # Mapping Roman numerals\n",
        "        digits = pynini.string_map([(\"1\", \"I\"),\n",
        "                          (\"2\",\t\"II\"),\n",
        "                          (\"3\",\t\"III\"),\n",
        "                          (\"4\",\t\"IV\"),\n",
        "                          (\"5\",\t\"V\"),\n",
        "                          (\"6\",\t\"VI\"),\n",
        "                          (\"7\",\t\"VII\"),\n",
        "                          (\"8\",\t\"VIII\"),\n",
        "                          (\"9\",\t\"IX\"),\n",
        "          ])\n",
        "        tens = pynini.string_map([(\"1\", \"X\"),\n",
        "                          (\"2\",\t\"XX\"),\n",
        "                          (\"3\",\t\"XXX\"),\n",
        "                          (\"4\",\t\"XL\"),\n",
        "                          (\"5\",\t\"L\"),\n",
        "                          (\"6\",\t\"LX\"),\n",
        "                          (\"7\",\t\"LXX\"),\n",
        "                          (\"8\",\t\"LXXX\"),\n",
        "                          (\"9\",\t\"XC\"),\n",
        "          ])\n",
        "        zero = pynutil.delete(\"0\") # No Roman representation for zero.\n",
        "\n",
        "        # filters for Roman digits\n",
        "        map_one_digit = NEMO_DIGIT\n",
        "        map_two_digits = NEMO_DIGIT ** 2 # pynini overloads the exponent function to allow self-concatenation.\n",
        "\n",
        "        # Composing onto roman digits\n",
        "        graph_one_digit_romans = NEMO_DIGIT @ digits\n",
        "\n",
        "        graph_two_digit_romans = tens + (digits | zero)\n",
        "        graph_two_digit_romans = map_two_digits @ graph_two_digit_romans\n",
        "\n",
        "        graph_romans = graph_one_digit_romans | graph_two_digit_romans\n",
        "\n",
        "        # Writing WFST for Roman\n",
        "        graph_romans = (graph_integer @ graph_romans) + graph_morphosyntactic_features\n",
        "        graph_romans += pynini.cross(\"/\", \" \") + \"siècle\" + pynutil.delete(\"\\\"\")\n",
        "\n",
        "        # Final composition\n",
        "        graph = (graph_romans | graph_Arabic)\n",
        "\n",
        "        delete_tokens = self.delete_tokens(graph)\n",
        "        self.fst = delete_tokens.optimize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2hEtyglkSRx"
      },
      "source": [
        "Trying out our examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokOK_8pkSRx"
      },
      "outputs": [],
      "source": [
        "example_regular = 'ordinal { integer: \"12\" morphosyntactic_features: \"es\" }'\n",
        "example_roman = 'ordinal { integer: \"12\" morphosyntactic_features: \"es/siècle\" }'\n",
        "\n",
        "fst = OrdinalFst().fst\n",
        "\n",
        "apply_fst(example_regular, fst)\n",
        "apply_fst(example_roman, fst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBgLhTq9pWZe"
      },
      "source": [
        "We have now completed an Ordinal WFST from the ground up, allowing a separate numbering system for special cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W1-BMVJUXXk"
      },
      "source": [
        "## Final notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR7E64P4pPU_"
      },
      "source": [
        "Before moving on, there are some key takeaways that you may find useful for most (if not all) languages:\n",
        "- Many ordinal systems rely on alteration of Cardinals. Even in the example of Korean, it is using a pre-existing counting system and adding a suffix to indicate ordering. As such, your Ordinal WFST will likely follow this tutorial's structure of changing the Ordinal to its original root and then relying on your Cardinal WFST for the majority of processing.\n",
        "- The `morphosyntactic_features` property will carry the vast majority of information necessary for normalization through your Verbalizer.\n",
        "- While not all writing systems have the same quirk as using Roman numerals in reference to centuries, you will likely find cases in your language when a specific token indicates unique rules for a semiotic class. Carrying this information to the Verbalizer is usually the simplest means of preserving the token while also facilitating normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx8-LuJOUaa5"
      },
      "source": [
        "# Decimal WFST <a id=\"decimal-wfst\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2MRXYxz8TGA"
      },
      "source": [
        "\n",
        "If the Cardinal WFST is the most crucial element of a normalization grammar, the construction of the Decimal WFST is a close second. Much like in the case of constructing Ordinals from Cardinal grammars, many aspects of the Decimal WFST will be reused throughout your other semiotic classes.\n",
        "\n",
        "To get started, you should study the numerical conventions in your language. In particular, you should take note of the following:\n",
        "- How is the decimal component of a number pronounced in your language of focus. (e.g. The English number `1.33` can be verbalized as \"one point three three\" or \"one and thirty three hundredths.\")\n",
        "- What is the punctuation mark used for decimal demarcation? (In North America, several writing systems use `.` while European nations will use `,`.)\n",
        "- Are there general rules regarding pronunciation/formatting of numbers past the decimal demarcation? (e.g. Does your language pronounce each digit or pronounce as a series of three digit numbers?)\n",
        "\n",
        "Such questions will likely require some deep familiarity with the language, and it may benefit to ask a native speaker for some input. Of course, the level of depth is dependent on your needs, but researching these questions will help your normalization system appear more organic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsK78ib4N-gb"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4CLOOA9OAwZ"
      },
      "source": [
        "In the case of French, we have the following guidelines:\n",
        "- French uses the comma ( `,` ) for decimal delineation. It is articulated as \"virgule\".\n",
        "- Decimals can be read as a series of digits or grouped as Cardinal numbers arbitrarily. (e.g. \"`.333` can be \"virgule trois trois trois\" or \"virgule trois-cent-trente-trois\".)\n",
        "\n",
        "As such, our grammar needs to accommodate the following pattern:\n",
        "\n",
        "`cardinal + \"virgule\" + string_of_cardinals`\n",
        "\n",
        "Given our experience with our previous WFSTs, this seems simple enough. We assume we have an instance of CardinalFST available and create a subcomponent to map the integer portion of a decimal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSp9FTzhf0XZ"
      },
      "outputs": [],
      "source": [
        "cardinal = CardinalFst().graph_no_exception # NeMo equivalent of just_cardinals\n",
        "\n",
        "# place cardinal under closure to permit values <=1\n",
        "graph_integer = pynini.closure(cardinal, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk3_3iawgAZE"
      },
      "source": [
        "Compose it on a subcomponent that detects the delineator \"virgule\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMzfAKkngH6z"
      },
      "outputs": [],
      "source": [
        "delete_virgule = pynutil.delete(\"virgule\")\n",
        "graph_decimal = graph_integer + delete_space + delete_virgule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXjbtbLYgn17"
      },
      "source": [
        "And permit the occurrence of several strings of cardinals to follow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMMNBJz8gtTA"
      },
      "outputs": [],
      "source": [
        "graph_string_of_cardinals = delete_space + graph_cardinal\n",
        "graph_string_of_cardinals = pynini.closure(graph_string_of_cardinals, 1)\n",
        "\n",
        "graph_decimal += graph_string_of_cardinals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTgnRLddhGdE"
      },
      "source": [
        "Let us try an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4rjDh0ShJAp"
      },
      "outputs": [],
      "source": [
        "example = \"trois virgule trois cinquante-cinq\"\n",
        "apply_fst(example, graph_decimal) # Should output only the cardinals in the string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD1d9JOioyl"
      },
      "source": [
        "### Ambiguity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IaI1mCIe_6i"
      },
      "source": [
        "Note that our decision to include multiple strings of cardinals after the decimal marker has introduced some ambiguity into our WFST. Consider if a decimal number was followed by an integer series (e.g. `2.5, 5, 6`). Now what should be an application of one DecimalFST and two applications of a CardinalFST can be interpreted as a single DecimalFST application (e.g. `2.556`). What can be done?\n",
        "\n",
        "While we will address this in greater depth later (see [Tokenize and Classify](#tokenize-and-classify)), the short answer is that cases such as these must be calibrated according to use and linguistic intuition. As this is an inherent ambiguity in the language and its writing system, we can never truly remove this possibility without restricting our ability to model the language. However, we can rely on a few logical assumptions to guide our decision making:\n",
        "- Unless the grammar is deployed in a restrictive setting (e.g. a Financial or environment where strings of numbers are often read in series) it's not likely for a valid string to exhibit this level of ambiguity. Speakers typically try to reduce possible ambiguity in their language production and would likely rephrase to avoid issues such as these. [See Grice's maxims](https://en.wikipedia.org/wiki/Cooperative_principle).\n",
        "- While a language may allow a specific string by *rule*, speakers may typically avoid them *in practice* due to conventions or difficulty. In our case, while it may be possible to read `2,100 05` as \"deux virgule dix-mille-cinq\" (\"two point ten-thousand and five\"), it's dubious that a speaker would find such easier to read than \"deux virgule une zéro zéro zéro cinq\". (The place value of large strings tend to take longer to recognize.)\n",
        "\n",
        "While hardly satisfying, these two points will allow us to dismiss *some* worry. With the former observation being outside our grammar's ability to manage, we accommodate the latter point by using an alternate WFST from our CardinalFST: `numbers_up_to_million`. (To utilize in your own language, create a WFST in the Cardinal class right before building up to `graph_millions`. Again, calling `optimize` is advised.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piNe1AWspa4J"
      },
      "outputs": [],
      "source": [
        "cardinal = CardinalFst().numbers_up_to_million\n",
        "\n",
        "# place cardinal under closure to permit values <=1\n",
        "graph_integer = pynini.closure(cardinal, 0, 1)\n",
        "\n",
        "delete_virgule = pynutil.delete(\"virgule\")\n",
        "graph_decimal = graph_integer + delete_space + delete_virgule\n",
        "\n",
        "graph_string_of_cardinals = delete_space + cardinal\n",
        "graph_string_of_cardinals = pynini.closure(graph_string_of_cardinals, 1)\n",
        "\n",
        "graph_decimal += graph_string_of_cardinals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1gglt0tfM5V"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVkOWkncgOZc"
      },
      "source": [
        "Like with our previous WFSTs, the main duty for the classifier is inserting the necessary properties for the semiotic token. For the `decimal` tag, the following properties are used:\n",
        "- `integer_part` - indicates value before decimal marker\n",
        "- `fractional_part` - indicates values after the decimal marker\n",
        "- `negative` - indicates if value is positive or negative (Optional)\n",
        "- `quantity` - designates if decimal is in regards to a specific quantity. (See Quantities.)\n",
        "\n",
        "We can begin by inserting the `integer_part` around our `cardinal` subcomponent and the `fractional_part` around our `graph_string_of_cardinals`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zw_cDszh-fB"
      },
      "outputs": [],
      "source": [
        "graph_integer = pynutil.insert(\"integer_part: \\\"\") + cardinal + pynutil.insert(\"\\\" \")\n",
        "graph_fractional = pynutil.insert(\"fractional_part: \\\"\") + graph_string_of_cardinals + pynutil.insert(\"\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxlnn_7tiQMn"
      },
      "source": [
        "We then concatenate them together with a component that recognizes and removes the decimal separator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxNS9_AwiWHf"
      },
      "outputs": [],
      "source": [
        "graph_integer_or_none = graph_integer | pynutil.insert(\"integer_part: \\\"0\\\" \", weight=.1) # In cases we don't always have an integer preceding\n",
        "graph_decimal_no_sign = graph_integer_or_none + delete_space + pynutil.delete(\"virgule\") + graph_fractional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7uGfsi4i5UI"
      },
      "source": [
        "*Note that we allow insertion of 0 if there is no integer to accommodate reading of only decimal values*\n",
        "\n",
        "Now we allow the possibility of negative values. (Recall French uses \"moins\" to indicate the negative.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsP79naojQZR"
      },
      "outputs": [],
      "source": [
        "graph_negative = pynini.cross(\"moins\", \"negative: \\\"-\\\" \") + delete_space\n",
        "graph_decimal = graph_negative + graph_decimal_no_sign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTcvq5HqllqW"
      },
      "outputs": [],
      "source": [
        "example = \"moins deux virgule cent-quatre\"\n",
        "apply_fst(example, graph_decimal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVKuGj_9mZ75"
      },
      "source": [
        "Placing within a `DecimalFst` class, we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXwr32ermesp"
      },
      "outputs": [],
      "source": [
        "class DecimalFst(GraphFst):\n",
        "    def __init__(self, cardinal: GraphFst):\n",
        "        super().__init__(name=\"decimal\", kind=\"classify\")\n",
        "        cardinal = cardinal.numbers_up_to_million\n",
        "        delete_virgule = pynutil.delete(\"virgule\")\n",
        "\n",
        "        graph_integer = pynutil.insert(\"integer_part: \\\"\") + cardinal + pynutil.insert(\"\\\" \") + delete_space\n",
        "        graph_integer_or_none = graph_integer | pynutil.insert(\"integer_part: \\\"0\\\" \", weight=.001) # In cases we don't always have an integer preceding\n",
        "\n",
        "        graph_string_of_cardinals = delete_space + cardinal\n",
        "        graph_string_of_cardinals = pynini.closure(graph_string_of_cardinals, 1)\n",
        "        graph_fractional = pynutil.insert(\"fractional_part: \\\"\") + graph_string_of_cardinals + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        graph_decimal_no_sign = graph_integer_or_none + pynutil.delete(\"virgule\") + graph_fractional\n",
        "\n",
        "        graph_negative = pynini.cross(\"moins\", \"negative: \\\"-\\\" \") + delete_space\n",
        "        graph_negative = pynini.closure(graph_negative, 0, 1)\n",
        "\n",
        "        graph_decimal = graph_negative + graph_decimal_no_sign\n",
        "\n",
        "        graph = self.add_tokens(graph_decimal)\n",
        "        self.fst = graph.optimize()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjxI5mEKfHLo"
      },
      "source": [
        "### Quantities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WuwWPf3py7G"
      },
      "source": [
        "Recalling our earlier remarks regarding convention in language use, you may find a need to adjust the DecimalFst when processing specific values. For instance, consider the following equivalencies from English:\n",
        "- `1,500,000` = \"one million five hundred thousand\" = \"one point five million\" = `1.5 million`\n",
        "- `2,750,000` = \"two million seven hundred and fifty thousand\" = \"two point seven five million\" = `2.75 million`\n",
        "\n",
        "For large numbers, there is a tendency to use the decimal system as though one is describing a quantity. Notably, there is a minimum value for which this is comfortable. (A speaker of English may say \"three point five trillion\" but \"three point five hundred\" comes off as odd.)\n",
        "\n",
        "This behavior can occur in other languages. For example, the amount of `$1,500,000` may be read in French as \"une virgule cinq million de dollars\" (\"one point five million dollars\").  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgMBIKlYdsGz"
      },
      "source": [
        "Our Classifier can be made to accommodate this behavior: we simply need to repeat what we did for `OrdinalFst` and set aside several key terms to trigger our model. For French, we will choose all terms added for values greater than a million. (Chosen empirically.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEcsUXw5fUEe"
      },
      "outputs": [],
      "source": [
        "suffix = pynini.union(\n",
        "        \"million\",\n",
        "        \"millions\",\n",
        "        \"milliard\",\n",
        "        \"milliards\",\n",
        "        \"billion\",\n",
        "        \"billions\",\n",
        "        \"billiard\",\n",
        "        \"billiards\",\n",
        "        \"trillion\",\n",
        "        \"trillions\",\n",
        "        \"trilliard\",\n",
        "        \"trilliards\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIIUAsR-fgQA"
      },
      "source": [
        "We will then need to use a WFST to graph any numbers the precede these amounts. Note, unlike for our `DecimalFst`, we need to permit cardinals as well as decimals. This is because we want to be able to normalize a phrase like \"three million\" to `3 million` as this will be less obtrusive than `3,000,000`.\n",
        "\n",
        "As such, we will call a `CardinalFst` and a `DecimalFst` in for `graph_quantities`. Since these are both utilized for our `DecimalFst`, it would be more efficient to just pass them along as function/class variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yern-idtycWg"
      },
      "outputs": [],
      "source": [
        "def get_quantity(decimal, cardinal_up_to_thousand):\n",
        "  key_values = pynini.union(\n",
        "        \"million\",\n",
        "        \"millions\",\n",
        "        \"milliard\",\n",
        "        \"milliards\",\n",
        "        \"billion\",\n",
        "        \"billions\",\n",
        "        \"billiard\",\n",
        "        \"billiards\",\n",
        "        \"trillion\",\n",
        "        \"trillions\",\n",
        "        \"trilliard\",\n",
        "        \"trilliards\",\n",
        "    )\n",
        "    # The French WFST that this borrows from has not removed leading zeroes yet.\n",
        "  numbers = cardinal_up_to_thousand @ (\n",
        "      pynutil.delete(pynini.closure(\"0\")) + pynini.difference(NEMO_DIGIT, \"0\") + pynini.closure(NEMO_DIGIT)\n",
        "  )\n",
        "  res = (\n",
        "      pynutil.insert(\"integer_part: \\\"\")\n",
        "      + numbers\n",
        "      + pynutil.insert(\"\\\"\")\n",
        "      + (\n",
        "          pynini.union(delete_hyphen, delete_extra_space)\n",
        "      )  # Can be written either as 'deux-millions' or 'deux millions' depending on whether it registers as a noun or part of cardinal.\n",
        "      + pynutil.insert(\" quantity: \\\"\")\n",
        "      + suffix\n",
        "      + pynutil.insert(\"\\\"\")\n",
        "  )\n",
        "  # Union with decimal to permit either a cardinal or decimal representation.\n",
        "  res |= decimal + delete_extra_space + pynutil.insert(\" quantity: \\\"\") + suffix + pynutil.insert(\"\\\"\")\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT4LMo8ADBAq"
      },
      "source": [
        "We can now insert this into our Classifier, producing the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2KrCuyGDLwh"
      },
      "outputs": [],
      "source": [
        "class DecimalFst(GraphFst):\n",
        "    def __init__(self, cardinal: GraphFst):\n",
        "        super().__init__(name=\"decimal\", kind=\"classify\")\n",
        "        quantities_cardinal = cardinal.graph_hundreds_component_at_least_one_none_zero_digit\n",
        "        cardinal = cardinal.graph_no_exception\n",
        "        delete_virgule = pynutil.delete(\"virgule\")\n",
        "\n",
        "        graph_integer = pynutil.insert(\"integer_part: \\\"\") + cardinal + pynutil.insert(\"\\\" \") + delete_space\n",
        "        graph_integer_or_none = graph_integer | pynutil.insert(\"integer_part: \\\"0\\\" \", weight=.001) # In cases we don't always have an integer preceding\n",
        "\n",
        "        graph_string_of_cardinals = delete_space + cardinal\n",
        "        graph_string_of_cardinals = pynini.closure(graph_string_of_cardinals, 1)\n",
        "        graph_fractional = pynutil.insert(\"fractional_part: \\\"\") + graph_string_of_cardinals + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        graph_decimal_no_sign = graph_integer_or_none + delete_virgule + graph_fractional\n",
        "\n",
        "        graph_negative = pynini.cross(\"moins\", \"negative: \\\"-\\\" \") + delete_space\n",
        "        graph_negative = pynini.closure(graph_negative, 0, 1)\n",
        "        graph_decimal = graph_negative + graph_decimal_no_sign\n",
        "\n",
        "        # Union default decimal with version that accepts quantities\n",
        "        graph_decimal |= graph_negative + get_quantity(\n",
        "            graph_decimal_no_sign, quantities_cardinal\n",
        "        )\n",
        "        final_graph = self.add_tokens(graph_decimal)\n",
        "        self.fst = final_graph.optimize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD-eKqO6qTyh"
      },
      "outputs": [],
      "source": [
        "cardinal = CardinalFst()\n",
        "decimal = DecimalFst(cardinal).fst\n",
        "example = \"trois virgule cent-quatre billion\"\n",
        "apply_fst(example, decimal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiSLKF3RfRZA"
      },
      "source": [
        "## Verbalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnkOV5FlteQA"
      },
      "source": [
        "As before, the Verbalizer is responsible for removing the formatting and rendering a given token in conventional form. As the process remains similar to  Ordinals and Cardinals (deleting strings in a regular matter) we will instead focus on a unique concern for `DecimalFst`: numeral spacing.\n",
        "\n",
        "For some writing systems, decimal numbers and other strings are typically not written as a single string, instead using punctuation to group numbers for clarity. For example, in the United States, integer digits greater than a thousand are separated by commas for every three digits:\n",
        "- `12345.678` -> `12,345.678`\n",
        "\n",
        "A similar rule occurs in French, save it employs spaces on each side of the decimal marker:\n",
        "- `12345,6789` -> `12 345,678 9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h4WQZ1a4Cpc"
      },
      "source": [
        "While simple enough, this rule poses a slight complication: it works from the left and right of the decimal separator, whereas WFSTs process linearly from the beginning (or end) of strings. As such we will need to break the formatting rule into two components: one for the integer component and one for the decimal component."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViOFNdZw4-qu"
      },
      "source": [
        "Starting with the integer component, we need our subcomponent to recognize every three digits and insert a space before. We can achieve this with some `graph_utils` helper objects - `NEMO_DIGIT` and `NEMO_NON_BREAKING_SPACE`, which accept all digits and non-breaking spaces, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z36be2Vo5VbR"
      },
      "outputs": [],
      "source": [
        "every_three_digits = NEMO_DIGIT ** 3 # accepts a string of three digits\n",
        "space_every_three_integer = pynini.closure(NEMO_NON_BREAKING_SPACE + every_three_digits) # inserts space before every three digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSB2gGH-5vwi"
      },
      "source": [
        "However, we cannot let the component insert spaces when there are *only* three digits (e.g. `100`.) As such, we need to make sure the insertion only begins starting from the beginning of a string (e.g. when there is a string between one and three digits.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfWp3ghH6mDQ"
      },
      "outputs": [],
      "source": [
        "space_every_three_integer = pynini.closure(NEMO_DIGIT, 1, 3) + space_every_three_integer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJrQYSfA6vyu"
      },
      "source": [
        "For the case of the decimal spacing, we simply reverse the logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBP6ncTp6yXX"
      },
      "outputs": [],
      "source": [
        "space_every_three_decimal = pynini.closure(NEMO_NON_BREAKING_SPACE + every_three_digits)\n",
        "space_every_three_decimal = space_every_three_decimal + pynini.closure(NEMO_DIGIT, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRXPN_gk69VV"
      },
      "source": [
        "Placed into our Verbalizer, we would see the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h49eztvs7BXH"
      },
      "outputs": [],
      "source": [
        "class DecimalFst(GraphFst):\n",
        "    \"\"\"\n",
        "    Finite state transducer for verbalizing decimal, e.g.\n",
        "        decimal { negative: \"true\" integer_part: \"12\"  fractional_part: \"5006\" quantity: \"billion\" } -> -12.5006 billion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"decimal\", kind=\"verbalize\")\n",
        "\n",
        "        # Need parser to group digits by threes\n",
        "        exactly_three_digits = NEMO_DIGIT ** 3\n",
        "        at_most_three_digits = pynini.closure(NEMO_DIGIT, 1, 3)\n",
        "\n",
        "        space_every_three_integer = (\n",
        "            at_most_three_digits + (pynutil.insert(NEMO_NON_BREAKING_SPACE) + exactly_three_digits).closure()\n",
        "        )\n",
        "        space_every_three_decimal = (\n",
        "            pynini.accep(\",\")\n",
        "            + (exactly_three_digits + pynutil.insert(NEMO_NON_BREAKING_SPACE)).closure()\n",
        "            + at_most_three_digits\n",
        "        )\n",
        "        group_by_threes = space_every_three_integer | space_every_three_decimal\n",
        "        self.group_by_threes = group_by_threes\n",
        "\n",
        "        optional_sign = pynini.closure(pynini.cross(\"negative: \\\"true\\\"\", \"-\") + delete_space, 0, 1)\n",
        "        integer = (\n",
        "            pynutil.delete(\"integer_part:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_NOT_QUOTE, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "        integer = integer @ group_by_threes\n",
        "        optional_integer = pynini.closure(integer + delete_space, 0, 1)\n",
        "        fractional = (\n",
        "            pynutil.insert(\",\")\n",
        "            + pynutil.delete(\"fractional_part:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_NOT_QUOTE, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "        fractional = fractional @ group_by_threes\n",
        "        optional_fractional = pynini.closure(fractional + delete_space, 0, 1)\n",
        "        quantity = (\n",
        "            pynutil.delete(\"quantity:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_NOT_QUOTE, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "        optional_quantity = pynini.closure(pynutil.insert(\" \") + quantity + delete_space, 0, 1)\n",
        "        graph = (optional_integer + optional_fractional + optional_quantity).optimize()\n",
        "        self.numbers = graph # Saving just the part of the graph used for numbers\n",
        "        graph = optional_sign + graph\n",
        "        delete_tokens = self.delete_tokens(graph)\n",
        "        self.fst = delete_tokens.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsToFus_kSR0"
      },
      "source": [
        "Trying out some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P242IbWckSR0"
      },
      "outputs": [],
      "source": [
        "fst = DecimalFst().fst\n",
        "\n",
        "example1 = 'decimal { integer_part: \"3\" fractional_part: \"10453\"  quantity: \"billion\" }'\n",
        "example2 = 'decimal { integer_part: \"22323\" fractional_part: \"104553\" }'\n",
        "\n",
        "apply_fst(example1, fst)\n",
        "apply_fst(example2, fst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZbshZCW8clI"
      },
      "source": [
        "# Money WFST <a id=\"money-wfst\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuiv8HMz7yjm"
      },
      "source": [
        "Now that we've handled some of the foundational classes, it's time to see how they build up to permit more concrete ones. Let's see how the previous WFSTs assist in building a WFST for normalizing currency: the `MoneyFst`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTU2c7MtUpqF"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqyRm8Ru8TDf"
      },
      "source": [
        "While the exact phrasing will vary, a valid string for currency will possess the following qualities:\n",
        "- A major and/or minor denomination of currency\n",
        "- A numeric quantity of the denomination\n",
        "\n",
        "As our `CardinalFst` and `OrdinalFst` already allow us to normalize the quantity, the only issue for `MoneyFst` is to graph the amounts and build a vocabulary to recognize the denominations.\n",
        "\n",
        "For French, we will use the following examples to build upon:\n",
        "- \"une euros\" -> `1 €`\n",
        "- \"deux euros\" -> `2 €`\n",
        "- \"deux euros cinq\" -> `2,5 €`\n",
        "- \"cinq centimes\" -> `0,5 €`\n",
        "- \"deux billions de euros\" -> `2 billions de euros`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMqUir9n9_cA"
      },
      "source": [
        "These suggest the following requirements of our grammar:\n",
        "- There must be a mapping between \"euro\" and \"centime\" and `€` in our vocabulary\n",
        "- This mapping must allow both singular and plural forms\n",
        "- The currency denomination is phrased between major and minor denominations (\"une euro cinq\" and not \"une cinq euro\")\n",
        "- Large quantities of currency are left 'as is' instead of normalized\n",
        "\n",
        "We may deal with the vocabulary in the typical fashion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN9nbNhB-vEV"
      },
      "outputs": [],
      "source": [
        "major_currency = pynini.string_map([(\"euro\", \"€\")])\n",
        "minor_currency = pynini.string_map([(\"centime\", \"€\")])\n",
        "\n",
        "graph_plural = pynutil.delete(\"s\").ques\n",
        "\n",
        "major_currency += graph_plural\n",
        "minor_currency += graph_plural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aHrm1qPAc-f"
      },
      "source": [
        "Moving to the numbers, note that we need to append a leading zero to the value of fractional currency amounts (\"five cents\" -> `$0.05`). We bring back the subgraph from `CardinalFst` that maps tokens to numbers without tokenization to assist with this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwi-yQW1AjvG"
      },
      "outputs": [],
      "source": [
        "from nemo_text_processing.inverse_text_normalization.fr.taggers import cardinal\n",
        "\n",
        "cardinal_graph = cardinal.CardinalFst()\n",
        "graph_cardinal = cardinal_graph.graph_no_exception # graphs cardinals w/o tokenization\n",
        "\n",
        "add_leading_zero_to_double_digit = (NEMO_DIGIT + NEMO_DIGIT) | (pynutil.insert(\"0\") + NEMO_DIGIT)\n",
        "graph_fractional_values = graph_cardinal @ add_leading_zero_to_double_digit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCkp2wNBkSR1"
      },
      "source": [
        "Now, let us consider how to manage arge quantities of currency. In our example (\"deux billions de euros\" -> `2 billions de euros`) we see that its behavior mirrors that of our `get_quantity` portion of `DecimalFst`. As such, it would be useful if there was a subcomponent of that graph that we could use in here. Like in the case of `CardinalFst`, let us go back and create a subgraph for later use. Since all our quantities are positive, this would be best accomplished right before incorporating the `negative` property, creating a `self.final_graph_wo_negative`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glA9IOOckSR1"
      },
      "outputs": [],
      "source": [
        "class DecimalFst(GraphFst):\n",
        "    def __init__(self, cardinal: GraphFst):\n",
        "        super().__init__(name=\"decimal\", kind=\"classify\")\n",
        "        quantities_cardinal = cardinal.graph_hundreds_component_at_least_one_none_zero_digit\n",
        "        cardinal = cardinal.graph_no_exception\n",
        "        delete_virgule = pynutil.delete(\"virgule\")\n",
        "\n",
        "        graph_integer = pynutil.insert(\"integer_part: \\\"\") + cardinal + pynutil.insert(\"\\\" \") + delete_space\n",
        "        graph_integer_or_none = graph_integer | pynutil.insert(\"integer_part: \\\"0\\\" \", weight=.001) # In cases we don't always have an integer preceding\n",
        "\n",
        "        graph_string_of_cardinals = delete_space + cardinal\n",
        "        graph_string_of_cardinals = pynini.closure(graph_string_of_cardinals, 1)\n",
        "        graph_fractional = pynutil.insert(\"fractional_part: \\\"\") + graph_string_of_cardinals + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        graph_decimal_no_sign = graph_integer_or_none + delete_virgule + graph_fractional\n",
        "\n",
        "        ### NEW GRAPH HERE\n",
        "        self.final_graph_wo_negative = graph_decimal_no_sign | get_quantity(\n",
        "            final_graph_wo_sign, cardinal.graph_hundreds_component_at_least_one_none_zero_digit\n",
        "        )\n",
        "\n",
        "        graph_negative = pynini.cross(\"moins\", \"negative: \\\"-\\\" \") + delete_space\n",
        "        graph_negative = pynini.closure(graph_negative, 0, 1)\n",
        "        graph_decimal = graph_negative + graph_decimal_no_sign\n",
        "\n",
        "        # Union default decimal with version that accepts quantities\n",
        "        graph_decimal |= graph_negative + get_quantity(\n",
        "            graph_decimal_no_sign, quantities_cardinal\n",
        "        )\n",
        "        final_graph = self.add_tokens(graph_decimal)\n",
        "        self.fst = final_graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHW9uy34kSR1"
      },
      "source": [
        "Allowing us to change our grammar to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcnL-GPPkSR1"
      },
      "outputs": [],
      "source": [
        "from nemo_text_processing.inverse_text_normalization.fr.taggers import cardinal, decimal\n",
        "\n",
        "cardinal_graph = cardinal.CardinalFst()\n",
        "decimal_graph = decimal.DecimalFst(cardinal_graph)\n",
        "\n",
        "graph_cardinal = cardinal_graph.graph_no_exception # graphs cardinals w/o tokenization\n",
        "graph_decimal = decimal_graph.final_graph_wo_negative # graphs positive decimals w/o tokenization\n",
        "\n",
        "add_leading_zero_to_double_digit = (NEMO_DIGIT + NEMO_DIGIT) | (pynutil.insert(\"0\") + NEMO_DIGIT)\n",
        "graph_fractional_values = graph_cardinal @ add_leading_zero_to_double_digit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1RHoW-TLzIz"
      },
      "source": [
        "Note that by doing this, we're also incorporating the formatting from the `decimal` class up to this point. Since these overlap with the `money` class (see next section), we have saved ourselves some work.  \n",
        "\n",
        "Since we already made `graph_quantity` part of our `DecimalFst`, we can avoid dealing with large quantities now. However, this does mean we still need a way to leave currencies 'as is' without normalization. We can do this by using the `project` method, which will create a WFST that excepts either all valid inputs or all valid outputs of another WFST (depending on argument)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l_TLtJkMluU"
      },
      "outputs": [],
      "source": [
        "major_currency_no_normalize = major_currency.project(\"input\")\n",
        "apply_fst(\"euro\", major_currency_no_normalize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raBdHc_WXEpG"
      },
      "source": [
        "We then append this WFST with a WFST that recognizes prepositions commonly used before large values of currency (\"d'\", \"des\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEuxiVgDXRBf"
      },
      "outputs": [],
      "source": [
        "graph_preposition = pynini.union(\"des \", \"d'\") # Used for large amounts (billions de euros)\n",
        "major_currency_no_normalize = pynini.closure(graph_preposition, 0, 1) + major_currency.project(\"input\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXmf8Fq_Rm1"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5BBuQRzLuXS"
      },
      "source": [
        "For the Money semiotic class, we have available the following properties for tokenization:\n",
        "- `integer_part`\n",
        "- `fractional_part`\n",
        "- `currency`\n",
        "\n",
        "Laying the initial groundwork seems simple enough. We first instantiate our `MoneyFst` classifier with our initial grammars:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZaCeHcFWVP3"
      },
      "outputs": [],
      "source": [
        "class MoneyFst(GraphFst):\n",
        "    def __init__(self, cardinal: GraphFst, decimal: GraphFst):\n",
        "        super().__init__(name=\"money\", kind=\"classify\")\n",
        "        major_currency = pynini.string_map([(\"euro\", \"€\")])\n",
        "        minor_currency = pynini.string_map([(\"centime\", \"€\")])\n",
        "\n",
        "        graph_plural = pynutil.delete(\"s\").ques\n",
        "\n",
        "        major_currency += graph_plural\n",
        "        minor_currency += graph_plural\n",
        "\n",
        "        major_currency_no_normalize = major_currency.project(\"input\")\n",
        "        graph_preposition = pynini.union(\"des \", \"d'\") # Used for large amounts (billions de euros)\n",
        "        major_currency_no_normalize = graph_preposition + major_currency.project(\"input\")\n",
        "\n",
        "        graph_cardinal = cardinal.graph_no_exception\n",
        "        graph_decimal = decimal.final_graph_wo_negative\n",
        "\n",
        "        add_leading_zero_to_double_digit = (NEMO_DIGIT + NEMO_DIGIT) | (pynutil.insert(\"0\") + NEMO_DIGIT)\n",
        "        graph_fractional_values = graph_cardinal @ add_leading_zero_to_double_digit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bpkXroLWaBo"
      },
      "source": [
        "Let us now manage the `currency` property. We have the following scenarios to consider:\n",
        "- Major denomination only\n",
        "- Minor denomination only\n",
        "- Major denomination and implicit minor denomination (\"cinq euro trois\")\n",
        "- Major denomination and explicit minor denomination (\"cinq euros et trois centimes\")\n",
        "- Large quantities of euros (\"cinq billion des euros\")\n",
        "\n",
        "Note how across cases the use of `graph_cardinal` and `graph_decimal` will be applied differently. Further, we may have varying orders in which tags are assigned proper values. For instance, if we have only minor denomination we would assign `fractional_part` before `currency`. Meanwhile, major denomination and implicit minor denomination would be the order of `integer_part`, `currency`, `fractional_part`. While we could try and figure out a way to preserve order, recall that the use of permutations in NeMo ITN makes that unnecessary: we can assume the desired order of tags reach our Verbalizer without make overt efforts in our Classifier!\n",
        "\n",
        "For example, let's say we need to process \"five dollars\" as `$5.00`. Processed linearly, we could get a token sequence along the lines of: `{ integer_part: \"5\" currency: \"$\" }`. If we passed this token array straight to a Verbalizer, we would need to configure a graph that effectively reverses the order so we could parse the `currency` field prior to the `integer_part` field, perhaps something along the lines of:\n",
        "\n",
        "`pynutil.insert(\"$\") + delete_space + pynutil.delete('integer_part: \\\"') +.... + pynutil.delete('currency: \"$\"')`\n",
        "\n",
        "But since NeMo creates permutations of our Classifier outputs, this is unnecessary. We can simply assume whatever would be the most convenient order for us (e.g. `{ currency: \"$\" integer_part: \"5\" }`) and build our Verbalizer around that:\n",
        "\n",
        "`pynutil.delete('currency: \\\"') + NEMO_SIGMA + pynutil.delete('\\\" integer_part: \\\"') + NEMO_DIGIT +...`\n",
        "\n",
        "Along with helping to keep our script simpler (we can focus simply on tokenization and not worry about what input order our Verbalizers will accept), this also allows us to overcome structural constraints of WFSTs, namely that they are [limited in reordering text strings](https://en.wikipedia.org/wiki/Pushdown_automaton)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMZ13D2Dh9ZF"
      },
      "source": [
        "Keeping this in mind, let's begin mapping the proper tags. Since they're relatively simple, we can start with only major and minor denominations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtwWLp7VbbjM"
      },
      "outputs": [],
      "source": [
        "graph_integer_component = pynutil.insert(\"integer_part: \\\"\") + graph_cardinal + pynutil.insert(\"\\\"\")\n",
        "graph_fractional_component =  pynutil.insert(\"fractional_part: \\\"\") + graph_fractional_values + pynutil.insert(\"\\\"\")\n",
        "\n",
        "graph_major_currency = pynutil.insert(\" currency: \\\"\") + major_currency + pynutil.insert(\"\\\"\")\n",
        "graph_minor_currency = pynutil.insert(\" currency: \\\"\") + minor_currency + pynutil.insert(\"\\\"\")\n",
        "\n",
        "graph_only_major_money = graph_integer_component + delete_space + graph_major_currency\n",
        "graph_only_minor_money = graph_fractional_component + delete_space + graph_minor_currency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTmxrK4DmS39"
      },
      "source": [
        "Now we may append the case of an implicit `fractional_part` to `graph_only_major_money`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvzn3pQinkT0"
      },
      "outputs": [],
      "source": [
        "implicit_fractional_part =  delete_space + pynutil.insert(\"fractional_part: \\\"\") + graph_fractional_values + pynutil.insert(\"\\\"\")\n",
        "implicit_fractional_part = pynini.closure(implicit_fractional_part, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKFZkCVmn1OX"
      },
      "source": [
        "And the explicit fractional portion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_h0pTlMn3jz"
      },
      "outputs": [],
      "source": [
        "delete_et = pynutil.delete(\"et \") # Sometimes prefaces the minor currency\n",
        "delete_et = pynini.closure(delete_et, 0 , 1)\n",
        "\n",
        "delete_minor = pynutil.delete(minor_currency.project(\"input\")) # to remove the minor currency\n",
        "\n",
        "explicit_fractional_part = pynutil.insert(\"fractional_part: \\\"\") + graph_fractional_values + pynutil.insert(\"\\\"\")\n",
        "explicit_fractional_part = delete_space + delete_et + explicit_fractional_part + delete_space + delete_minor\n",
        "explicit_fractional_part = pynini.closure(explicit_fractional_part, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvnpAudgo-o3"
      },
      "source": [
        "We join them together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYzlIRWTpD8e"
      },
      "outputs": [],
      "source": [
        "graph_major_money = graph_only_major_money + (implicit_fractional_part | explicit_fractional_part)\n",
        "graph_standard_money = graph_major_money | graph_only_minor_money"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzeaKXVzpYs8"
      },
      "source": [
        "Finishing with the case the large quantities of money, we need to use `graph_decimal` so we can exploit its ability to map quantities. Note that since we are using a pre-existing WFST, we can ignore inserting the tags ourselves, since this is already done by the Decimal WFST. As long as we remember to process this aspect with our Verbalizer, we can spare ourselves the extra step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnqX9mGFpmJm"
      },
      "outputs": [],
      "source": [
        "graph_large_money = pynutil.insert(\" currency: \\\"\") + major_currency_no_normalize + pynutil.insert(\"\\\"\")\n",
        "graph_large_money = graph_decimal + delete_space + graph_large_money"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24TUZnJKqgPA"
      },
      "source": [
        "Alltogether, this would give the following Classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7-muCO2qizg"
      },
      "outputs": [],
      "source": [
        "class MoneyFst(GraphFst):\n",
        "    def __init__(self, cardinal: GraphFst, decimal: GraphFst):\n",
        "        super().__init__(name=\"money\", kind=\"classify\")\n",
        "        major_currency = pynini.string_map([(\"euro\", \"€\")])\n",
        "        minor_currency = pynini.string_map([(\"centime\", \"€\")])\n",
        "\n",
        "        graph_plural = pynutil.delete(\"s\").ques\n",
        "\n",
        "        major_currency += graph_plural\n",
        "        minor_currency += graph_plural\n",
        "\n",
        "        major_currency_no_normalize = major_currency.project(\"input\")\n",
        "        graph_preposition = pynini.union(\"des \", \"d'\") # Used for large amounts (billions de euros)\n",
        "        major_currency_no_normalize = graph_preposition + major_currency.project(\"input\")\n",
        "\n",
        "        graph_cardinal = cardinal.graph_no_exception\n",
        "        graph_decimal = decimal.final_graph_wo_negative\n",
        "\n",
        "        add_leading_zero_to_double_digit = (NEMO_DIGIT + NEMO_DIGIT) | (pynutil.insert(\"0\") + NEMO_DIGIT)\n",
        "        graph_fractional_values = graph_cardinal @ add_leading_zero_to_double_digit\n",
        "\n",
        "        graph_integer_component = pynutil.insert(\"integer_part: \\\"\") + graph_cardinal + pynutil.insert(\"\\\"\")\n",
        "        graph_fractional_component =  pynutil.insert(\"fractional_part: \\\"\") + graph_fractional_values + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        graph_major_currency = pynutil.insert(\" currency: \\\"\") + major_currency + pynutil.insert(\"\\\"\")\n",
        "        graph_minor_currency = pynutil.insert(\" currency: \\\"\") + minor_currency + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        graph_only_major_money = graph_integer_component + delete_space + graph_major_currency\n",
        "        graph_only_minor_money = graph_fractional_component + delete_space + graph_minor_currency\n",
        "\n",
        "        implicit_fractional_part =  delete_space + pynutil.insert(\"fractional_part: \\\"\") + graph_fractional_values + pynutil.insert(\"\\\"\")\n",
        "        implicit_fractional_part = pynini.closure(implicit_fractional_part, 0, 1)\n",
        "\n",
        "\n",
        "        delete_et = pynutil.delete(\"et \") # Sometimes prefaces the minor currency\n",
        "        delete_et = pynini.closure(delete_et, 0 , 1)\n",
        "\n",
        "        delete_minor = pynutil.delete(minor_currency.project(\"input\")) # to remove the minor currency\n",
        "\n",
        "        explicit_fractional_part = pynutil.insert(\"fractional_part: \\\"\") + graph_fractional_values + pynutil.insert(\"\\\"\")\n",
        "        explicit_fractional_part = delete_space + delete_et + explicit_fractional_part + delete_space + delete_minor\n",
        "        explicit_fractional_part = pynini.closure(explicit_fractional_part, 0, 1)\n",
        "\n",
        "        graph_major_money = graph_only_major_money + (implicit_fractional_part | explicit_fractional_part)\n",
        "\n",
        "        graph_large_money = pynutil.insert(\" currency: \\\"\") + major_currency_no_normalize + pynutil.insert(\"\\\"\")\n",
        "        graph_large_money = graph_decimal + delete_space + graph_large_money\n",
        "\n",
        "        final_graph = graph_large_money | graph_major_money | graph_only_minor_money\n",
        "\n",
        "        final_graph = self.add_tokens(final_graph)\n",
        "        self.fst = final_graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6mW7qdWkSR2"
      },
      "source": [
        "Let's see the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrE8S1m1kSR2"
      },
      "outputs": [],
      "source": [
        "from nemo_text_processing.inverse_text_normalization.fr.taggers import decimal, cardinal\n",
        "\n",
        "cardFst = cardinal.CardinalFst()\n",
        "decFst = decimal.DecimalFst(cardFst)\n",
        "\n",
        "moneyFst = MoneyFst(cardFst, decFst).fst\n",
        "\n",
        "example = \"douze virgule cinq billions d'euros\"\n",
        "\n",
        "apply_fst(example, moneyFst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxdcyuLmAZZa"
      },
      "source": [
        "## Verbalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZFDWNwY6sOG"
      },
      "source": [
        "By this point, the creation of the Verbalizer should be rather straight-forward - delete the expected tokens and perform any specific formatting that was not caught by the Classifier.\n",
        "\n",
        "In fact, it is so straight-forward that much of the work does not even need to be explicitly managed by the Verbalizer. As mentioned previously, two of the properties we inserted in our Classifier where already referenced in our `DecimalFst` - `integer_part` and `fractional_part`. We even went so far to directly call a component of `DecimalFst` in our Classifier. As such, outside of the `currency` property - there is little in our Money token that is different from a standard Decimal token. Indeed, even the normalized forms are similar (`200,5` vs. `200,5 €`.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7sgH0t79tmU"
      },
      "source": [
        "Given these similarities, it seems that we can save ourselves some work and simply use the Decimal Verbalizer to manage much of the normalization. Let's look at the basic format of our `MoneyFst` verbalizer, writing it so it accepts a `DecimalFst` as input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEu8nITP9mSG"
      },
      "outputs": [],
      "source": [
        "class MoneyFst(GraphFst):\n",
        "    def __init__(self, decimal: GraphFst):\n",
        "        super().__init__(name=\"money\", kind=\"verbalize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYVLou5N-Dk8"
      },
      "source": [
        "We manage the issue of deleting the `currency` property:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO35tJ7G-H6N"
      },
      "outputs": [],
      "source": [
        "class MoneyFst(GraphFst):\n",
        "    def __init__(self, decimal: GraphFst):\n",
        "        super().__init__(name=\"money\", kind=\"verbalize\")\n",
        "        unit = (\n",
        "            pynutil.delete(\"currency:\")\n",
        "            + delete_extra_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_NOT_QUOTE, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDS8XSII-Dpd"
      },
      "source": [
        "Now consider, we need to normalize an integer component, a fractional component, and a decimal to separate them. Since NeMo will automatically permutate all tags, we can assume whatever order we want. As such, we can assume we get the exact order that is accepted by our `DecimalFst`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtGfpjVA-r3u"
      },
      "outputs": [],
      "source": [
        "    def __init__(self, decimal: GraphFst):\n",
        "        super().__init__(name=\"money\", kind=\"verbalize\")\n",
        "        unit = (\n",
        "            pynutil.delete(\"currency:\")\n",
        "            + delete_extra_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_NOT_QUOTE, 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "        graph = decimal.numbers + delete_space + unit\n",
        "        delete_tokens = self.delete_tokens(graph)\n",
        "        self.fst = delete_tokens.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZefxZLIU-uRU"
      },
      "source": [
        "It is as simple and compact as appending the `unit` component to the preexisting `decimal.numbers`.\n",
        "\n",
        "This feature is worth keeping in mind as you build up to more concrete classes: the combination of guaranteed tag permutations and prebuilt Verbalizers make the addition of semiotic classes progressively simpler despite the building complexity of your entire grammar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WydC7Cn28l5Y"
      },
      "source": [
        "# Time WFST <a id=\"time-wfst\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VelunbumCJJe"
      },
      "source": [
        "Our next composite graph will be for the Time WFST. Here, you may see more variation between your language and our example than with our previous classes. This is for a number of reasons, among them being that while there may be some standard cross linguistic patterns regarding time (e.g. `quantity_of_hours + quantity_of_minutes`), the use of various equivalent phrases can make an exhaustive grammar incredibly specific (e.g. consider managing \"twelve fifteen\", \"twelve and a quarter\", \"quarter past twelve\", \"quarter after twelve\", and \"forty five until one\" all together). You may find yourself drawing upon WFSTs that accommodate Cardinals, Fractions, and some basic subtraction.\n",
        "\n",
        "As such, we are going to focus on those aspects of the Time WFST that are necessary for a functional normalization of time related phrases, saving a more exhaustive grammar for your own specific languages and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wqb28wzATOR"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVntDM3AEz0v"
      },
      "source": [
        "For our Time WFST, we will focus on the following aspects:\n",
        "- Use of 24 or 12 hour base\n",
        "- Use of fraction terminology (e.g. \"quarter\" = `15`)\n",
        "- Accommodation of key-words (\"noon\", \"midnight\")\n",
        "- Counting backwards from the hour (\"ten to five\", \"five to three\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seU9hTbgFgu7"
      },
      "source": [
        "We'll start with the basic system.\n",
        "\n",
        "For French, time operates on a twenty-four hour system, with the zeroth hour being midnight. Time is given in the following format:\n",
        "\n",
        "`cardinal + heure(s) + (cardinal)`\n",
        "\n",
        "This is normalized as:\n",
        "\n",
        "`cardinal h (cardinal)`\n",
        "\n",
        "For instance, for `3:03`, we would have:\n",
        "- input: \"trois heures trois\"\n",
        "- output: `3 h 03`\n",
        "\n",
        "As such, our grammar needs to utilize a Cardinal WFST and have a means to accept \"heures\" from the input. Taking care of the latter case is simple enough:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTSVxf4fI_ND"
      },
      "outputs": [],
      "source": [
        "graph_heures = pynini.accep(\"heure\") + pynini.accep(\"s\").ques\n",
        "graph_heures = pynutil.delete(graph_heures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LW7pXaXJSZa"
      },
      "source": [
        "For the cardinals, we could pass an instance of `CardinalFST` to our graph. But do we really need that level of coverage? We only really need to cover the numbers 0 - 60, which we could simply write a new WFST for. Further, it may be beneficial to allow our graph to separate possible ambiguity. While we will not cover it in our tutorial, you may in the future find it necessary to build a WFST for Measurements, of which quantities of time may play a part. Would it not be helpful for you WFST to know that \"thirty hours\" could only ever be a measurement instead of a possible time of day?\n",
        "\n",
        "Given the little amount of effort necessary and the quick benefit, we choose to make our hours and minutes explicit in the Time WFST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4aa06ZPLKIR"
      },
      "outputs": [],
      "source": [
        "hours = pynini.string_map([\n",
        "                           (\"zéro\",\"0\"),\n",
        "                           (\"une\",\"1\"),\n",
        "                           (\"deux\",\"2\"),\n",
        "                           (\"trois\",\"3\"),\n",
        "                           (\"quatre\",\"4\"),\n",
        "                           (\"cinq\",\"5\"),\n",
        "                           (\"six\",\"6\"),\n",
        "                           (\"sept\",\"7\"),\n",
        "                           (\"huit\",\"8\"),\n",
        "                           (\"neuf\",\"9\"),\n",
        "                           (\"dix\",\"10\"),\n",
        "                           (\"onze\",\"11\"),\n",
        "                           (\"douze\",\"12\"),\n",
        "                           (\"treize\",\"13\"),\n",
        "                           (\"quatorze\",\"14\"),\n",
        "                           (\"quinze\",\"15\"),\n",
        "                           (\"seize\",\"16\"),\n",
        "                           (\"dix-sept\",\"17\"),\n",
        "                           (\"dix-huit\",\"18\"),\n",
        "                           (\"dix-neuf\",\"19\"),\n",
        "                           (\"vingt\",\"20\"),\n",
        "                           (\"vingt-et-une\",\"21\"),\n",
        "                           (\"vingt et une\",\"21\"),\n",
        "                           (\"vingt-deux\",\"22\"),\n",
        "                           (\"vingt-trois\",\"23\"),\n",
        "                           (\"vingt-quatre\",\"24\"),\n",
        "])\n",
        "minutes = pynini.string_map([\n",
        "                  (\"une\", \"01\"),\n",
        "                  (\"deux\", \"02\"),\n",
        "                  (\"trois\", \"03\"),\n",
        "                  (\"quatre\", \"04\"),\n",
        "                  (\"cinq\", \"05\"),\n",
        "                  (\"six\", \"06\"),\n",
        "                  (\"sept\", \"07\"),\n",
        "                  (\"huit\", \"08\"),\n",
        "                  (\"neuf\", \"09\"),\n",
        "                  (\"dix\", \"10\"),\n",
        "                  (\"onze\", \"11\"),\n",
        "                  (\"douze\", \"12\"),\n",
        "                  (\"treize\", \"13\"),\n",
        "                  (\"quatorze\", \"14\"),\n",
        "                  (\"quinze\", \"15\"),\n",
        "                  (\"seize\", \"16\"),\n",
        "                  (\"dix-sept\", \"17\"),\n",
        "                  (\"dix-huit\", \"18\"),\n",
        "                  (\"dix-neuf\", \"19\"),\n",
        "                  (\"vingt\", \"20\"),\n",
        "                  (\"vingt-et-une\", \"21\"),\n",
        "                  (\"vingt et une\", \"21\"),\n",
        "                  (\"vingt-deux\", \"22\"),\n",
        "                  (\"vingt-trois\", \"23\"),\n",
        "                  (\"vingt-quatre\", \"27\"),\n",
        "                  (\"vingt-cinq\", \"25\"),\n",
        "                  (\"vingt-six\", \"26\"),\n",
        "                  (\"vingt-sept\", \"27\"),\n",
        "                  (\"vingt-huit\", \"28\"),\n",
        "                  (\"vingt-neuf\", \"29\"),\n",
        "                  (\"trente\", \"30\"),\n",
        "                  (\"trente-et-une\", \"31\"),\n",
        "                  (\"trente et une\", \"31\"),\n",
        "                  (\"trente-deux\", \"32\"),\n",
        "                  (\"trente-trois\", \"33\"),\n",
        "                  (\"trente-quatre\", \"34\"),\n",
        "                  (\"trente-cinq\", \"35\"),\n",
        "                  (\"trente-six\", \"36\"),\n",
        "                  (\"trente-sept\", \"37\"),\n",
        "                  (\"trente-huit\", \"38\"),\n",
        "                  (\"trente-neuf\", \"39\"),\n",
        "                  (\"quarante\", \"40\"),\n",
        "                  (\"quarante-et-une\", \"41\"),\n",
        "                  (\"quarante et une\", \"41\"),\n",
        "                  (\"quarante-deux\", \"42\"),\n",
        "                  (\"quarante-trois\", \"43\"),\n",
        "                  (\"quarante-quatre\", \"44\"),\n",
        "                  (\"quarante-cinq\", \"45\"),\n",
        "                  (\"quarante-six\", \"46\"),\n",
        "                  (\"quarante-sept\", \"47\"),\n",
        "                  (\"quarante-huit\", \"48\"),\n",
        "                  (\"quarante-neuf\", \"49\"),\n",
        "                  (\"cinquante\", \"50\"),\n",
        "                  (\"cinquante-et-une\", \"51\"),\n",
        "                  (\"cinquante et une\", \"51\"),\n",
        "                  (\"cinquante-deux\", \"52\"),\n",
        "                  (\"cinquante-trois\", \"53\"),\n",
        "                  (\"cinquante-quatre\", \"54\"),\n",
        "                  (\"cinquante-cinq\", \"55\"),\n",
        "                  (\"cinquante-six\", \"56\"),\n",
        "                  (\"cinquante-sept\", \"57\"),\n",
        "                  (\"cinquante-huit\", \"58\"),\n",
        "                  (\"cinquante-neuf\", \"59\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SmNsNKLM9cC"
      },
      "source": [
        "Now that we've managed the basic graph, we can address some of the more niche rules of French timekeeping.\n",
        "\n",
        "To start, French employs some colloquialisms that will be familiar to English speakers: minutes that are multiples of fifteen are referred to as fractions of a clock. In particular:\n",
        "- `5 h 15` -> \"cinq heures **et quart**\"\n",
        "- `5 h 30` -> \"cinq heures **et demie**\"\n",
        "- `5 h 45` -> \"cinq eures **et trois quarts**\"\n",
        "\n",
        "We thus need a means of rendering these as their numerical equivalents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHe3nfrpSlrE"
      },
      "outputs": [],
      "source": [
        "# Mapping 'et demi' and 'et qart'\n",
        "graph_et = pynutil.delete(\"et\") + delete_space\n",
        "\n",
        "graph_demi = pynini.accep(\"demi\")\n",
        "graph_demi += pynini.accep(\"e\").ques  # people vary on feminine or masculine form\n",
        "graph_demi = pynini.cross(graph_demi, \"30\")\n",
        "\n",
        "graph_quart = pynini.accep('quart')\n",
        "graph_quart = pynini.cross(graph_quart, '15')\n",
        "graph_trois_quart = pynini.cross(\"trois quarts\", \"45\")\n",
        "\n",
        "graph_fractions = graph_demi | graph_quart | graph_trois_quart\n",
        "graph_fractions = graph_et + graph_fractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD2wobIQS3fX"
      },
      "source": [
        "Also like English, French will use key words to designate a specific timeslot. Noon and midnight are \"midi\" and \"minuit\" respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahbkiZFuTN2t"
      },
      "outputs": [],
      "source": [
        "# Midi and minuit\n",
        "graph_midi = pynini.cross(\"midi\", \"12\")\n",
        "graph_minuit = pynini.cross(\"minuit\", \"0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OyMoqfZTX1U"
      },
      "source": [
        "Now it's time to throw a wrench into things: counting backwards from the hour. How are we to get what is essentially a graph to do the subtraction necessarily for \"ten to twelve\" to become `11:50`?\n",
        "\n",
        "Easy: we build the subtraction into the graph itself. That is, we map the hours and minutes produced by our graph onto another graph that produces their amount shifted back a value.\n",
        "\n",
        "Let's take our \"ten to twelve\" example. Normally \"ten\" would map to `10` and \"twelve\" to `12`. But with these new graphs, the detection of the pattern `minute + to + hour` would signal that `10` should now become `50` and `12` become `11`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMWifbm1VQjP"
      },
      "source": [
        "Let us do this for our French example. Luckily enough, the indication that a French string is regular: counting backwards from the hour is by use of the pattern `cardinal + heures + moins + minutes`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4bV3T1pViCH"
      },
      "outputs": [],
      "source": [
        "hours_to = pynini.string_map([\n",
        "                           (\"1\",\"0\"),\n",
        "                           (\"2\",\"1\"),\n",
        "                           (\"3\",\"2\"),\n",
        "                           (\"4\",\"3\"),\n",
        "                           (\"5\",\"4\"),\n",
        "                           (\"6\",\"5\"),\n",
        "                           (\"7\",\"6\"),\n",
        "                           (\"8\",\"7\"),\n",
        "                           (\"9\",\"8\"),\n",
        "                           (\"10\",\"9\"),\n",
        "                           (\"11\",\"10\"),\n",
        "                           (\"12\",\"11\"),\n",
        "                           (\"13\",\"12\"),\n",
        "                           (\"14\",\"13\"),\n",
        "                           (\"15\",\"14\"),\n",
        "                           (\"16\",\"15\"),\n",
        "                           (\"17\",\"16\"),\n",
        "                           (\"18\",\"17\"),\n",
        "                           (\"19\",\"18\"),\n",
        "                           (\"20\",\"19\"),\n",
        "                           (\"21\",\"20\"),\n",
        "                           (\"22\",\"21\"),\n",
        "                           (\"23\",\"22\"),\n",
        "                           (\"24\",\"23\"),\n",
        "                           (\"0\",\"23\"),\n",
        "])\n",
        "minutes_to = pynini.string_map([\n",
        "                  (\"59\", \"01\"),\n",
        "                  (\"58\", \"02\"),\n",
        "                  (\"57\", \"03\"),\n",
        "                  (\"56\", \"04\"),\n",
        "                  (\"55\", \"05\"),\n",
        "                  (\"54\", \"06\"),\n",
        "                  (\"53\", \"07\"),\n",
        "                  (\"52\", \"08\"),\n",
        "                  (\"51\", \"09\"),\n",
        "                  (\"50\", \"10\"),\n",
        "                  (\"49\", \"11\"),\n",
        "                  (\"48\", \"12\"),\n",
        "                  (\"47\", \"13\"),\n",
        "                  (\"46\", \"14\"),\n",
        "                  (\"45\", \"15\"),\n",
        "                  (\"44\", \"16\"),\n",
        "                  (\"43\", \"17\"),\n",
        "                  (\"42\", \"18\"),\n",
        "                  (\"41\", \"19\"),\n",
        "                  (\"40\", \"20\"),\n",
        "                  (\"39\", \"21\"),\n",
        "                  (\"38\", \"22\"),\n",
        "                  (\"37\", \"23\"),\n",
        "                  (\"36\", \"24\"),\n",
        "                  (\"35\", \"25\"),\n",
        "                  (\"34\", \"26\"),\n",
        "                  (\"33\", \"27\"),\n",
        "                  (\"32\", \"28\"),\n",
        "                  (\"31\", \"29\"),\n",
        "                  (\"30\", \"30\"),\n",
        "                  (\"29\", \"31\"),\n",
        "                  (\"28\", \"32\"),\n",
        "                  (\"27\", \"33\"),\n",
        "                  (\"26\", \"34\"),\n",
        "                  (\"25\", \"35\"),\n",
        "                  (\"24\", \"36\"),\n",
        "                  (\"23\", \"37\"),\n",
        "                  (\"22\", \"38\"),\n",
        "                  (\"21\", \"39\"),\n",
        "                  (\"20\", \"40\"),\n",
        "                  (\"19\", \"41\"),\n",
        "                  (\"18\", \"42\"),\n",
        "                  (\"17\", \"43\"),\n",
        "                  (\"16\", \"44\"),\n",
        "                  (\"15\", \"45\"),\n",
        "                  (\"14\", \"46\"),\n",
        "                  (\"13\", \"47\"),\n",
        "                  (\"12\", \"48\"),\n",
        "                  (\"11\", \"49\"),\n",
        "                  (\"10\", \"50\"),\n",
        "                  (\"09\", \"51\"),\n",
        "                  (\"08\", \"52\"),\n",
        "                  (\"07\", \"53\"),\n",
        "                  (\"06\", \"54\"),\n",
        "                  (\"05\", \"55\"),\n",
        "                  (\"04\", \"56\"),\n",
        "                  (\"03\", \"57\"),\n",
        "                  (\"02\", \"58\"),\n",
        "                  (\"01\", \"59\"),\n",
        "])\n",
        "graph_moins = pynutil.delete(\"moins\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOKETkIYZy5M"
      },
      "source": [
        "Why graph the digits instead of the tokens themselves? Along with avoiding some minor repetition and making editing more apparent, it allows this subgraph to be ported to other languages - if so desired.\n",
        "\n",
        "Further, it helps us illustrate a helpful idea within this tutorial: as long as a pattern is regular and/or finite, it is no major issue to accommodate it in our graph, regardless of mathematic or logic system it employs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJbFiD2fAUc5"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK0SGXntaDkI"
      },
      "source": [
        "Once again we place the grammar within the proper child class of `GraphFst`. We also insert the proper tags for the `Time` class, which are:\n",
        "- `hours`\n",
        "- `minutes`\n",
        "- `suffix` (explained within this section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Eq5r-_VbBIg"
      },
      "outputs": [],
      "source": [
        "graph_hours_component = pynini.union(hours, graph_midi, graph_minuit)\n",
        "graph_hours_component = pynutil.insert(\"hours: \\\"\") + graph_hours_component + pynutil.insert(\"\\\"\")\n",
        "\n",
        "graph_minutes_component = (\n",
        "    pynutil.insert(\" minutes: \\\"\") + pynini.union(minutes, graph_fractions) + pynutil.insert(\"\\\"\")\n",
        ")\n",
        "graph_minutes_component = delete_space + graph_minutes_component\n",
        "\n",
        "graph_time_standard = (graph_hours_component + delete_space + graph_heures\n",
        "                       + pynini.closure(graph_minutes_component, 0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2avfS3IacSiC"
      },
      "source": [
        "We now setup the alternate graph that allows backwards counting. Note, this is triggered by the occurrence of \"moins\" between the hour and minute component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmpwisOVcn0T"
      },
      "outputs": [],
      "source": [
        "graph_hours_to_component = hours | graph_midi | graph_minuit\n",
        "graph_hours_to_component @= hours_to\n",
        "graph_hours_to_component = pynutil.insert(\"hours: \\\"\") + graph_hours_to_component + pynutil.insert(\"\\\"\")\n",
        "graph_hours_to_component = graph_hours_to_component + delete_space + graph_heures\n",
        "\n",
        "graph_minutes_to_component = (minutes | graph_demi |  # No 'et' in fractions\n",
        "                              (pynutil.delete(\"le \") + graph_quart) | graph_trois_quart)\n",
        "graph_minutes_to_component @= minutes_to\n",
        "graph_minutes_to_component = pynutil.insert(\" minutes: \\\"\") + graph_minutes_to_component + pynutil.insert(\"\\\"\")\n",
        "\n",
        "graph_time_to = graph_hours_to_component + delete_space + graph_moins + delete_space + graph_minutes_to_component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkO4tRRfdQT4"
      },
      "source": [
        "We now join it with our main component, allowing us to graph all times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O0vUVizdU8c"
      },
      "outputs": [],
      "source": [
        "graph_time = graph_time_standard | graph_time_to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbX4JV-LdY3Y"
      },
      "source": [
        "Once again we throw a wrench into things with the `suffix` feature. As in the case of Ordinals and Decimals, key-words can play into our Time WFST. For French, this occurs with the words \"du matin\", \"de l'après-midi\",  and \"du soir\". (Respectively: \"in the morning\", \"in the afternoon\", and \"in the evening\".) Much like in English, these phrases alter how we write down the time. But instead of indicating `a.m.` or `p.m.`, these indicate *what hour system is used*. For example:\n",
        "- \"deux heures du matin\" -> `2 h` = `2:00 a.m.`\n",
        "- \"deux heures de l'après-midi\" -> `14 h` = `2:00 p.m.`\n",
        "\n",
        "Only a twelve hour system is used when these suffixes accompany the time. As such, our Classifier will need to either adjust the times like in the case of counting backwards or must pass the information to the Verbalizer so it can adjust.\n",
        "\n",
        "Since our Classifier is long enough as is, we will simply store this information in the `suffix` property and allow the Verbalizer to manage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqVa78zRgJw9"
      },
      "outputs": [],
      "source": [
        "graph_suffix_am = pynini.cross(\"du matin\", \"am\")\n",
        "graph_suffix_pm = pynini.string_map([(\"de l'après-midi\", \"pm\"),(\"du soir\", \"pm\")])\n",
        "\n",
        "graph_suffix = pynini.cross(graph_suffix_am, \"am\") | pynini.cross(graph_suffix_pm, \"pm\")\n",
        "\n",
        "graph_suffix_component = pynutil.insert(\" suffix: \\\"\") + graph_suffix + pynutil.insert(\"\\\"\")\n",
        "graph_suffix_component = delete_space + graph_suffix_component\n",
        "graph_suffix_component = pynini.closure(graph_suffix_component, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LaJMIjUf1XR"
      },
      "source": [
        "And we append to our graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76myCFiggX3E"
      },
      "outputs": [],
      "source": [
        "class TimeFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"time\", kind=\"classify\")\n",
        "        \"\"\"grammar omitted for length\n",
        "        ....\n",
        "        ....\n",
        "        ....\n",
        "        \"\"\"\n",
        "        graph_hours_component = pynini.union(hours, graph_midi, graph_minuit)\n",
        "        graph_hours_component = pynutil.insert(\"hours: \\\"\") + graph_hours_component + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        graph_minutes_component = (\n",
        "            pynutil.insert(\" minutes: \\\"\") + pynini.union(minutes, graph_fractions) + pynutil.insert(\"\\\"\")\n",
        "        )\n",
        "        graph_minutes_component = delete_space + graph_minutes_component\n",
        "\n",
        "        graph_time_standard = (graph_hours_component + delete_space + graph_heures\n",
        "                               + pynini.closure(graph_minutes_component, 0, 1))\n",
        "\n",
        "        graph_hours_to_component = hours | graph_midi | graph_minuit\n",
        "        graph_hours_to_component @= hours_to\n",
        "        graph_hours_to_component = pynutil.insert(\"hours: \\\"\") + graph_hours_to_component + pynutil.insert(\"\\\"\")\n",
        "        graph_hours_to_component = graph_hours_to_component + delete_space + graph_heures\n",
        "\n",
        "        graph_minutes_to_component = (minutes | graph_demi |  # No 'et' in fractions\n",
        "                                      (pynutil.delete(\"le \") + graph_quart) | graph_trois_quart)\n",
        "        graph_minutes_to_component @= minutes_to\n",
        "        graph_minutes_to_component = pynutil.insert(\" minutes: \\\"\") + graph_minutes_to_component + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        graph_time_to = graph_hours_to_component + delete_space + graph_moins + delete_space + graph_minutes_to_component\n",
        "\n",
        "        graph_time_no_suffix = graph_time_standard | graph_time_to\n",
        "\n",
        "        graph_suffix_am = pynini.cross(\"du matin\", \"am\")\n",
        "        graph_suffix_pm = pynini.string_map([(\"de l'après-midi\", \"pm\"),(\"du soir\", \"pm\")])\n",
        "\n",
        "        graph_suffix = pynini.cross(graph_suffix_am, \"am\") | pynini.cross(graph_suffix_pm, \"pm\")\n",
        "\n",
        "        graph_suffix_component = pynutil.insert(\" suffix: \\\"\") + graph_suffix + pynutil.insert(\"\\\"\")\n",
        "        graph_suffix_component = delete_space + graph_suffix_component\n",
        "        graph_suffix_component = pynini.closure(graph_suffix_component, 0, 1)\n",
        "\n",
        "        final_graph = graph_time_no_suffix + graph_suffix_component\n",
        "\n",
        "        final_graph = self.add_tokens(final_graph)\n",
        "\n",
        "        self.fst = final_graph.optimize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huxZgDYTkSR4"
      },
      "source": [
        "Let's see how we did:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vcuMU48kSR4"
      },
      "outputs": [],
      "source": [
        "time = TimeFst().fst\n",
        "example = \"quatre heures moins cinq\"\n",
        "apply_fst(example, time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPlJ1qyeAWOL"
      },
      "source": [
        "## Verbalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrO-xtJ87PEl"
      },
      "source": [
        "The initial part of the Verbalizer should appear familiar. We delete the property tags `hours` and `minutes`, making sure they preserve the actual values for formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCzZKR7ek0Mz"
      },
      "outputs": [],
      "source": [
        "hour = (\n",
        "    pynutil.delete(\"hours:\")\n",
        "    + delete_space\n",
        "    + pynutil.delete(\"\\\"\")\n",
        "    + pynini.closure(NEMO_DIGIT, 1, 2)\n",
        "    + pynutil.delete(\"\\\"\")\n",
        ")\n",
        "minute = (\n",
        "    pynutil.delete(\"minutes:\")\n",
        "    + delete_extra_space\n",
        "    + pynutil.delete(\"\\\"\")\n",
        "    + pynini.closure(NEMO_DIGIT, 1, 2)\n",
        "    + pynutil.delete(\"\\\"\")\n",
        ")\n",
        "graph = hour + delete_extra_space + pynutil.insert(\"h\") + minute.ques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnVV9GUKk-b7"
      },
      "source": [
        "We then deal with the case of `suffix`. We first note that if the suffix is for a morning time (before noon), then there is no further conversion that is needed. We may simply delete the property and its value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haOEiSbglc6s"
      },
      "outputs": [],
      "source": [
        "day_suffixes = pynutil.delete(\"suffix: \\\"am\\\"\")\n",
        "\n",
        "graph = hours + delete_extra_space + pynutil.insert(\"h\") + minute.ques + delete_space + day_suffixes.ques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL0FNg6Xlhb-"
      },
      "source": [
        "Meanwhile, the post-noon suffixes would require us shifting the hours value by twelve. Much like in the case of counting backwards from the hour, we can simply create a WFST to do this addition work for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLrabUNplwG7"
      },
      "outputs": [],
      "source": [
        "hour_to_night = pynini.string_map([\n",
        "                                   (\"1\", \"13\"),\n",
        "                                   (\"2\", \"14\"),\n",
        "                                   (\"3\", \"15\"),\n",
        "                                   (\"4\", \"16\"),\n",
        "                                   (\"5\", \"17\"),\n",
        "                                   (\"6\", \"18\"),\n",
        "                                   (\"7\", \"19\"),\n",
        "                                   (\"8\", \"20\"),\n",
        "                                   (\"9\", \"21\"),\n",
        "                                   (\"10\", \"22\"),\n",
        "                                   (\"11\", \"23\"), # Note that 12 and 24 would be phrased \"midi\" and \"minuit\" respectively\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0-z-qJAmIiI"
      },
      "source": [
        "We then create an alternate graph where this conversion is mapped onto the hours function - given a post-noon suffix - and create a union with our earlier graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CdEmo9NmN7u"
      },
      "outputs": [],
      "source": [
        "night_suffixes = pynutil.delete(\"suffix: \\\"pm\\\"\")\n",
        "graph |= (\n",
        "            hour @ hour_to_night\n",
        "            + delete_extra_space\n",
        "            + pynutil.insert(\"h\")\n",
        "            + minute.ques\n",
        "            + delete_space\n",
        "            + night_suffixes\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnoIkZBqmaTo"
      },
      "source": [
        "Giving us a final Verbalizer of:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfXimvFBmdDD"
      },
      "outputs": [],
      "source": [
        "class TimeFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"time\", kind=\"verbalize\")\n",
        "\n",
        "        hour_to_night = pynini.string_map([\n",
        "                                   (\"1\", \"13\"),\n",
        "                                   (\"2\", \"14\"),\n",
        "                                   (\"3\", \"15\"),\n",
        "                                   (\"4\", \"16\"),\n",
        "                                   (\"5\", \"17\"),\n",
        "                                   (\"6\", \"18\"),\n",
        "                                   (\"7\", \"19\"),\n",
        "                                   (\"8\", \"20\"),\n",
        "                                   (\"9\", \"21\"),\n",
        "                                   (\"10\", \"22\"),\n",
        "                                   (\"11\", \"23\"),\n",
        "])\n",
        "\n",
        "        day_suffixes = pynutil.delete(\"suffix: \\\"am\\\"\")\n",
        "        night_suffixes = pynutil.delete(\"suffix: \\\"pm\\\"\")\n",
        "\n",
        "        hour = (\n",
        "            pynutil.delete(\"hours:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_DIGIT, 1, 2)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "        minute = (\n",
        "            pynutil.delete(\"minutes:\")\n",
        "            + delete_extra_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_DIGIT, 1, 2)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "\n",
        "        graph = hour + delete_extra_space + pynutil.insert(\"h\") + minute.ques + delete_space + day_suffixes.ques\n",
        "\n",
        "        graph |= (\n",
        "            hour @ hour_to_night\n",
        "            + delete_extra_space\n",
        "            + pynutil.insert(\"h\")\n",
        "            + minute.ques\n",
        "            + delete_space\n",
        "            + night_suffixes\n",
        "        )\n",
        "        delete_tokens = self.delete_tokens(graph)\n",
        "        self.fst = delete_tokens.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5tPcCaSYuhY"
      },
      "source": [
        "If you've noticed, the Verbalizer process has become simpler as we've progressed through our WFSTs. Commonly, you will seldom need to even provide the amount of overhead we've seen in `TimeFst`, `MoneyFst`, and `OrdinalFst`, and the majority of this component is simply removing tokens as an intermediary step, as we'll see for our Name class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHmRe3UIhyIH"
      },
      "source": [
        "# WhiteList WFST <a id=\"whitelist-wfst\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kMn2qB9bVFy"
      },
      "source": [
        "\n",
        "While developing your grammars, you may encounter tokens that refuse standard categorization and yet still require normalization. For example, you may need to render \"Mister Brown\" as `Mr. Brown` or \"H M S Nelson\" as `H.M.S. Nelson`. As these cases are rather specific, they lack a regular pattern for a specific classifier. (What about \"mister\" as a token requires tokenization as opposed to \"Brown\".) Instead, we need to explicitly list their input-output mappings (i.e. a whitelist).\n",
        "\n",
        "For NeMo, this is performed through the `WhiteListFst`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B4oPXYcccWs"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RThTLUCRceOO"
      },
      "source": [
        "`WhitelistFst` is essentially just a wrapper for a `string_map` or `string_file` mapping with the appropriate formatting for deployment. Per our example, we can make a graph with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIOOb_wJdMMx"
      },
      "outputs": [],
      "source": [
        "graph = pynini.string_map([\n",
        "                           (\"mister\", \"mr.\"),\n",
        "                           (\"h m s\", \"h.m.s\"),\n",
        "                           (\"doctor\", \"dr.\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5kTXwmPZ9Tt"
      },
      "source": [
        "As previously mentioned, here is where the use of `string_file` will make maintenance much easier. Discovering whitelist mappings is an iterative process and you will more than likely need to return to your list throughout development. For instance, it may be obvious that tokens such as \"madame\", \"miss\", \"esquire\", but would you think of providing abbreviations for \"the right honorable\" or \"tennessee valley authority\"? Keeping a tsv file available for quick insertions greatly assists here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC5Cf-Z8dYVk"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "144nvAHEdfBJ"
      },
      "source": [
        "Unlike for our other WFSTs, There is no specific semiotic class for `WhiteListFst`. It instead falls under the default Name class to designate there is no need for further processing beyond obligatory tokenization. Indeed, we can simply insert the token ourselves instead of calling `add_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPkrmg2gdznd"
      },
      "outputs": [],
      "source": [
        "class WhiteListFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"whitelist\", kind=\"classify\")\n",
        "\n",
        "        whitelist = pynini.string_map([\n",
        "                           (\"mister\", \"mr.\"),\n",
        "                           (\"h m s\", \"h.m.s\"),\n",
        "                           (\"doctor\", \"dr.\")])\n",
        "        graph = pynutil.insert(\"name: \\\"\") + convert_space(whitelist) + pynutil.insert(\"\\\"\")\n",
        "        self.fst = graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B05kdSIdd2dv"
      },
      "source": [
        "## Verbalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7RmZ4zRkSR5"
      },
      "source": [
        "Since the whitelisted token has already been rendered in the desired normalized form, all that is necessary is to strip the `name` token and render the string 'as is'. This can be done by through the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaq3voIYiUCA"
      },
      "outputs": [],
      "source": [
        "class WhiteListFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"whitelist\", kind=\"verbalize\")\n",
        "        graph = (\n",
        "            pynutil.delete(\"name:\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "            + pynini.closure(NEMO_CHAR - \" \", 1)\n",
        "            + pynutil.delete(\"\\\"\")\n",
        "        )\n",
        "        graph = graph @ pynini.cdrewrite(pynini.cross(u\"\\u00A0\", \" \"), \"\", \"\", NEMO_SIGMA) # Removes possible null token\n",
        "        self.fst = graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUE7Gg35bWKb"
      },
      "source": [
        "While the graph is largely self-explanatory, take note that the default implementation assumes a character string without spacing. If you intend to include additional formatting in your normalization (e.g. `H. M. S.` instead of `H.M.S.`), you may need to adjust the graph to expand coverage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o_a15Fg7niv"
      },
      "source": [
        "# Word and Punctuation WFST <a id=\"word-and-punctuation-wfst\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi6lP7mTmnUV"
      },
      "source": [
        "Continuing with the Name class, we will conclude with the Word and Punctuation WFSTs. These are among the simplest and most crucial classes of the entire ITN system, as they classify all tokens that are not caught by other semiotic classes. Since these other tokens make up the majority of all strings your normalization system will encounter, they are essential for general functionality.\n",
        "\n",
        "However, they escape discussion as their function is self-evident: since they function as default classes, tokens only reach Word WFST and Punctuation WFST if they have not been accepted by the other WFSTs. As such, we can simply accept the tokens as they are, providing them a `name` tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zCqczLqp5NW"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWum5U0p99c"
      },
      "source": [
        "For instance, consider the entire `WordFst` Classifier in its entirety:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCZSTeDHofDl"
      },
      "outputs": [],
      "source": [
        "class WordFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"word\", kind=\"classify\")\n",
        "        word = pynutil.insert(\"name: \\\"\") + pynini.closure(NEMO_NOT_SPACE, 1) + pynutil.insert(\"\\\"\")\n",
        "        self.fst = word.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ys2VpjjoiEC"
      },
      "source": [
        "It just processes the entire token string with the `NEMO_NOT_SPACE` utility WFST (which accepts any string that is not a space). For your language, you may simply use one of the preexisting `WordFst`.\n",
        "\n",
        "Depending on language, the `PunctuationFst` may require some (minimal) adjustment. Note the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnnd3PVMpF4t"
      },
      "outputs": [],
      "source": [
        "class PunctuationFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"punctuation\", kind=\"classify\")\n",
        "\n",
        "        s = \"!#$%&\\'()*+,-./:;<=>?@^_`{|}~\"\n",
        "        punct = pynini.union(*s)\n",
        "\n",
        "        graph = pynutil.insert(\"name: \\\"\") + punct + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        self.fst = graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_afW02LXpLtz"
      },
      "source": [
        "If your language uses other punctuation than that in the `s` string (or reserves some of the punctuation as characters), you may simply edit `s` to accommodate.\n",
        "\n",
        "For instance, French has a unique quotation style that utilizes guillemets \"« »\". We may add their Unicode codepoints (to avoid encoding issues) to `s`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgfZIKzVplVm"
      },
      "outputs": [],
      "source": [
        "class PunctuationFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"punctuation\", kind=\"classify\")\n",
        "\n",
        "        s = \"!#$%&\\'()*+,-./:;<=>?@^_`{|}~\"\n",
        "        guillemets = \"\\u00AB\" + \"\\u00BB\"  # quotation marks in French.\n",
        "        s += guillemets\n",
        "        punct = pynini.union(*s)\n",
        "\n",
        "        graph = pynutil.insert(\"name: \\\"\") + punct + pynutil.insert(\"\\\"\")\n",
        "\n",
        "        self.fst = graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Upb5-wcp_7H"
      },
      "source": [
        "## Verbalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufWT1T6GqCCT"
      },
      "source": [
        "Note that both `PunctuationFst` and `WordFst` both encode with the `name` property. This leaves no differentiation between the two for a Verbalizer. This makes sense as there are no particular formatting rules for them, they simply need a placeholder tag to avoid alteration between the Classifier and Verbalizer step. Once passed to the verbalizer, they are rendered as normal by simply removing the tag (this is practically identical to the WhiteListFST):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqyhqQKZqcph"
      },
      "outputs": [],
      "source": [
        "class WordFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"word\", kind=\"verbalize\")\n",
        "        chars = pynini.closure(NEMO_CHAR - \" \", 1)\n",
        "        char = pynutil.delete(\"name:\") + delete_space + pynutil.delete(\"\\\"\") + chars + pynutil.delete(\"\\\"\")\n",
        "        graph = char @ pynini.cdrewrite(pynini.cross(u\"\\u00A0\", \" \"), \"\", \"\", NEMO_SIGMA) # Cleans up possible null character\n",
        "\n",
        "        self.fst = graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGbrUkcpapyi"
      },
      "source": [
        "For many languages, the writing of your `WordFst` and `PunctuationFst` (both Classifiers and Verbalizers) will require no more than duplicating the preexisting grammars found in NeMo Text Processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y9jhkhQ7p4W"
      },
      "source": [
        "# Other Classes <a id=\"other-classes\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1mgnISmiu-g"
      },
      "source": [
        "While the preceding discussion should be suitable for development of the remaining classes, some helpful notes may be of use before continuing:\n",
        "- Fraction WFST: This is the last of the 'fundamental' classes and should take priority after completion of the Decimal WFST. It operates very similarly to the Ordinal WFST in that you wish to recover the Cardinal roots for the numerator and denominator prior to tagging. Its properties are: `negative`, `integer_part`, `numerator`, and `denominator`.\n",
        "- Measure WFST: Like the Money WFST, this will require management of several 'parent' WFSTS (Fraction, Cardinal, Decimal) to be suitably comprehensive. As well, you may find it more productive to find ways to compose new measurement units instead of simply listing all (e.g. micrometers, petameters, miles per hour, feet per second). Its properties are: `negative`, `units` and it allows subgraphs of the `cardinal`, `decimal`, and `fraction` classes. (This is, it allows tokenization within the tokenization.)\n",
        "- Date WFST: Depending on writing conventions, this may vary in complexity. For instance, English speakers may write dates as `01/01/2021/` or `Jan. 1 2021`. Are there specific use cases where one is preferred or should you simply decide on a format? Further, you may wish to take advantage of the `preserve order` property to avoid possible unwanted verbalizations (some implementations will permit both `Jan. 1` and `1 Jan.` if not careful.) Its properties are: `month`, `day`, and `year`.\n",
        "- Telephone WFST: These will be heavily dependent not only on writing conventions but even regional preference. For instance, the U.S. commonly uses a ten digit system broken into the following sequence: `###-###-####`. Meanwhile, mainland France breaks a ten digit sequence into groups of two: `##-##-##-##-##`. Take careful note of how your language's target region verbalizes these figures and leave room for some variation in development. The `telephone` class has only one property: `number_part`.\n",
        "- Electronic WFST: For normalizing email addresses or urls, you will need to develop for the `electronic` class. The main concerns will be managing alphanumeric strings and parsing the reserved symbols used for protocols and domains. (How does your target language pronounce \"https://\"? www? '.' or '@'?\") Depending on whether you are normalizing a url or email, the following properties will be needed:\n",
        "    - email: `username`, `domain`\n",
        "    - url: `protocol` (Sparrowhawk allows further detail here but NeMo passes the entire url through the `protocol` property)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i25X8mK90n3"
      },
      "source": [
        "# Tokenize and Classify <a id=\"tokenize-and-classify\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4bcigU6b9ss"
      },
      "source": [
        "We are now ready to build a general Classifier for our entire language. Upon completion of your grammars, the next step is to unite them together in a general Classifier WFST - located within a `tokenize_and_classify.py` file, preferably. This WFST will be responsible for determining the appropriate semiotic class for each token in your string and processing the necessary properties for normalization.\n",
        "\n",
        "For this section, we will focus on the following: grammar composition, assignment of weights, and importing/exporting as a FAR file. Since we will need to work with some instantiated graphs, let's preload them before proceeding. (Note the compilingtime.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKTlYhZekSR6"
      },
      "outputs": [],
      "source": [
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.cardinal import CardinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.decimal import DecimalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.money import MoneyFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.ordinal import OrdinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.punctuation import PunctuationFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.time import TimeFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.whitelist import WhiteListFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.taggers.word import WordFst\n",
        "\n",
        "cardinal = CardinalFst()\n",
        "cardinal_graph = cardinal.fst\n",
        "\n",
        "ordinal = OrdinalFst(cardinal)\n",
        "ordinal_graph = ordinal.fst\n",
        "\n",
        "decimal = DecimalFst(cardinal)\n",
        "decimal_graph = decimal.fst\n",
        "\n",
        "whitelist_graph = WhiteListFst().fst\n",
        "word_graph = WordFst().fst\n",
        "time_graph = TimeFst().fst\n",
        "money_graph = MoneyFst(cardinal, decimal).fst\n",
        "punct_graph = PunctuationFst().fst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIv58eSocOV1"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_RPlnfVdG5E"
      },
      "source": [
        "As for all previous grammars, the `tokenize_and_classify` grammar inherits from a `GraphFst` as an individual class: `ClassifyFst`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHKG4c2WdW0G"
      },
      "outputs": [],
      "source": [
        "class ClassifyFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9_I6DJmdcOG"
      },
      "source": [
        "This class is responsible for instantiating all subgraphs and passing necessary dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YtmcxLOdlas"
      },
      "outputs": [],
      "source": [
        "class ClassifyFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")\n",
        "\n",
        "        cardinal = CardinalFst()\n",
        "        cardinal_graph = cardinal.fst\n",
        "\n",
        "        ordinal = OrdinalFst(cardinal)\n",
        "        ordinal_graph = ordinal.fst\n",
        "\n",
        "        decimal = DecimalFst(cardinal)\n",
        "        decimal_graph = decimal.fst\n",
        "\n",
        "        whitelist_graph = WhiteList().fst\n",
        "        word_graph = WordFst().fst\n",
        "        time_graph = TimeFst().fst\n",
        "        money_graph = MoneyFst(cardinal, decimal).fst\n",
        "        punct_graph = PunctuationFst().fst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5vGvv3HeAY9"
      },
      "source": [
        "We then join all the grammars together so `ClassifyFst` can apply them. Rather unceremoniously, this is accomplished by performing a union across all grammars (excluding `PunctuationFst`, to assist tokenization). We then follow this union by inserting the `tokens` class around the resulting formatting (required for processing):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oocgPQ5geZJO"
      },
      "outputs": [],
      "source": [
        "class ClassifyFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")\n",
        "\n",
        "        cardinal = CardinalFst()\n",
        "        cardinal_graph = cardinal.fst\n",
        "\n",
        "        ordinal = OrdinalFst(cardinal)\n",
        "        ordinal_graph = ordinal.fst\n",
        "\n",
        "        decimal = DecimalFst(cardinal)\n",
        "        decimal_graph = decimal.fst\n",
        "\n",
        "        whitelist_graph = WhiteListFst().fst\n",
        "        word_graph = WordFst().fst\n",
        "        time_graph = TimeFst().fst\n",
        "        money_graph = MoneyFst(cardinal, decimal).fst\n",
        "        punct_graph = PunctuationFst().fst\n",
        "\n",
        "        classify = (\n",
        "            time_graph\n",
        "            | whitelist_graph\n",
        "            | decimal_graph\n",
        "            | cardinal_graph\n",
        "            | ordinal_graph\n",
        "            | money_graph\n",
        "            | word_graph\n",
        "        )\n",
        "        token = pynutil.insert(\"tokens { \") + classify + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWDXWQjfLEU"
      },
      "source": [
        "Our graph is now able to process an individual token. But what about a string? Here you will need to be mindful of the tokenization behavior for your language and decide on your desired treatment of punctuation (hence exclusion from the main graph).\n",
        "\n",
        "For our purposes, we will assume the convention of space and punctuation serving as token separators. We graph punctuation as individual tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6WztK2jwhFt"
      },
      "outputs": [],
      "source": [
        "punct_graph = PunctuationFst().fst\n",
        "punct = pynutil.insert(\"tokens { \") + punct_graph + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2rT89jw3T1"
      },
      "source": [
        "and join the `punct` graph with our `tokens` graph (inserting spaces between tokens for formatting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGtVOK-txKOP"
      },
      "outputs": [],
      "source": [
        "token = \"PLACEHOLDER\"\n",
        "token_plus_punct = (\n",
        "      pynini.closure(punct + pynutil.insert(\" \")) + token + pynini.closure(pynutil.insert(\" \") + punct)\n",
        "            ) # Note the use of closure incase there are multiple punctuations\n",
        "graph = token_plus_punct + pynini.closure(delete_extra_space + token_plus_punct)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gixfQ69xWPe"
      },
      "source": [
        "then address space between tokens:\n",
        "\n",
        "`graph = delete_space + graph + delete_space`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWnmazWecyUG"
      },
      "source": [
        "## Weighting <a id=\"classifyweights\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egHbwIbMx-hT"
      },
      "source": [
        "Were we to leave our `ClassifyFst` like this, we would undoubtedly encounter a mountain of errors. What will stop our graph from treating punctuation that is part of a previous grammar as a token separator (e.g. \"vingt-et-un\")? How do we ensure that a currency string isn't treated as solely a decimal string with a `name` token following?\n",
        "\n",
        "As in previous cases, the solution lies in our choice of weights for the grammar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3U7_M8CyxZ1"
      },
      "source": [
        "Let us return to the main graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VXe1dfsy3Be"
      },
      "outputs": [],
      "source": [
        "classify = (\n",
        "                time_graph\n",
        "                | whitelist_graph\n",
        "                | decimal_graph\n",
        "                | cardinal_graph\n",
        "                | ordinal_graph\n",
        "                | money_graph\n",
        "                | word_graph\n",
        "            )\n",
        "punct = pynutil.insert(\"tokens { \") + punct_graph + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY4vOFqxy5ua"
      },
      "source": [
        "Beyond the path weights that we explicitly added, these graphs are currently weightless. Since we want the graphs themselves to be the general determiners of a path, let us use some default weights an order of magnitude beyond our path weights (we use `pynutil.add_weight`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bthyt_Le2rsA"
      },
      "outputs": [],
      "source": [
        "classify = (\n",
        "                pynutil.add_weight(time_graph, 1)\n",
        "                | pynutil.add_weight(whitelist_graph, 1)\n",
        "                | pynutil.add_weight(decimal_graph, 1)\n",
        "                | pynutil.add_weight(cardinal_graph, 1)\n",
        "                | pynutil.add_weight(ordinal_graph, 1)\n",
        "                | pynutil.add_weight(money_graph, 1)\n",
        "                | pynutil.add_weight(word_graph, 1)\n",
        "            )\n",
        "punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, 1) + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMNIJbzj3MMP"
      },
      "source": [
        "Let's see what logical adjustments should be made. First off, we know that we want each class token to span the largest string possible. (e.g. We don't want \"quatre-vingt\" to be rendered as two `cardinal` classes with a hyphen in between.) As such, we want to penalize our graph for using more than one tokens. We can do so by establishing the following constraint: the sum of two or more tokens cannot be less than the weight of a single token. Or, for any pair of tokens `w_1` and `w_2`, their sum must always be greater than any other individual token (including themselves):\n",
        "\n",
        "`w_1 + w_2 > k >= w`\n",
        "\n",
        "To keep things simple, let us make the upper limit `2`. This means we should increase all the weights to keep our constraint:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16IJdPBWkSR7"
      },
      "outputs": [],
      "source": [
        "classify = (\n",
        "                pynutil.add_weight(time_graph, 1.1)\n",
        "                | pynutil.add_weight(whitelist_graph, 1.1)\n",
        "                | pynutil.add_weight(decimal_graph, 1.1)\n",
        "                | pynutil.add_weight(cardinal_graph, 1.1)\n",
        "                | pynutil.add_weight(ordinal_graph, 1.1)\n",
        "                | pynutil.add_weight(money_graph, 1.1)\n",
        "                | pynutil.add_weight(word_graph, 1.1)\n",
        "            )\n",
        "punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, 1.1) + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iVw6vc_kSR7"
      },
      "source": [
        "Do we want this constraint to include all tokens? Imagine if we had a string of multiple semiotic tokens in a row. Since this string's combined weight would be larger than any single class token, a grammar that served as a universal acceptor (i.e. `word_graph`) would be preferred over these individual classes. This would be obviously incorrect. As such, we want to make sure that `word_graph` would only be traversed when there is truly no other option:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc_CU2ro63eg"
      },
      "outputs": [],
      "source": [
        "classify = (\n",
        "                pynutil.add_weight(time_graph, 1.1)\n",
        "                | pynutil.add_weight(whitelist_graph, 1.1)\n",
        "                | pynutil.add_weight(decimal_graph, 1.1)\n",
        "                | pynutil.add_weight(cardinal_graph, 1.1)\n",
        "                | pynutil.add_weight(ordinal_graph, 1.1)\n",
        "                | pynutil.add_weight(money_graph, 1.1)\n",
        "                | pynutil.add_weight(word_graph, 100)\n",
        "            )\n",
        "punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, 1.1) + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPyM6QMQkSR7"
      },
      "source": [
        "Now, even with a string of fifty different class tokens, `word_graph` would still not be considered as a path to traverse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW8C3vD-7Dbl"
      },
      "source": [
        "Next, let us consider our foundational graph: `cardinal_graph`. As Cardinals occur in practically all our WFSTs, it's possible for `cardinal_graph` to apply in almost all cases. Yet, we've specifically invoked `CardinalFST` when it was required in any of the other classes, so it will never be needed in any of those cases. This means that we want all those graphs to have *priority* over `cardinal_graph`. As such, we will increase its weight so it takes second lowest precedence (while still paying attention to the combined weight constraint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97UwGaEn8pj7"
      },
      "outputs": [],
      "source": [
        "classify = (\n",
        "                pynutil.add_weight(time_graph, 1.1)\n",
        "                | pynutil.add_weight(whitelist_graph, 1.1)\n",
        "                | pynutil.add_weight(decimal_graph, 1.1)\n",
        "                | pynutil.add_weight(cardinal_graph, 1.2)\n",
        "                | pynutil.add_weight(ordinal_graph, 1.1)\n",
        "                | pynutil.add_weight(money_graph, 1.1)\n",
        "                | pynutil.add_weight(word_graph, 100)\n",
        "            )\n",
        "punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, 1.1) + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d9Lw4Ot88_B"
      },
      "source": [
        "This form of thinking can be applied to all the 'foundational' graphs you may develop: the dependent graphs should take higher precedence than the graphs they borrow from. For instance, since `money_graph` utilizes `decimal_graph`, we know it should take precedence. However, since `decimal_graph` borrows from `cardinal_graph`, its weight must still be less than `1.2`. As such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wF8cgLK9tpU"
      },
      "outputs": [],
      "source": [
        "classify = (\n",
        "                pynutil.add_weight(time_graph, 1)\n",
        "                | pynutil.add_weight(whitelist_graph, 1)\n",
        "                | pynutil.add_weight(decimal_graph, 1.1)\n",
        "                | pynutil.add_weight(cardinal_graph, 1.2)\n",
        "                | pynutil.add_weight(ordinal_graph, 1)\n",
        "                | pynutil.add_weight(money_graph, 1.09)\n",
        "                | pynutil.add_weight(word_graph, 100)\n",
        "            )\n",
        "punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, 1) + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huMzDoZ2-FD2"
      },
      "source": [
        "For those classes that don't seem affected, we can set their weights as the same as those below their 'foundation' graphs, simply to prevent prioritization when not required\n",
        "\n",
        "Meanwhile, `whitelist_graph` should take precedence over all others, as it may contain unique normalizations that may get accidentally caught by the other graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWG6ttyd-bbD"
      },
      "outputs": [],
      "source": [
        "classify = (\n",
        "                pynutil.add_weight(time_graph, 1.1)\n",
        "                | pynutil.add_weight(whitelist_graph, 1.07)\n",
        "                | pynutil.add_weight(decimal_graph, 1.1)\n",
        "                | pynutil.add_weight(cardinal_graph, 1.2)\n",
        "                | pynutil.add_weight(ordinal_graph, 1.1)\n",
        "                | pynutil.add_weight(money_graph, 1.08)\n",
        "                | pynutil.add_weight(word_graph, 100)\n",
        "            )\n",
        "punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, 1.1) + pynutil.insert(\" }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TH08f8O-fWx"
      },
      "source": [
        "Keep in mind that building weights in this manner is hardly a rule for grammar development and is instead intended as a means to initialize weights for empirical development. You will find that actual strings will cause unexpected behavior that require fine tuning.\n",
        "\n",
        "For instance, the Classifier for French in NeMo ITN benefits from having varying precedence for some weights, as seen in the following excerpt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKdkyDK3_r46"
      },
      "outputs": [],
      "source": [
        "class ClassifyFst(GraphFst):\n",
        "    \"\"\"\n",
        "    Final class that composes all other classification grammars. This class can process an entire sentence, that is lower cased.\n",
        "    For deployment, this grammar will be compiled and exported to OpenFst Finate State Archiv (FAR) File.\n",
        "    More details to deployment at NeMo/tools/text_processing_deployment.\n",
        "\n",
        "    Args:\n",
        "        cache_dir: path to a dir with .far grammar file. Set to None to avoid using cache.\n",
        "        overwrite_cache: set to True to overwrite .far files\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir: str = None, overwrite_cache: bool = False):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")\n",
        "\n",
        "        far_file = None\n",
        "        if cache_dir is not None and cache_dir != \"None\":\n",
        "            os.makedirs(cache_dir, exist_ok=True)\n",
        "            far_file = os.path.join(cache_dir, \"_fr_itn.far\")\n",
        "        if not overwrite_cache and far_file and os.path.exists(far_file):\n",
        "            self.fst = pynini.Far(far_file, mode=\"r\")[\"tokenize_and_classify\"]\n",
        "            logging.info(f\"ClassifyFst.fst was restored from {far_file}.\")\n",
        "        else:\n",
        "            logging.info(f\"Creating ClassifyFst grammars.\")\n",
        "\n",
        "            cardinal = CardinalFst()\n",
        "            cardinal_graph = cardinal.fst\n",
        "\n",
        "            fraction = FractionFst(cardinal)\n",
        "            fraction_graph = fraction.fst\n",
        "\n",
        "            ordinal = OrdinalFst(cardinal)\n",
        "            ordinal_graph = ordinal.fst\n",
        "\n",
        "            decimal = DecimalFst(cardinal)\n",
        "            decimal_graph = decimal.fst\n",
        "\n",
        "            measure_graph = MeasureFst(cardinal=cardinal, decimal=decimal, fraction=fraction).fst\n",
        "            date_graph = DateFst(cardinal).fst\n",
        "            word_graph = WordFst().fst\n",
        "            time_graph = TimeFst().fst\n",
        "            money_graph = MoneyFst(cardinal, decimal).fst\n",
        "            whitelist_graph = WhiteListFst().fst\n",
        "            punct_graph = PunctuationFst().fst\n",
        "            electronic_graph = ElectronicFst().fst\n",
        "            telephone_graph = TelephoneFst().fst\n",
        "\n",
        "            classify = (\n",
        "                pynutil.add_weight(whitelist_graph, 1.01)\n",
        "                | pynutil.add_weight(time_graph, 1.05)\n",
        "                | pynutil.add_weight(date_graph, 1.09)\n",
        "                | pynutil.add_weight(decimal_graph, 1.08)\n",
        "                | pynutil.add_weight(measure_graph, 1.1)\n",
        "                | pynutil.add_weight(cardinal_graph, 1.1)\n",
        "                | pynutil.add_weight(ordinal_graph, 1.1)\n",
        "                | pynutil.add_weight(fraction_graph, 1.09)\n",
        "                | pynutil.add_weight(money_graph, 1.07)\n",
        "                | pynutil.add_weight(telephone_graph, 1.1)\n",
        "                | pynutil.add_weight(electronic_graph, 1.1)\n",
        "                | pynutil.add_weight(word_graph, 100)\n",
        "            )\n",
        "\n",
        "            punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, weight=1.1) + pynutil.insert(\" }\")\n",
        "            token = pynutil.insert(\"tokens { \") + classify + pynutil.insert(\" }\")\n",
        "            token_plus_punct = (\n",
        "                pynini.closure(punct + pynutil.insert(\" \")) + token + pynini.closure(pynutil.insert(\" \") + punct)\n",
        "            )\n",
        "\n",
        "            graph = token_plus_punct + pynini.closure(delete_extra_space + token_plus_punct)\n",
        "            graph = delete_space + graph + delete_space\n",
        "\n",
        "            self.fst = graph.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc4B_0rNcQZu"
      },
      "source": [
        "## FAR import/export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nRRPvy-AYsA"
      },
      "source": [
        "While working through these code excerpts, you may have noticed some latency with each instantiation of our WFSTs (notably wherever `CardinalFst` was involved). This is because the `pynini.optimize` that we call with each graph's instantiation is computationally expensive. For our ultimate purpose of deployment, it seems a waste of resources to recreate stable graphs for each use.\n",
        "\n",
        "To address this, NeMo ITN supports WFST caching through use of `pynini.Far`, storing and recovering Classify grammars as FAR (Fst ARchives).\n",
        "\n",
        "Let us update our `ClassifyFst` to permit passing a cache and allowing overwriting (for development):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XgWevUzD1AE"
      },
      "outputs": [],
      "source": [
        "class ClassifyFst(GraphFst):\n",
        "    def __init__(self, cache_dir: str = None, overwrite_cache: bool = False):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l28GMR70ESz0"
      },
      "source": [
        "For storing our graphs as FARs, we can use `graph_utils.generator_main`, which saves our WFSTs by type for easier management. For arguments it takes a string name and a dict mapping of WFST type to graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzTkcmAWFLYm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "class ClassifyFst(GraphFst):\n",
        "    def __init__(self, cache_dir: str = None, overwrite_cache: bool = False):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")\n",
        "        # Grammar here\n",
        "        # ....\n",
        "        if cache_dir is not None and cache_dir != \"None\":\n",
        "                    os.makedirs(cache_dir, exist_ok=True)\n",
        "                    far_file = os.path.join(cache_dir, \"_fr_itn.far\")\n",
        "                    generator_main(far_file, {\"tokenize_and_classify\": self.fst})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz8wjCQSD6eJ"
      },
      "source": [
        "We pair this with the ability to load from cache (note the `\"tokenize_and_classify\"` key being passed):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRFYgMmuD_53"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "class ClassifyFst(GraphFst):\n",
        "    def __init__(self, cache_dir: str = None, overwrite_cache: bool = False):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")\n",
        "        if not overwrite_cache and far_file and os.path.exists(far_file):\n",
        "            self.fst = pynini.Far(far_file, mode=\"r\")[\"tokenize_and_classify\"]\n",
        "        else:\n",
        "            # Grammar here\n",
        "            # ....\n",
        "            if cache_dir is not None and cache_dir != \"None\":\n",
        "                os.makedirs(cache_dir, exist_ok=True)\n",
        "                far_file = os.path.join(cache_dir, \"_fr_itn.far\")\n",
        "                generator_main(far_file, {\"tokenize_and_classify\": self.fst})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib9nggZxF38s"
      },
      "source": [
        "Producing our `ClassifyFst` as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2BZyx6sGGg2"
      },
      "outputs": [],
      "source": [
        "class ClassifyFst(GraphFst):\n",
        "    def __init__(self, cache_dir: str = None, overwrite_cache: bool = False):\n",
        "        super().__init__(name=\"tokenize_and_classify\", kind=\"classify\")\n",
        "\n",
        "        far_file = None\n",
        "        if cache_dir is not None and cache_dir != \"None\":\n",
        "            os.makedirs(cache_dir, exist_ok=True)\n",
        "            far_file = os.path.join(cache_dir, \"_fr_itn.far\")\n",
        "        if not overwrite_cache and far_file and os.path.exists(far_file):\n",
        "            self.fst = pynini.Far(far_file, mode=\"r\")[\"tokenize_and_classify\"]\n",
        "        else:\n",
        "            cardinal = CardinalFst()\n",
        "            cardinal_graph = cardinal.fst\n",
        "\n",
        "            ordinal = OrdinalFst(cardinal)\n",
        "            ordinal_graph = ordinal.fst\n",
        "\n",
        "            decimal = DecimalFst(cardinal)\n",
        "            decimal_graph = decimal.fst\n",
        "\n",
        "            whitelist_graph = WhiteList().fst\n",
        "            word_graph = WordFst().fst\n",
        "            time_graph = TimeFst().fst\n",
        "            money_graph = MoneyFst(cardinal, decimal).fst\n",
        "            whitelist_graph = WhiteListFst().fst\n",
        "            punct_graph = PunctuationFst().fst\n",
        "\n",
        "            classify = (\n",
        "                pynutil.add_weight(time_graph, 1.1)\n",
        "                | pynutil.add_weight(whitelist_graph, 1.01)\n",
        "                | pynutil.add_weight(decimal_graph, 1.09)\n",
        "                | pynutil.add_weight(cardinal_graph, 1.1)\n",
        "                | pynutil.add_weight(ordinal_graph, 1.09)\n",
        "                | pynutil.add_weight(money_graph, 1.08)\n",
        "                | pynutil.add_weight(word_graph, 100)\n",
        "            )\n",
        "\n",
        "            punct = pynutil.insert(\"tokens { \") + pynutil.add_weight(punct_graph, weight=1.1) + pynutil.insert(\" }\")\n",
        "            token = pynutil.insert(\"tokens { \") + classify + pynutil.insert(\" }\")\n",
        "            token_plus_punct = (\n",
        "                pynini.closure(punct + pynutil.insert(\" \")) + token + pynini.closure(pynutil.insert(\" \") + punct)\n",
        "            )\n",
        "\n",
        "            graph = token_plus_punct + pynini.closure(delete_extra_space + token_plus_punct)\n",
        "            graph = delete_space + graph + delete_space\n",
        "\n",
        "            self.fst = graph.optimize()\n",
        "\n",
        "            if far_file:\n",
        "                generator_main(far_file, {\"tokenize_and_classify\": self.fst})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEhY6wKKtfhn"
      },
      "source": [
        "You should find the caching to vastly speed up compilingtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTtCnC5w95CI"
      },
      "source": [
        "# Verbalize and Verbalize Final <a id=\"verbalize-and-verbalize-final\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9y5yuk1HaGj"
      },
      "source": [
        "Our last step is to create a universal Verbalizer for all classes. This is very similar to development of `ClassifierFst`, except that the Verbalizer breaks its normalization task into two components:\n",
        "- `VerbalizeFst`, which removes formatting for each token\n",
        "- `VerbalizeFinalFst`, which extends `VerbalizeFst` across all tokens in a string\n",
        "Why two components when `tokenize_and_classify` was one? Because Sparrowhawk performs all the functionality of `VerbalizeFinalFst`, so its inclusion would break deployment. However, without it, your NeMo grammar would be unable to function at base. So we separate the two to allow the best of both world."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUawTJVuH8iR"
      },
      "source": [
        "## VerbalizeFst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghiBV06IIWU"
      },
      "source": [
        "Much like `ClassifyFst`, `VerbalizeFst` instantiates all its subgraphs and then joins them together under a union operation. However, it does not need to employ weighting. Why? Because `ClassifyFst` has assigned each token a specific class. As each class is unique, there is no possibility that a subgraph will be employed for the wrong token.\n",
        "\n",
        "As such, our `VerbalizeFst` is formed by a simple union operation across all previous Verbalizer graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMVCqCvsIt2v"
      },
      "outputs": [],
      "source": [
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.cardinal import CardinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.decimal import DecimalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.money import MoneyFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.ordinal import OrdinalFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.time import TimeFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.whitelist import WhiteListFst\n",
        "from nemo_text_processing.inverse_text_normalization.fr.verbalizers.word import WordFst\n",
        "\n",
        "class VerbalizeFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"verbalize\", kind=\"verbalize\")\n",
        "        cardinal = CardinalFst()\n",
        "        cardinal_graph = cardinal.fst\n",
        "        ordinal_graph = OrdinalFst().fst\n",
        "        decimal = DecimalFst()\n",
        "        decimal_graph = decimal.fst\n",
        "        whitelist_graph = WhiteListFst().fst\n",
        "        money_graph = MoneyFst(decimal=decimal).fst\n",
        "        time_graph = TimeFst().fst\n",
        "        graph = (\n",
        "            time_graph\n",
        "            | whitelist_graph\n",
        "            | money_graph\n",
        "            | ordinal_graph\n",
        "            | decimal_graph\n",
        "            | cardinal_graph\n",
        "        )\n",
        "        self.fst = graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wap-LU6EI2Iu"
      },
      "source": [
        "## Verbalize Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYaEt_0tI47t"
      },
      "source": [
        "With `VerbalizeFst` complete, we now extend our graph to cover any series of tokens. All this requires is deletion of the `tokens` formatting (note the absence of such in our previous graph) and use of closure for any series of one or more tokens.\n",
        "\n",
        "This provides the following graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-9lJNE6JPCW"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VerbalizeFinalFst(GraphFst):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"verbalize_final\", kind=\"verbalize\")\n",
        "        verbalize = VerbalizeFst().fst\n",
        "        word = WordFst().fst\n",
        "        types = verbalize | word\n",
        "        graph = (\n",
        "            pynutil.delete(\"tokens\")\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"{\")\n",
        "            + delete_space\n",
        "            + types\n",
        "            + delete_space\n",
        "            + pynutil.delete(\"}\")\n",
        "        )\n",
        "        graph = delete_space + pynini.closure(graph + delete_extra_space) + graph + delete_space\n",
        "        self.fst = graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwMKFw-QJVgm"
      },
      "source": [
        "Unlike `ClassifyFst`, NeMo ITN does not cache `VerbalizeFst` or `VerbalizeFinalFst`. (While you are welcome to provide such functionality in your own development, keep in mind that the limited complexity of our Verbalizers makes compilingtimes less significant.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U21AZearZMK"
      },
      "source": [
        "# Deployment <a id=\"deployment\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrSccoh9K6JK"
      },
      "source": [
        "Now that we have done all the groundwork, we can finally move to deployment. This final section will just cover the minor code alterations required to call your language through NeMo ITN and deploy through Sparrowhawk. For further information on using NeMo ITN, please see [this tutorial](https://colab.research.google.com/github/NVIDIA/NeMo-text-processing/blob/main/tutorials/Text_(Inverse)_Normalization.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Le2aJvFIAKd"
      },
      "source": [
        "## InverseNormalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2R3TUCDLi5-"
      },
      "source": [
        "NeMo calls upon the `InverseNormalizer` class for all ITN tasks. Given a string and language, it will instantiate both the `ClassifierFst` and `VerbalizeFst` respective for the given language. (Note: we do not use `VerbalizeFinal` as its functions are managed by Sparrowhawk.) To make your language deployable in the general NeMo ITN system, you must designate the availability of these classes for instantiation. (For more information, see the [source code](https://github.com/NVIDIA/NeMo/blob/main/nemo_text_processing/inverse_text_normalization/inverse_normalize.py).)\n",
        "\n",
        "To do so requires only two changes. The first is providing a string to identify your language as an option for `parse_args` ([ISO codes are advised](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfv4Ee3ML-Fg"
      },
      "source": [
        "```Python\n",
        "def parse_args():\n",
        "    parser = ArgumentParser()\n",
        "    ...\n",
        "    parser.add_argument(\"--language\", choices=[..., 'MY_LANGUAGE'], type=str)\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awVl5nAsMUTl"
      },
      "source": [
        "The next is to call your `ClassifyFst` and `VerbalizeFst` from `__init__`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ2saT9CkSR9"
      },
      "source": [
        "```bash\n",
        "class InverseNormalizer(Normalizer):\n",
        "    def __init__(self, lang: str = 'en', cache_dir: str = None, overwrite_cache: bool = False):\n",
        "\n",
        "      if lang == 'en':\n",
        "              from nemo_text_processing.inverse_text_normalization.en.taggers.tokenize_and_classify import ClassifyFst\n",
        "              from nemo_text_processing.inverse_text_normalization.en.verbalizers.verbalize_final import (\n",
        "                  VerbalizeFinalFst,\n",
        "              )\n",
        "      # Other languages\n",
        "      # ....\n",
        "      elif lang == 'MY_LANGUAGE':\n",
        "\n",
        "        from nemo_text_processing.inverse_text_normalization.MY_LANGUAGE.taggers.tokenize_and_classify import ClassifyFst\n",
        "\n",
        "        from nemo_text_processing.inverse_text_normalization.MY_LANGUAGE.verbalizers.verbalize_final import (\n",
        "\n",
        "                  VerbalizeFst,\n",
        "\n",
        "              )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI1PuejLMxdI"
      },
      "source": [
        "And you're done! NeMo will handle the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrksINQoICfj"
      },
      "source": [
        "## Grammar export and Deployment to C++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP9-dmMJSg3h"
      },
      "source": [
        "Find information here:\n",
        "https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/wfst_text_processing_deployment.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDoVUxCE-Dax"
      },
      "source": [
        "# Final Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw-9mU7ql8iY"
      },
      "source": [
        "Congratulations, you have now constructed an entire ITN system from the ground up! While your experience will vary with each language, you will find several commonalities that will assist you in further development.\n",
        "\n",
        "If you are interested in working further with your language WFSTs, you may wish to construct a TN system. Broadly, this is accomplished by inverting your previous graphs (`pynini.invert` may assist here) and changing your outputs to avoid indeterminacy (i.e. decide on one canonical output for your grammar for each class). But outside of such grammar specific edits, you repeat many of the steps exhibited here, such as:\n",
        "- Use of a two step classifier-verbalizer system\n",
        "- Same semiotic classes for tagging\n",
        "- Inheritance of `GraphFst`\n",
        "\n",
        "For Audio-based non-deterministic text normalization please extend your grammars with additional output options for ambiguous options. Every semiotic class has a input flag `deterministic` which is by default set to True. For non-deterministic text normalization add additional grammar for the case `deterministic=False`\n",
        "\n",
        "We also recommend to look at the source of some of the existing [languages](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_normalization/wfst/wfst_text_normalization.html#language-support), in particular English: https://github.com/NVIDIA/NeMo-text-processing/tree/main/nemo_text_processing/inverse_text_normalization/en."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "WFST Tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}